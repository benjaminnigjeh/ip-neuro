{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3451b3d3",
   "metadata": {},
   "source": [
    "Complete aggregated spectra (MS1 from all the scans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d22e23c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fisher_py.data.business import Scan\n",
    "from fisher_py import RawFile\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "def wholeCasting(folder_path, cast_path):\n",
    "    os.chdir(folder_path)\n",
    "\n",
    "    def helper_regex(text):\n",
    "        match = re.search(rf\"{'Full'}\\s+(\\w+)\", text)\n",
    "        if match:\n",
    "            return match.group(1)\n",
    "        return None\n",
    "    def find_matching_keys(sequence: str, substring_dict: dict) -> list:\n",
    "        return [key for key, substrings in substring_dict.items() if any(substring in sequence for substring in substrings)]\n",
    "\n",
    "\n",
    "    files = [f for f in os.listdir(folder_path) if os.path.isfile(os.path.join(folder_path, f))]\n",
    "    substring_dict_sample = {\"TreatmentA\": [\"TreatmentA\"], \"TreatmentB\": [\"TreatmentB\"], \"TreatmentC\": [\"TreatmentC\"],}\n",
    "\n",
    "\n",
    "    file_name = []\n",
    "    sample_group = []\n",
    "\n",
    "\n",
    "    cast_spectra = []\n",
    "\n",
    "\n",
    "\n",
    "    for raw_name in files:\n",
    "        raw = RawFile(raw_name)\n",
    "        print(raw_name)\n",
    "        data_intensities = [0]*13690\n",
    "        file_name = raw_name\n",
    "        sample_group = find_matching_keys(raw_name, substring_dict_sample)[0]\n",
    "\n",
    "        for i in tqdm(range(1, raw.number_of_scans), desc=\"Processing scans\", ncols=100):\n",
    "            raw_scan = Scan.from_file(raw._raw_file_access, scan_number=i)\n",
    "\n",
    "            if str(helper_regex(raw_scan.scan_type)) == 'ms':\n",
    "                \n",
    "                scan_masses = raw_scan.preferred_masses\n",
    "                scan_intensities = raw_scan.preferred_intensities\n",
    "\n",
    "                for j in range(0,len(scan_masses)):\n",
    "                    index = int(round(scan_masses[j], 2)*10)\n",
    "                    if index > 6000 and index < 19360:\n",
    "                        data_intensities[index-6000] = scan_intensities[j] + data_intensities[index-6000]\n",
    "\n",
    "        cast_spectra.append(data_intensities)\n",
    "        scan_dict = {'sample_name': file_name, 'group_name': sample_group, 'cast spectra': cast_spectra}\n",
    "        df = pd.DataFrame(scan_dict)\n",
    "        df.to_csv(f\"{file_name}.csv\")\n",
    "\n",
    "    return()\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1ecc0be",
   "metadata": {},
   "source": [
    "Doing the actual data aggregation and saving each data in a separate CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e6a33e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# wholeCasting: per RAW file -> single-row CSV with cast spectrum expanded\n",
    "# ------------------------------------------------------------\n",
    "def wholeCasting(folder_path, cast_path):\n",
    "    \"\"\"\n",
    "    For each .raw file in folder_path:\n",
    "      - Sum casted MS1 intensities across all MS1 scans into a fixed grid\n",
    "      - Expand the summed vector into columns\n",
    "      - Save ONE CSV per file (named after the RAW filename) into cast_path\n",
    "\n",
    "    Output columns:\n",
    "      ['sample_name', 'group_name', 'cast_00000', 'cast_00001', ..., 'cast_13689']\n",
    "    \"\"\"\n",
    "    # ---- Helpers ----\n",
    "    def helper_regex(text):\n",
    "        # Extract token after \"Full\", e.g. \"Full ms\"\n",
    "        m = re.search(r\"Full\\s+(\\w+)\", str(text))\n",
    "        return m.group(1).lower() if m else None\n",
    "\n",
    "    def find_matching_keys(sequence: str, substring_dict: dict) -> str:\n",
    "        for key, subs in substring_dict.items():\n",
    "            if any(s in sequence for s in subs):\n",
    "                return key\n",
    "        return \"Unknown\"\n",
    "\n",
    "    def safe_stem(name: str) -> str:\n",
    "        # Drop extension and replace Windows-illegal filename chars\n",
    "        stem = os.path.splitext(str(name))[0]\n",
    "        return re.sub(r'[<>:\"/\\\\|?*]+', '_', stem)\n",
    "\n",
    "    # ---- Config: match your original grid logic ----\n",
    "    CAST_MIN = 6000\n",
    "    CAST_MAX = 19360\n",
    "    N_BINS   = 13690  # kept from your code\n",
    "    CAST_COLS = [f\"cast_{i:05d}\" for i in range(N_BINS)]\n",
    "\n",
    "    # ---- I/O setup ----\n",
    "    folder_path = os.path.abspath(folder_path)\n",
    "    out_dir = os.path.abspath(cast_path)  # treat cast_path as output directory\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "    # Only .raw files in folder\n",
    "    files = [f for f in os.listdir(folder_path)\n",
    "             if os.path.isfile(os.path.join(folder_path, f)) and f.lower().endswith(\".raw\")]\n",
    "\n",
    "    # Group inference from filename\n",
    "    substring_dict_sample = {\n",
    "        \"TreatmentA\": [\"TreatmentA\"],\n",
    "        \"TreatmentB\": [\"TreatmentB\"],\n",
    "        \"TreatmentC\": [\"TreatmentC\"],\n",
    "    }\n",
    "\n",
    "    # ---- Process each RAW file ----\n",
    "    for raw_name in files:\n",
    "        raw_path = os.path.join(folder_path, raw_name)\n",
    "        print(f\"Processing: {raw_name}\")\n",
    "\n",
    "        # Open RAW (uses your environment's RawFile/Scan)\n",
    "        try:\n",
    "            raw = RawFile(raw_path)\n",
    "        except Exception as e:\n",
    "            print(f\"[WARN] Skipping {raw_name}: cannot open RAW ({e})\")\n",
    "            continue\n",
    "\n",
    "        # One accumulator per file (single-row output)\n",
    "        summed = np.zeros(N_BINS, dtype=np.float32)\n",
    "\n",
    "        # Infer group from filename\n",
    "        group_label = find_matching_keys(raw_name, substring_dict_sample)\n",
    "\n",
    "        # Iterate scans and accumulate MS1\n",
    "        n_scans = getattr(raw, \"number_of_scans\", 0)\n",
    "        for i in tqdm(range(1, n_scans), desc=f\"Summing MS1 {raw_name}\", ncols=100):\n",
    "            try:\n",
    "                raw_scan = Scan.from_file(raw._raw_file_access, scan_number=i)\n",
    "            except Exception:\n",
    "                continue\n",
    "\n",
    "            if helper_regex(raw_scan.scan_type) != \"ms\":\n",
    "                continue\n",
    "\n",
    "            masses = raw_scan.preferred_masses\n",
    "            intens = raw_scan.preferred_intensities\n",
    "            if not masses or not intens:\n",
    "                continue\n",
    "\n",
    "            # ---- Vectorized cast & add ----\n",
    "            m = np.asarray(masses, dtype=np.float64)\n",
    "            y = np.asarray(intens, dtype=np.float32)\n",
    "            # idx = int(round(m,2) * 10)  (fast equivalent)\n",
    "            idx = (np.rint(m * 100.0).astype(np.int64) // 10)\n",
    "\n",
    "            mask = (idx > CAST_MIN) & (idx < CAST_MAX)\n",
    "            if not np.any(mask):\n",
    "                continue\n",
    "\n",
    "            bin_idx = idx[mask] - CAST_MIN\n",
    "            # guard against any stray values vs N_BINS\n",
    "            good = (bin_idx >= 0) & (bin_idx < N_BINS)\n",
    "            if not np.any(good):\n",
    "                continue\n",
    "\n",
    "            np.add.at(summed, bin_idx[good], y[mask][good])\n",
    "\n",
    "        # ---- Build single-row DataFrame and save ----\n",
    "        row_df = pd.DataFrame(\n",
    "            [[raw_name, group_label] + summed.tolist()],\n",
    "            columns=[\"sample_name\", \"group_name\"] + CAST_COLS\n",
    "        )\n",
    "\n",
    "        out_path = os.path.join(out_dir, f\"{safe_stem(raw_name)}.csv\")\n",
    "        row_df.to_csv(out_path, index=False)\n",
    "        print(\"Saved:\", out_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a6d475",
   "metadata": {},
   "outputs": [],
   "source": [
    "wholeCasting(\"D:/TreatmentA/\",'D:/databank/databank')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4be7cf2",
   "metadata": {},
   "source": [
    "Doing the casting with 10 minute time intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f78f9edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# wholeCasting: per RAW file -> multi-row CSV (10-min RT bins)\n",
    "# ------------------------------------------------------------\n",
    "def wholeCasting(folder_path, cast_path, rt_bin_minutes: float = 10.0):\n",
    "    \"\"\"\n",
    "    For each .raw file in folder_path:\n",
    "      - Group MS1 scans by retention time (RT) bins of `rt_bin_minutes` (default 10.0)\n",
    "      - Sum casted MS1 intensities across scans within each RT bin into a fixed grid\n",
    "      - Expand each summed vector into columns\n",
    "      - Save ONE CSV per file into cast_path, with one row per RT bin\n",
    "\n",
    "    Output columns:\n",
    "      ['sample_name', 'group_name', 'rt_start_min', 'rt_end_min', 'rt_center_min', 'n_scans',\n",
    "       'cast_00000', 'cast_00001', ..., 'cast_13689']\n",
    "    \"\"\"\n",
    "    # ---- Helpers ----\n",
    "    def helper_regex(text):\n",
    "        # Extract token after \"Full\", e.g. \"Full ms\"\n",
    "        m = re.search(r\"Full\\s+(\\w+)\", str(text))\n",
    "        return m.group(1).lower() if m else None\n",
    "\n",
    "    def find_matching_keys(sequence: str, substring_dict: dict) -> str:\n",
    "        for key, subs in substring_dict.items():\n",
    "            if any(s in sequence for s in subs):\n",
    "                return key\n",
    "        return \"Unknown\"\n",
    "\n",
    "    def safe_stem(name: str) -> str:\n",
    "        # Drop extension and replace Windows-illegal filename chars\n",
    "        stem = os.path.splitext(str(name))[0]\n",
    "        return re.sub(r'[<>:\"/\\\\|?*]+', '_', stem)\n",
    "\n",
    "    # ---- Cast grid config (matches previous logic) ----\n",
    "    # Mass/m/z domain ~ 600.0 .. 1969.0 in 0.1 steps  -> 13,690 bins\n",
    "    CAST_MIN = 6000   # corresponds to 600.0 when /10\n",
    "    CAST_MAX = 19690  # corresponds to 1969.0 when /10\n",
    "    N_BINS   = 13690\n",
    "    CAST_COLS = [f\"cast_{i:05d}\" for i in range(N_BINS)]\n",
    "\n",
    "    # ---- I/O setup ----\n",
    "    folder_path = os.path.abspath(folder_path)\n",
    "    out_dir = os.path.abspath(cast_path)  # treat cast_path as output directory\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "    # Only .raw files in folder\n",
    "    files = [f for f in os.listdir(folder_path)\n",
    "             if os.path.isfile(os.path.join(folder_path, f)) and f.lower().endswith(\".raw\")]\n",
    "\n",
    "    # Group inference from filename\n",
    "    substring_dict_sample = {\n",
    "        \"TreatmentA\": [\"TreatmentA\"],\n",
    "        \"TreatmentB\": [\"TreatmentB\"],\n",
    "        \"TreatmentC\": [\"TreatmentC\"],\n",
    "    }\n",
    "\n",
    "    # ---- Process each RAW file ----\n",
    "    for raw_name in files:\n",
    "        raw_path = os.path.join(folder_path, raw_name)\n",
    "        print(f\"\\nProcessing: {raw_name}\")\n",
    "\n",
    "        # Open RAW (uses your environment's RawFile/Scan)\n",
    "        try:\n",
    "            raw = RawFile(raw_path)\n",
    "        except Exception as e:\n",
    "            print(f\"[WARN] Skipping {raw_name}: cannot open RAW ({e})\")\n",
    "            continue\n",
    "\n",
    "        group_label = find_matching_keys(raw_name, substring_dict_sample)\n",
    "\n",
    "        # Per-RT-bin accumulators: dict[bin_index] -> (sum_vector, n_scans, rt_min_seen, rt_max_seen)\n",
    "        bin_sums = {}\n",
    "        bin_counts = defaultdict(int)\n",
    "        bin_rt_min = {}\n",
    "        bin_rt_max = {}\n",
    "\n",
    "        n_scans = getattr(raw, \"number_of_scans\", 0)\n",
    "        for i in tqdm(range(1, n_scans), desc=f\"Binning MS1 by RT ({rt_bin_minutes} min) - {raw_name}\", ncols=100):\n",
    "            try:\n",
    "                raw_scan = Scan.from_file(raw._raw_file_access, scan_number=i)\n",
    "            except Exception:\n",
    "                continue\n",
    "\n",
    "            if helper_regex(raw_scan.scan_type) != \"ms\":\n",
    "                continue\n",
    "\n",
    "            # Retention time (assumed minutes)\n",
    "            rt = raw_scan.scan_statistics.start_time\n",
    "            if rt is None:\n",
    "                continue\n",
    "            # Determine RT bin index\n",
    "            bin_idx = int(math.floor(rt / rt_bin_minutes))\n",
    "\n",
    "            masses = raw_scan.preferred_masses\n",
    "            intens = raw_scan.preferred_intensities\n",
    "            if not masses or not intens:\n",
    "                # still track counts/rt bounds for completeness\n",
    "                bin_counts[bin_idx] += 1\n",
    "                if bin_idx not in bin_rt_min or rt < bin_rt_min[bin_idx]:\n",
    "                    bin_rt_min[bin_idx] = rt\n",
    "                if bin_idx not in bin_rt_max or rt > bin_rt_max[bin_idx]:\n",
    "                    bin_rt_max[bin_idx] = rt\n",
    "                continue\n",
    "\n",
    "            # ---- Vectorized cast for this scan ----\n",
    "            m = np.asarray(masses, dtype=np.float64)\n",
    "            y = np.asarray(intens, dtype=np.float32)\n",
    "            # Fast ~ int(round(m,2)*10)\n",
    "            idx = (np.rint(m * 100.0).astype(np.int64) // 10)\n",
    "\n",
    "            mask = (idx > CAST_MIN) & (idx < CAST_MAX)\n",
    "            if not np.any(mask):\n",
    "                # still track counts/rt bounds\n",
    "                bin_counts[bin_idx] += 1\n",
    "                if bin_idx not in bin_rt_min or rt < bin_rt_min[bin_idx]:\n",
    "                    bin_rt_min[bin_idx] = rt\n",
    "                if bin_idx not in bin_rt_max or rt > bin_rt_max[bin_idx]:\n",
    "                    bin_rt_max[bin_idx] = rt\n",
    "                continue\n",
    "\n",
    "            bin_idx_vec = idx[mask] - CAST_MIN\n",
    "            good = (bin_idx_vec >= 0) & (bin_idx_vec < N_BINS)\n",
    "            if not np.any(good):\n",
    "                bin_counts[bin_idx] += 1\n",
    "                if bin_idx not in bin_rt_min or rt < bin_rt_min[bin_idx]:\n",
    "                    bin_rt_min[bin_idx] = rt\n",
    "                if bin_idx not in bin_rt_max or rt > bin_rt_max[bin_idx]:\n",
    "                    bin_rt_max[bin_idx] = rt\n",
    "                continue\n",
    "\n",
    "            # Lazily allocate the sum vector for this RT bin\n",
    "            if bin_idx not in bin_sums:\n",
    "                bin_sums[bin_idx] = np.zeros(N_BINS, dtype=np.float32)\n",
    "\n",
    "            np.add.at(bin_sums[bin_idx], bin_idx_vec[good], y[mask][good])\n",
    "\n",
    "            # Bookkeeping\n",
    "            bin_counts[bin_idx] += 1\n",
    "            if bin_idx not in bin_rt_min or rt < bin_rt_min[bin_idx]:\n",
    "                bin_rt_min[bin_idx] = rt\n",
    "            if bin_idx not in bin_rt_max or rt > bin_rt_max[bin_idx]:\n",
    "                bin_rt_max[bin_idx] = rt\n",
    "\n",
    "        # ---- Build DataFrame with one row per RT bin (only bins with scans) ----\n",
    "        rows = []\n",
    "        # Use union of bins encountered (from counts), not only those with signal\n",
    "        all_bins = sorted(set(list(bin_counts.keys()) + list(bin_sums.keys())))\n",
    "        for b in all_bins:\n",
    "            # Skip bins with zero scans just in case\n",
    "            if bin_counts[b] <= 0:\n",
    "                continue\n",
    "            rt_start = b * rt_bin_minutes\n",
    "            rt_end   = (b + 1) * rt_bin_minutes\n",
    "            # Prefer observed min/max within the bin if available\n",
    "            rt_obs_min = bin_rt_min.get(b, rt_start)\n",
    "            rt_obs_max = bin_rt_max.get(b, rt_end)\n",
    "            rt_center  = int(0.5 * (rt_obs_min + rt_obs_max))\n",
    "\n",
    "            summed_vec = bin_sums.get(b, np.zeros(N_BINS, dtype=np.float32))\n",
    "\n",
    "            meta = [raw_name, group_label, rt_obs_min, rt_obs_max, rt_center, int(bin_counts[b])]\n",
    "            rows.append(meta + summed_vec.tolist())\n",
    "\n",
    "        if not rows:\n",
    "            print(f\"[WARN] No MS1 bins produced for {raw_name}. Skipping CSV.\")\n",
    "            continue\n",
    "\n",
    "        col_meta = [\"sample_name\", \"group_name\", \"rt_start_min\", \"rt_end_min\", \"rt_center_min\", \"n_scans\"]\n",
    "        df = pd.DataFrame(rows, columns=col_meta + CAST_COLS)\n",
    "\n",
    "        out_path = os.path.join(out_dir, f\"{safe_stem(raw_name)}.csv\")\n",
    "        df.to_csv(out_path, index=False)\n",
    "        print(\"Saved:\", out_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "430148b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "wholeCasting(\"D:/TreatmentA/\",'D:/databank/databank')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4213f1f9",
   "metadata": {},
   "source": [
    "Combine all the csv files into a single file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52478cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import shutil\n",
    "import tempfile\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def _infer_dtype_map(ref_cols):\n",
    "    dtype_map = {}\n",
    "    cast_cols = [c for c in ref_cols if str(c).startswith(\"cast_\")]\n",
    "    for c in cast_cols:\n",
    "        dtype_map[c] = np.float32\n",
    "    for c in (\"rt_start_min\", \"rt_end_min\", \"rt_center_min\"):\n",
    "        if c in ref_cols:\n",
    "            dtype_map[c] = np.float32\n",
    "    if \"n_scans\" in ref_cols:\n",
    "        dtype_map[\"n_scans\"] = \"Int32\"  # pandas nullable int\n",
    "    for c in (\"sample_name\", \"group_name\"):\n",
    "        if c in ref_cols:\n",
    "            dtype_map[c] = \"string\"\n",
    "    return dtype_map\n",
    "\n",
    "def _load_and_align(path: str, ref_cols, dtype_map):\n",
    "    try:\n",
    "        df = pd.read_csv(path, dtype=dtype_map)\n",
    "    except Exception:\n",
    "        df = pd.read_csv(path)\n",
    "\n",
    "    # Add missing columns with sensible defaults\n",
    "    missing = [c for c in ref_cols if c not in df.columns]\n",
    "    for c in missing:\n",
    "        if str(c).startswith(\"cast_\"):\n",
    "            df[c] = np.float32(0.0)\n",
    "        elif c in (\"rt_start_min\", \"rt_end_min\", \"rt_center_min\"):\n",
    "            df[c] = np.float32(np.nan)\n",
    "        elif c == \"n_scans\":\n",
    "            df[c] = pd.Series([pd.NA] * len(df), dtype=\"Int32\")\n",
    "        elif c == \"sample_name\":\n",
    "            df[c] = os.path.basename(path)\n",
    "        elif c == \"group_name\":\n",
    "            df[c] = \"Unknown\"\n",
    "        else:\n",
    "            df[c] = pd.NA\n",
    "\n",
    "    # Drop extras not in the reference and reorder\n",
    "    extras = [c for c in df.columns if c not in ref_cols]\n",
    "    if extras:\n",
    "        df = df.drop(columns=extras)\n",
    "    df = df[ref_cols]\n",
    "\n",
    "    # Ensure cast_* are float32\n",
    "    for c in ref_cols:\n",
    "        if str(c).startswith(\"cast_\"):\n",
    "            df[c] = pd.to_numeric(df[c], errors=\"coerce\").astype(\"float32\")\n",
    "    return df\n",
    "\n",
    "def combine_cast_outputs(\n",
    "    input_dir: str,\n",
    "    output_csv: str,\n",
    "    recursive: bool = False,\n",
    "    streaming: bool = True,\n",
    "    write_parquet: bool = False,\n",
    "):\n",
    "    \"\"\"\n",
    "    Combine all per-RAW CSVs (from wholeCasting) into one CSV.\n",
    "    - Aligns schemas to the first file.\n",
    "    - Streaming append by default (low memory).\n",
    "    \"\"\"\n",
    "    pattern = \"**/*.csv\" if recursive else \"*.csv\"\n",
    "    files = sorted(glob.glob(os.path.join(input_dir, pattern), recursive=recursive))\n",
    "    if not files:\n",
    "        print(f\"[WARN] No CSV files found in: {input_dir}\")\n",
    "        return\n",
    "\n",
    "    # Reference schema\n",
    "    ref_header = pd.read_csv(files[0], nrows=0)\n",
    "    ref_cols = list(ref_header.columns)\n",
    "    dtype_map = _infer_dtype_map(ref_cols)\n",
    "\n",
    "    out_dir = os.path.dirname(os.path.abspath(output_csv)) or \".\"\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "    # temp file for safer write\n",
    "    tmp_fd, tmp_path = tempfile.mkstemp(dir=out_dir, prefix=\".combine_tmp_\", suffix=\".csv\")\n",
    "    os.close(tmp_fd)\n",
    "\n",
    "    try:\n",
    "        if streaming:\n",
    "            # write header once\n",
    "            pd.DataFrame(columns=ref_cols).to_csv(tmp_path, index=False)\n",
    "            total_rows = 0\n",
    "            for i, f in enumerate(files, 1):\n",
    "                df = _load_and_align(f, ref_cols, dtype_map)\n",
    "                df.to_csv(tmp_path, mode=\"a\", index=False, header=False)\n",
    "                total_rows += len(df)\n",
    "                if i % 10 == 0 or i == len(files):\n",
    "                    print(f\"[INFO] Appended {len(df):6d} rows from {os.path.basename(f)} \"\n",
    "                          f\"({i}/{len(files)}) â†’ total {total_rows}\")\n",
    "            print(f\"[OK] Combined CSV rows: {total_rows}\")\n",
    "        else:\n",
    "            # in-memory (lets you sort)\n",
    "            parts = [_load_and_align(f, ref_cols, dtype_map) for f in files]\n",
    "            big = pd.concat(parts, ignore_index=True)\n",
    "            sort_cols = [c for c in (\"sample_name\", \"rt_start_min\", \"rt_center_min\") if c in big.columns]\n",
    "            if sort_cols:\n",
    "                big = big.sort_values(sort_cols, kind=\"mergesort\", ignore_index=True)\n",
    "            big.to_csv(tmp_path, index=False)\n",
    "            print(f\"[OK] Combined CSV rows: {len(big)}\")\n",
    "\n",
    "        # move temp to final\n",
    "        if os.path.exists(output_csv):\n",
    "            os.remove(output_csv)\n",
    "        shutil.move(tmp_path, output_csv)\n",
    "\n",
    "        if write_parquet:\n",
    "            big2 = pd.read_csv(output_csv, dtype=dtype_map)\n",
    "            pq_path = os.path.splitext(output_csv)[0] + \".parquet\"\n",
    "            big2.to_parquet(pq_path, index=False)\n",
    "            print(f\"[OK] Parquet written: {pq_path}\")\n",
    "\n",
    "        print(f\"[SAVED] {output_csv}\")\n",
    "\n",
    "    finally:\n",
    "        if os.path.exists(tmp_path):\n",
    "            try:\n",
    "                os.remove(tmp_path)\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "# ====== EDIT THESE PATHS AND RUN ======\n",
    "combine_cast_outputs(\n",
    "    input_dir=r\"D:\\databank\\databank\",                        # folder with your per-RAW CSVs\n",
    "    output_csv=r\"D:\\ALL_casts_combined.csv\",  # combined output path\n",
    "    recursive=False,       # True if CSVs are in subfolders\n",
    "    streaming=True,        # memory-friendly\n",
    "    write_parquet=False    # set True to also write Parquet\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "casting",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
