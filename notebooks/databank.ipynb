{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9612f8ce",
   "metadata": {},
   "source": [
    "Receives Raw files and TDReport and generates MS1 and MS2 datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bb729ffc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MS1 CSV: F:\\binary\\ms1_aggregate_per_sample.csv\n",
      "MS2 CSV: F:\\binary\\ms2_per_scan.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing scans: 100%|████████████████████████████████████| 19307/19307 [00:00<00:00, 22997.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set()\n",
      "Final MS2+IDs CSV: F:\\binary\\ms2_with_ids.csv\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "End-to-end:\n",
    "1) Process Thermo .raw in a folder -> MS1 and MS2 CSVs\n",
    "2) Make MS2 compatible with ID_import (scan column, sample_name alignment)\n",
    "3) Run ID_import to attach IDs from a tdportal table\n",
    "\n",
    "Requires: fisher-py, numpy, pandas, tqdm\n",
    "  pip install fisher-py numpy pandas tqdm\n",
    "\"\"\"\n",
    "\n",
    "import os, glob, re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import List, Tuple, Dict\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ---- fisher-py imports (RAW access) ----\n",
    "from fisher_py.data.business import Scan\n",
    "from fisher_py import RawFile\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Binning configuration\n",
    "# -----------------------------\n",
    "# MS1: 0.1 m/z bins, 600.0 .. ~1968.9 (index = round(mz * 10))\n",
    "MS1_MIN_IDX, MS1_LEN = 6000, 13690\n",
    "MS1_MAX_EXC = MS1_MIN_IDX + MS1_LEN\n",
    "\n",
    "# MS2: 1.0 m/z bins, 400 .. 1999 (index = round(mz))\n",
    "MS2_MIN_IDX, MS2_LEN = 400, 1600\n",
    "MS2_MAX_EXC = MS2_MIN_IDX + MS2_LEN\n",
    "\n",
    "# Normalize each MS2 scan to its max intensity (per-scan TIC-like)\n",
    "MS2_NORMALIZE = True\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Helpers\n",
    "# -----------------------------\n",
    "def _scan_type_label(text: str) -> str:\n",
    "    m = re.search(r\"Full\\s+(\\w+)\", str(text), flags=re.IGNORECASE)\n",
    "    return m.group(1).lower() if m else \"\"\n",
    "\n",
    "def _as_float_array(x):\n",
    "    if x is None:\n",
    "        return np.array([], dtype=float)\n",
    "    a = np.asarray(x)\n",
    "    return a.astype(float, copy=False) if a.size else np.array([], dtype=float)\n",
    "\n",
    "def _raw_files_in(folder: str) -> List[str]:\n",
    "    folder = os.path.abspath(folder)\n",
    "    if not os.path.isdir(folder):\n",
    "        raise FileNotFoundError(f'Folder not found: \"{folder}\"')\n",
    "    out = sorted(set(glob.glob(os.path.join(folder, \"*.raw\")) +\n",
    "                     glob.glob(os.path.join(folder, \"*.RAW\"))))\n",
    "    if not out:\n",
    "        raise FileNotFoundError(f'No .raw files in: {folder}')\n",
    "    return out\n",
    "\n",
    "def _precursor_from_scan(raw_scan) -> float:\n",
    "    for attr in (\"precursor_mz\", \"master_precursor_mz\", \"isolation_mz\"):\n",
    "        if hasattr(raw_scan, attr):\n",
    "            try:\n",
    "                return float(getattr(raw_scan, attr))\n",
    "            except Exception:\n",
    "                pass\n",
    "    try:\n",
    "        nums = re.findall(r'\\d+\\.\\d+', str(raw_scan.scan_type))\n",
    "        return float(nums[1]) if len(nums) > 1 else np.nan\n",
    "    except Exception:\n",
    "        return np.nan\n",
    "\n",
    "def _make_cast_headers(prefix: str, length: int) -> List[str]:\n",
    "    width = max(5, len(str(length)))\n",
    "    return [f\"{prefix}_{i:0{width}d}\" for i in range(length)]\n",
    "\n",
    "def _strip_ext_basename(x: str) -> str:\n",
    "    base = os.path.basename(str(x))\n",
    "    return base[:-4] if base.lower().endswith(\".raw\") else base\n",
    "\n",
    "def _norm_key(x: str) -> str:\n",
    "    \"\"\"Case-insensitive, extension-agnostic normalization.\"\"\"\n",
    "    return _strip_ext_basename(x).lower()\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# RAW processing\n",
    "# -----------------------------\n",
    "def process_raw_folder(raw_folder: str,\n",
    "                       out_ms1_csv: str,\n",
    "                       out_ms2_csv: str) -> Tuple[str, str]:\n",
    "    \"\"\"\n",
    "    1) MS1 CSV: one row per sample_name; columns: sample_name, cast_.....\n",
    "    2) MS2 CSV: one row per MS2 scan; columns: sample_name, scan_number, retention_time, precursor_mz, cast_...\n",
    "\n",
    "    Returns (ms1_csv_path, ms2_csv_path).\n",
    "    \"\"\"\n",
    "    raw_paths = _raw_files_in(raw_folder)\n",
    "\n",
    "    # MS1 accumulation per sample\n",
    "    ms1_acc: Dict[str, np.ndarray] = {}\n",
    "\n",
    "    # MS2 rows\n",
    "    ms2_rows: List[List[float]] = []\n",
    "\n",
    "    for raw_path in raw_paths:\n",
    "        sample_name = os.path.splitext(os.path.basename(raw_path))[0]\n",
    "        if sample_name not in ms1_acc:\n",
    "            ms1_acc[sample_name] = np.zeros(MS1_LEN, dtype=np.float32)\n",
    "\n",
    "        try:\n",
    "            raw = RawFile(raw_path)\n",
    "        except Exception as e:\n",
    "            print(f'[skip] Cannot open RAW: {raw_path} ({e})')\n",
    "            continue\n",
    "\n",
    "        total_scans = int(getattr(raw, \"number_of_scans\", 0) or 0)\n",
    "\n",
    "        for scan_number in range(1, total_scans + 1):\n",
    "            try:\n",
    "                raw_scan = Scan.from_file(raw._raw_file_access, scan_number=scan_number)\n",
    "            except Exception:\n",
    "                continue\n",
    "\n",
    "            stype = _scan_type_label(raw_scan.scan_type)\n",
    "            sc_num = getattr(raw_scan.scan_statistics, \"scan_number\", scan_number)\n",
    "            try:\n",
    "                rt = float(raw.get_retention_time_from_scan_number(sc_num))\n",
    "            except Exception:\n",
    "                rt = np.nan\n",
    "\n",
    "            mz = _as_float_array(getattr(raw_scan, \"preferred_masses\", None))\n",
    "            it = _as_float_array(getattr(raw_scan, \"preferred_intensities\", None))\n",
    "            if mz.size == 0 or it.size == 0:\n",
    "                continue\n",
    "\n",
    "            if stype == \"ms\":\n",
    "                idx = np.rint(mz * 10.0).astype(np.int32)\n",
    "                mask = (idx >= MS1_MIN_IDX) & (idx < MS1_MAX_EXC)\n",
    "                if mask.any():\n",
    "                    np.add.at(ms1_acc[sample_name],\n",
    "                              idx[mask] - MS1_MIN_IDX,\n",
    "                              it[mask].astype(np.float32, copy=False))\n",
    "\n",
    "            elif stype == \"ms2\":\n",
    "                idx = np.rint(mz).astype(np.int32)\n",
    "                mask = (idx >= MS2_MIN_IDX) & (idx < MS2_MAX_EXC)\n",
    "                if not mask.any():\n",
    "                    continue\n",
    "                v = np.zeros(MS2_LEN, dtype=np.float32)\n",
    "                np.add.at(v, idx[mask] - MS2_MIN_IDX, it[mask].astype(np.float32, copy=False))\n",
    "                if MS2_NORMALIZE:\n",
    "                    vmax = float(v.max())\n",
    "                    if vmax > 0:\n",
    "                        v /= vmax\n",
    "                prec = _precursor_from_scan(raw_scan)\n",
    "                ms2_rows.append([sample_name, int(sc_num), float(rt), float(prec)] + v.astype(np.float32).tolist())\n",
    "\n",
    "        try:\n",
    "            raw.dispose()\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    # Write MS1 CSV\n",
    "    ms1_headers = [\"sample_name\"] + _make_cast_headers(\"cast\", MS1_LEN)\n",
    "    ms1_df = pd.DataFrame([[sn] + vec.tolist() for sn, vec in ms1_acc.items()], columns=ms1_headers)\n",
    "    os.makedirs(os.path.dirname(os.path.abspath(out_ms1_csv)) or \".\", exist_ok=True)\n",
    "    ms1_df.to_csv(out_ms1_csv, index=False)\n",
    "\n",
    "    # Write MS2 CSV\n",
    "    ms2_headers = [\"sample_name\", \"scan_number\", \"retention_time\", \"precursor_mz\"] + _make_cast_headers(\"cast\", MS2_LEN)\n",
    "    ms2_df = pd.DataFrame(ms2_rows, columns=ms2_headers)\n",
    "    os.makedirs(os.path.dirname(os.path.abspath(out_ms2_csv)) or \".\", exist_ok=True)\n",
    "    ms2_df.to_csv(out_ms2_csv, index=False)\n",
    "\n",
    "    return out_ms1_csv, out_ms2_csv\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Make MS2 compatible with ID_import\n",
    "# -----------------------------\n",
    "def prepare_ms2_for_id_import(ms2_df: pd.DataFrame, tdportal_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    - Renames 'scan_number' -> 'scan'\n",
    "    - Coerces 'scan' to int\n",
    "    - Aligns 'sample_name' to EXACT keys present in tdportal['File Name'] (case/extension robust)\n",
    "    \"\"\"\n",
    "    if \"scan_number\" in ms2_df.columns:\n",
    "        ms2_df = ms2_df.rename(columns={\"scan_number\": \"scan\"})\n",
    "\n",
    "    # enforce int scans\n",
    "    ms2_df[\"scan\"] = pd.to_numeric(ms2_df[\"scan\"], errors=\"coerce\").astype(\"Int64\")\n",
    "    ms2_df = ms2_df.dropna(subset=[\"scan\"]).copy()\n",
    "    ms2_df[\"scan\"] = ms2_df[\"scan\"].astype(int)\n",
    "\n",
    "    # Build a resolver from normalized forms -> canonical td key\n",
    "    if \"File Name\" not in tdportal_df.columns:\n",
    "        raise KeyError(\"tdportal must contain 'File Name' column.\")\n",
    "    td_names = list(tdportal_df[\"File Name\"].astype(str).values)\n",
    "\n",
    "    resolver: Dict[str, str] = {}\n",
    "    for name in td_names:\n",
    "        resolver[_norm_key(name)] = name  # canonical map\n",
    "\n",
    "    # map ms2 sample_name to the canonical td key if possible\n",
    "    def _resolve_sample(s: str) -> str:\n",
    "        k = _norm_key(s)\n",
    "        return resolver.get(k, s)  # if unmatched, keep original (ID_import will handle KeyError)\n",
    "\n",
    "    ms2_df[\"sample_name\"] = ms2_df[\"sample_name\"].astype(str).map(_resolve_sample)\n",
    "\n",
    "    return ms2_df\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Your original ID_import (unchanged)\n",
    "# -----------------------------\n",
    "def ID_import(tdportal, databank, cast_path):\n",
    "    def str_to_int(st):\n",
    "        internal = []\n",
    "        digits = re.findall(r'\\d+', st)\n",
    "        for i in range(0, len(digits)):\n",
    "            internal.append(int(digits[i]))\n",
    "        return(internal)\n",
    "\n",
    "    scan_number = [0]*len(tdportal['File Name'])\n",
    "    td_samples = []\n",
    "\n",
    "    for i in range(0, len(tdportal['File Name'])):\n",
    "        scan_number[i] = str_to_int(str(tdportal['Fragment Scans'][i]))\n",
    "        if tdportal['File Name'][i] not in td_samples:\n",
    "            td_samples.append(tdportal['File Name'][i])\n",
    "\n",
    "    my_dic_scan = {key: [] for key in td_samples}\n",
    "    my_dic_index = {key: [] for key in td_samples}\n",
    "\n",
    "    for i in range(0, len(tdportal['File Name'])):\n",
    "        my_dic_scan[tdportal['File Name'][i]].append(scan_number[i])\n",
    "        my_dic_index[tdportal['File Name'][i]].append([i]*len(scan_number[i]))\n",
    "\n",
    "    for i in range(0, len(td_samples)):\n",
    "        nested_list = my_dic_scan[td_samples[i]]\n",
    "        flat_list = []\n",
    "        for item in nested_list:\n",
    "            if isinstance(item, list):\n",
    "                flat_list.extend(item)\n",
    "            else:\n",
    "                flat_list.append(item)\n",
    "        my_dic_scan[td_samples[i]] = [elem for sublist in flat_list for elem in (sublist if isinstance(sublist, list) else [sublist])]\n",
    "\n",
    "    for i in range(0, len(td_samples)):\n",
    "        nested_list = my_dic_index[td_samples[i]]\n",
    "        flat_list = []\n",
    "        for item in nested_list:\n",
    "            if isinstance(item, list):\n",
    "                flat_list.extend(item)\n",
    "            else:\n",
    "                flat_list.append(item)\n",
    "        my_dic_index[td_samples[i]] = [elem for sublist in flat_list for elem in (sublist if isinstance(sublist, list) else [sublist])]\n",
    "\n",
    "    sequence, MASS, Accession, missing, PFR = [], [], [], [], []\n",
    "\n",
    "    for i in tqdm(range(len(databank['scan'])), desc=\"Processing scans\", ncols=100):\n",
    "        try:\n",
    "            sample = databank['sample_name'][i]\n",
    "            scan   = databank['scan'][i]\n",
    "\n",
    "            if scan in my_dic_scan[sample]:\n",
    "                tt = my_dic_index[sample][my_dic_scan[sample].index(scan)]\n",
    "                sequence.append(tdportal.at[tt, 'Sequence'])\n",
    "                MASS.append(tdportal.at[tt, 'Average Mass'])\n",
    "                Accession.append(tdportal.at[tt, 'Accession'])\n",
    "                PFR.append(tdportal.at[tt, 'PFR'])\n",
    "            else:\n",
    "                sequence.append(None)\n",
    "                MASS.append(None)\n",
    "                Accession.append(None)\n",
    "                PFR.append(None)\n",
    "\n",
    "        except KeyError as e:\n",
    "            missing.append(sample)\n",
    "            sequence.append(None)\n",
    "            MASS.append(None)\n",
    "            Accession.append(None)\n",
    "            PFR.append(None)\n",
    "\n",
    "        except Exception as e:\n",
    "            sequence.append(None)\n",
    "            MASS.append(None)\n",
    "            Accession.append(None)\n",
    "            PFR.append(None)\n",
    "\n",
    "    print(set(missing))\n",
    "\n",
    "    databank['sequence'] = sequence\n",
    "    databank['MASS'] = MASS\n",
    "    databank['Accession'] = Accession\n",
    "    databank['PFR'] = PFR\n",
    "\n",
    "    databank = pd.DataFrame(databank)\n",
    "    databank.to_csv(cast_path, index=False)\n",
    "    return()\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Orchestrator\n",
    "# -----------------------------\n",
    "def run_all(raw_folder: str,\n",
    "            out_ms1_csv: str,\n",
    "            out_ms2_csv: str,\n",
    "            tdportal_csv: str,\n",
    "            cast_out_csv: str):\n",
    "    # 1) Process RAWs -> MS1 & MS2\n",
    "    ms1_csv, ms2_csv = process_raw_folder(raw_folder, out_ms1_csv, out_ms2_csv)\n",
    "    print(\"MS1 CSV:\", ms1_csv)\n",
    "    print(\"MS2 CSV:\", ms2_csv)\n",
    "\n",
    "    # 2) Load tdportal + ms2, make ms2 compatible\n",
    "    tdportal = pd.read_csv(tdportal_csv)\n",
    "    ms2_df = pd.read_csv(ms2_csv)\n",
    "    ms2_df = prepare_ms2_for_id_import(ms2_df, tdportal)\n",
    "\n",
    "    # 3) ID_import\n",
    "    #    Pass DataFrame (dict-like) so ID_import can index as databank['col']\n",
    "    ID_import(tdportal=tdportal, databank=ms2_df, cast_path=cast_out_csv)\n",
    "    print(\"Final MS2+IDs CSV:\", cast_out_csv)\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Example usage\n",
    "# -----------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    RAW_FOLDER   = r\"F:\\old_data\\usb1\\samples\"         # folder with .raw files\n",
    "    OUT_MS1      = r\"F:\\binary\\ms1_aggregate_per_sample.csv\"\n",
    "    OUT_MS2      = r\"F:\\binary\\ms2_per_scan.csv\"\n",
    "    TDPORTAL_CSV = r\"F:\\binary\\tdreport.csv\"    # must have 'File Name' and 'Fragment Scans'\n",
    "    CAST_OUT     = r\"F:\\binary\\ms2_with_ids.csv\"\n",
    "\n",
    "    run_all(RAW_FOLDER, OUT_MS1, OUT_MS2, TDPORTAL_CSV, CAST_OUT)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "casting",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
