{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9612f8ce",
   "metadata": {},
   "source": [
    "Receives Raw files and TDReport and generates MS1 and MS2 datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bb729ffc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MS1 CSV: F:\\binary\\final\\ms1_per_sample.csv\n",
      "MS2 CSV: F:\\binary\\final\\ms2_per_scan.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing scans: 100%|████████████████████████████████████| 19307/19307 [00:00<00:00, 25338.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set()\n",
      "Final MS2+IDs CSV: F:\\binary\\final\\ms2_per_scan_with_ids.csv\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "End-to-end:\n",
    "1) Process Thermo .raw in a folder -> MS1 and MS2 CSVs\n",
    "2) Make MS2 compatible with ID_import (scan column, sample_name alignment)\n",
    "3) Run ID_import to attach IDs from a tdportal table\n",
    "\n",
    "Requires: fisher-py, numpy, pandas, tqdm\n",
    "  pip install fisher-py numpy pandas tqdm\n",
    "\"\"\"\n",
    "\n",
    "import os, glob, re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import List, Tuple, Dict\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ---- fisher-py imports (RAW access) ----\n",
    "from fisher_py.data.business import Scan\n",
    "from fisher_py import RawFile\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Binning configuration\n",
    "# -----------------------------\n",
    "# MS1: 0.1 m/z bins, 600.0 .. ~1968.9 (index = round(mz * 10))\n",
    "MS1_MIN_IDX, MS1_LEN = 6000, 13690\n",
    "MS1_MAX_EXC = MS1_MIN_IDX + MS1_LEN\n",
    "\n",
    "# MS2: 1.0 m/z bins, 400 .. 1999 (index = round(mz))\n",
    "MS2_MIN_IDX, MS2_LEN = 400, 1600\n",
    "MS2_MAX_EXC = MS2_MIN_IDX + MS2_LEN\n",
    "\n",
    "# Normalize each MS2 scan to its max intensity (per-scan TIC-like)\n",
    "MS2_NORMALIZE = True\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Helpers\n",
    "# -----------------------------\n",
    "def _scan_type_label(text: str) -> str:\n",
    "    m = re.search(r\"Full\\s+(\\w+)\", str(text), flags=re.IGNORECASE)\n",
    "    return m.group(1).lower() if m else \"\"\n",
    "\n",
    "def _as_float_array(x):\n",
    "    if x is None:\n",
    "        return np.array([], dtype=float)\n",
    "    a = np.asarray(x)\n",
    "    return a.astype(float, copy=False) if a.size else np.array([], dtype=float)\n",
    "\n",
    "def _raw_files_in(folder: str) -> List[str]:\n",
    "    folder = os.path.abspath(folder)\n",
    "    if not os.path.isdir(folder):\n",
    "        raise FileNotFoundError(f'Folder not found: \"{folder}\"')\n",
    "    out = sorted(set(glob.glob(os.path.join(folder, \"*.raw\")) +\n",
    "                     glob.glob(os.path.join(folder, \"*.RAW\"))))\n",
    "    if not out:\n",
    "        raise FileNotFoundError(f'No .raw files in: {folder}')\n",
    "    return out\n",
    "\n",
    "def _precursor_from_scan(raw_scan) -> float:\n",
    "    for attr in (\"precursor_mz\", \"master_precursor_mz\", \"isolation_mz\"):\n",
    "        if hasattr(raw_scan, attr):\n",
    "            try:\n",
    "                return float(getattr(raw_scan, attr))\n",
    "            except Exception:\n",
    "                pass\n",
    "    try:\n",
    "        nums = re.findall(r'\\d+\\.\\d+', str(raw_scan.scan_type))\n",
    "        return float(nums[1]) if len(nums) > 1 else np.nan\n",
    "    except Exception:\n",
    "        return np.nan\n",
    "\n",
    "def _make_cast_headers(prefix: str, length: int) -> List[str]:\n",
    "    width = max(5, len(str(length)))\n",
    "    return [f\"{prefix}_{i:0{width}d}\" for i in range(length)]\n",
    "\n",
    "def _strip_ext_basename(x: str) -> str:\n",
    "    base = os.path.basename(str(x))\n",
    "    return base[:-4] if base.lower().endswith(\".raw\") else base\n",
    "\n",
    "def _norm_key(x: str) -> str:\n",
    "    \"\"\"Case-insensitive, extension-agnostic normalization.\"\"\"\n",
    "    return _strip_ext_basename(x).lower()\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# RAW processing\n",
    "# -----------------------------\n",
    "def process_raw_folder(raw_folder: str,\n",
    "                       out_ms1_csv: str,\n",
    "                       out_ms2_csv: str) -> Tuple[str, str]:\n",
    "    \"\"\"\n",
    "    1) MS1 CSV: one row per sample_name; columns: sample_name, cast_.....\n",
    "    2) MS2 CSV: one row per MS2 scan; columns: sample_name, scan_number, retention_time, precursor_mz, cast_...\n",
    "\n",
    "    Returns (ms1_csv_path, ms2_csv_path).\n",
    "    \"\"\"\n",
    "    raw_paths = _raw_files_in(raw_folder)\n",
    "\n",
    "    # MS1 accumulation per sample\n",
    "    ms1_acc: Dict[str, np.ndarray] = {}\n",
    "\n",
    "    # MS2 rows\n",
    "    ms2_rows: List[List[float]] = []\n",
    "\n",
    "    for raw_path in raw_paths:\n",
    "        sample_name = os.path.splitext(os.path.basename(raw_path))[0]\n",
    "        if sample_name not in ms1_acc:\n",
    "            ms1_acc[sample_name] = np.zeros(MS1_LEN, dtype=np.float32)\n",
    "\n",
    "        try:\n",
    "            raw = RawFile(raw_path)\n",
    "        except Exception as e:\n",
    "            print(f'[skip] Cannot open RAW: {raw_path} ({e})')\n",
    "            continue\n",
    "\n",
    "        total_scans = int(getattr(raw, \"number_of_scans\", 0) or 0)\n",
    "\n",
    "        for scan_number in range(1, total_scans + 1):\n",
    "            try:\n",
    "                raw_scan = Scan.from_file(raw._raw_file_access, scan_number=scan_number)\n",
    "            except Exception:\n",
    "                continue\n",
    "\n",
    "            stype = _scan_type_label(raw_scan.scan_type)\n",
    "            sc_num = getattr(raw_scan.scan_statistics, \"scan_number\", scan_number)\n",
    "            try:\n",
    "                rt = float(raw.get_retention_time_from_scan_number(sc_num))\n",
    "            except Exception:\n",
    "                rt = np.nan\n",
    "\n",
    "            mz = _as_float_array(getattr(raw_scan, \"preferred_masses\", None))\n",
    "            it = _as_float_array(getattr(raw_scan, \"preferred_intensities\", None))\n",
    "            if mz.size == 0 or it.size == 0:\n",
    "                continue\n",
    "\n",
    "            if stype == \"ms\":\n",
    "                idx = np.rint(mz * 10.0).astype(np.int32)\n",
    "                mask = (idx >= MS1_MIN_IDX) & (idx < MS1_MAX_EXC)\n",
    "                if mask.any():\n",
    "                    np.add.at(ms1_acc[sample_name],\n",
    "                              idx[mask] - MS1_MIN_IDX,\n",
    "                              it[mask].astype(np.float32, copy=False))\n",
    "\n",
    "            elif stype == \"ms2\":\n",
    "                idx = np.rint(mz).astype(np.int32)\n",
    "                mask = (idx >= MS2_MIN_IDX) & (idx < MS2_MAX_EXC)\n",
    "                if not mask.any():\n",
    "                    continue\n",
    "                v = np.zeros(MS2_LEN, dtype=np.float32)\n",
    "                np.add.at(v, idx[mask] - MS2_MIN_IDX, it[mask].astype(np.float32, copy=False))\n",
    "                if MS2_NORMALIZE:\n",
    "                    vmax = float(v.max())\n",
    "                    if vmax > 0:\n",
    "                        v /= vmax\n",
    "                prec = _precursor_from_scan(raw_scan)\n",
    "                ms2_rows.append([sample_name, int(sc_num), float(rt), float(prec)] + v.astype(np.float32).tolist())\n",
    "\n",
    "        try:\n",
    "            raw.dispose()\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    # Write MS1 CSV\n",
    "    ms1_headers = [\"sample_name\"] + _make_cast_headers(\"cast\", MS1_LEN)\n",
    "    ms1_df = pd.DataFrame([[sn] + vec.tolist() for sn, vec in ms1_acc.items()], columns=ms1_headers)\n",
    "    os.makedirs(os.path.dirname(os.path.abspath(out_ms1_csv)) or \".\", exist_ok=True)\n",
    "    ms1_df.to_csv(out_ms1_csv, index=False)\n",
    "\n",
    "    # Write MS2 CSV\n",
    "    ms2_headers = [\"sample_name\", \"scan_number\", \"retention_time\", \"precursor_mz\"] + _make_cast_headers(\"cast\", MS2_LEN)\n",
    "    ms2_df = pd.DataFrame(ms2_rows, columns=ms2_headers)\n",
    "    os.makedirs(os.path.dirname(os.path.abspath(out_ms2_csv)) or \".\", exist_ok=True)\n",
    "    ms2_df.to_csv(out_ms2_csv, index=False)\n",
    "\n",
    "    return out_ms1_csv, out_ms2_csv\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Make MS2 compatible with ID_import\n",
    "# -----------------------------\n",
    "def prepare_ms2_for_id_import(ms2_df: pd.DataFrame, tdportal_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    - Renames 'scan_number' -> 'scan'\n",
    "    - Coerces 'scan' to int\n",
    "    - Aligns 'sample_name' to EXACT keys present in tdportal['File Name'] (case/extension robust)\n",
    "    \"\"\"\n",
    "    if \"scan_number\" in ms2_df.columns:\n",
    "        ms2_df = ms2_df.rename(columns={\"scan_number\": \"scan\"})\n",
    "\n",
    "    # enforce int scans\n",
    "    ms2_df[\"scan\"] = pd.to_numeric(ms2_df[\"scan\"], errors=\"coerce\").astype(\"Int64\")\n",
    "    ms2_df = ms2_df.dropna(subset=[\"scan\"]).copy()\n",
    "    ms2_df[\"scan\"] = ms2_df[\"scan\"].astype(int)\n",
    "\n",
    "    # Build a resolver from normalized forms -> canonical td key\n",
    "    if \"File Name\" not in tdportal_df.columns:\n",
    "        raise KeyError(\"tdportal must contain 'File Name' column.\")\n",
    "    td_names = list(tdportal_df[\"File Name\"].astype(str).values)\n",
    "\n",
    "    resolver: Dict[str, str] = {}\n",
    "    for name in td_names:\n",
    "        resolver[_norm_key(name)] = name  # canonical map\n",
    "\n",
    "    # map ms2 sample_name to the canonical td key if possible\n",
    "    def _resolve_sample(s: str) -> str:\n",
    "        k = _norm_key(s)\n",
    "        return resolver.get(k, s)  # if unmatched, keep original (ID_import will handle KeyError)\n",
    "\n",
    "    ms2_df[\"sample_name\"] = ms2_df[\"sample_name\"].astype(str).map(_resolve_sample)\n",
    "\n",
    "    return ms2_df\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Your original ID_import (unchanged)\n",
    "# -----------------------------\n",
    "def ID_import(tdportal, databank, cast_path):\n",
    "    def str_to_int(st):\n",
    "        internal = []\n",
    "        digits = re.findall(r'\\d+', st)\n",
    "        for i in range(0, len(digits)):\n",
    "            internal.append(int(digits[i]))\n",
    "        return(internal)\n",
    "\n",
    "    scan_number = [0]*len(tdportal['File Name'])\n",
    "    td_samples = []\n",
    "\n",
    "    for i in range(0, len(tdportal['File Name'])):\n",
    "        scan_number[i] = str_to_int(str(tdportal['Fragment Scans'][i]))\n",
    "        if tdportal['File Name'][i] not in td_samples:\n",
    "            td_samples.append(tdportal['File Name'][i])\n",
    "\n",
    "    my_dic_scan = {key: [] for key in td_samples}\n",
    "    my_dic_index = {key: [] for key in td_samples}\n",
    "\n",
    "    for i in range(0, len(tdportal['File Name'])):\n",
    "        my_dic_scan[tdportal['File Name'][i]].append(scan_number[i])\n",
    "        my_dic_index[tdportal['File Name'][i]].append([i]*len(scan_number[i]))\n",
    "\n",
    "    for i in range(0, len(td_samples)):\n",
    "        nested_list = my_dic_scan[td_samples[i]]\n",
    "        flat_list = []\n",
    "        for item in nested_list:\n",
    "            if isinstance(item, list):\n",
    "                flat_list.extend(item)\n",
    "            else:\n",
    "                flat_list.append(item)\n",
    "        my_dic_scan[td_samples[i]] = [elem for sublist in flat_list for elem in (sublist if isinstance(sublist, list) else [sublist])]\n",
    "\n",
    "    for i in range(0, len(td_samples)):\n",
    "        nested_list = my_dic_index[td_samples[i]]\n",
    "        flat_list = []\n",
    "        for item in nested_list:\n",
    "            if isinstance(item, list):\n",
    "                flat_list.extend(item)\n",
    "            else:\n",
    "                flat_list.append(item)\n",
    "        my_dic_index[td_samples[i]] = [elem for sublist in flat_list for elem in (sublist if isinstance(sublist, list) else [sublist])]\n",
    "\n",
    "    sequence, MASS, Accession, missing, PFR = [], [], [], [], []\n",
    "\n",
    "    for i in tqdm(range(len(databank['scan'])), desc=\"Processing scans\", ncols=100):\n",
    "        try:\n",
    "            sample = databank['sample_name'][i]\n",
    "            scan   = databank['scan'][i]\n",
    "\n",
    "            if scan in my_dic_scan[sample]:\n",
    "                tt = my_dic_index[sample][my_dic_scan[sample].index(scan)]\n",
    "                sequence.append(tdportal.at[tt, 'Sequence'])\n",
    "                MASS.append(tdportal.at[tt, 'Average Mass'])\n",
    "                Accession.append(tdportal.at[tt, 'Accession'])\n",
    "                PFR.append(tdportal.at[tt, 'PFR'])\n",
    "            else:\n",
    "                sequence.append(None)\n",
    "                MASS.append(None)\n",
    "                Accession.append(None)\n",
    "                PFR.append(None)\n",
    "\n",
    "        except KeyError as e:\n",
    "            missing.append(sample)\n",
    "            sequence.append(None)\n",
    "            MASS.append(None)\n",
    "            Accession.append(None)\n",
    "            PFR.append(None)\n",
    "\n",
    "        except Exception as e:\n",
    "            sequence.append(None)\n",
    "            MASS.append(None)\n",
    "            Accession.append(None)\n",
    "            PFR.append(None)\n",
    "\n",
    "    print(set(missing))\n",
    "\n",
    "    databank['sequence'] = sequence\n",
    "    databank['MASS'] = MASS\n",
    "    databank['Accession'] = Accession\n",
    "    databank['PFR'] = PFR\n",
    "\n",
    "    databank = pd.DataFrame(databank)\n",
    "    databank.to_csv(cast_path, index=False)\n",
    "    return()\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Orchestrator\n",
    "# -----------------------------\n",
    "def run_all(raw_folder: str,\n",
    "            out_ms1_csv: str,\n",
    "            out_ms2_csv: str,\n",
    "            tdportal_csv: str,\n",
    "            cast_out_csv: str):\n",
    "    # 1) Process RAWs -> MS1 & MS2\n",
    "    ms1_csv, ms2_csv = process_raw_folder(raw_folder, out_ms1_csv, out_ms2_csv)\n",
    "    print(\"MS1 CSV:\", ms1_csv)\n",
    "    print(\"MS2 CSV:\", ms2_csv)\n",
    "\n",
    "    # 2) Load tdportal + ms2, make ms2 compatible\n",
    "    tdportal = pd.read_csv(tdportal_csv)\n",
    "    ms2_df = pd.read_csv(ms2_csv)\n",
    "    ms2_df = prepare_ms2_for_id_import(ms2_df, tdportal)\n",
    "\n",
    "    # 3) ID_import\n",
    "    #    Pass DataFrame (dict-like) so ID_import can index as databank['col']\n",
    "    ID_import(tdportal=tdportal, databank=ms2_df, cast_path=cast_out_csv)\n",
    "    print(\"Final MS2+IDs CSV:\", cast_out_csv)\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Example usage\n",
    "# -----------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    RAW_FOLDER   = r\"F:\\old_data\\usb1\\samples\"         # folder with .raw files\n",
    "    OUT_MS1      = r\"F:\\binary\\final\\ms1_per_sample.csv\"\n",
    "    OUT_MS2      = r\"F:\\binary\\final\\ms2_per_scan.csv\"\n",
    "    TDPORTAL_CSV = r\"F:\\binary\\tdreport.csv\"    # must have 'File Name' and 'Fragment Scans'\n",
    "    CAST_OUT     = r\"F:\\binary\\final\\ms2_per_scan_with_ids.csv\"\n",
    "\n",
    "    run_all(RAW_FOLDER, OUT_MS1, OUT_MS2, TDPORTAL_CSV, CAST_OUT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cb93e415",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Done. Saved with new columns → F:\\binary\\final\\ms1_per_sample1.csv\n",
      "                                         sample_name  cast_00000  cast_00001  \\\n",
      "0  20130215_smp4245_AlphSyn_Other_Soluble_0002_PD...  18587108.0  37565644.0   \n",
      "1  20130215_smp4245_AlphSyn_Other_Soluble_0131_NC...  17228884.0  35624848.0   \n",
      "2  20130215_smp4245_AlphSyn_Other_Soluble_0704_NC...  14287125.0  36654144.0   \n",
      "3  20130215_smp4245_AlphSyn_Other_Soluble_0888_NC...  12796031.0  25576836.0   \n",
      "4  20130215_smp4245_AlphSyn_Other_Soluble_1004_PD...  16891818.0  37302412.0   \n",
      "\n",
      "   cast_00002  cast_00003  cast_00004  cast_00005  cast_00006  cast_00007  \\\n",
      "0  37719492.0  41584304.0  54957464.0  55359060.0  55809436.0  39938660.0   \n",
      "1  35591744.0  36933960.0  47113852.0  45205584.0  40316612.0  33812524.0   \n",
      "2  34616104.0  40935356.0  46965768.0  48612952.0  47507156.0  32869136.0   \n",
      "3  27933800.0  28212676.0  32740908.0  30305518.0  26870286.0  24770404.0   \n",
      "4  40510660.0  49253436.0  56422292.0  54578916.0  52350572.0  38943600.0   \n",
      "\n",
      "   cast_00008  ...     cast_13682    cast_13683    cast_13684    cast_13685  \\\n",
      "0  32724188.0  ...  780808.312500  1.817540e+06  1.153403e+06  1.608954e+06   \n",
      "1  32534698.0  ...  988512.500000  7.893936e+05  6.434593e+05  1.071704e+06   \n",
      "2  35157516.0  ...  108306.421875  3.998314e+05  2.037534e+05  5.284534e+05   \n",
      "3  24535608.0  ...  364033.406250  2.377790e+05  5.646992e+05  6.724446e+05   \n",
      "4  41852312.0  ...  923685.437500  7.421542e+05  4.019595e+05  2.003344e+05   \n",
      "\n",
      "     cast_13686    cast_13687    cast_13688   cast_13689  target  fractions  \n",
      "0  879637.81250  8.351088e+05  1.119420e+06  850910.4375       1    Soluble  \n",
      "1  393855.62500  9.808251e+05  9.008757e+05  661097.3750       0    Soluble  \n",
      "2  343600.40625  3.674465e+05  4.868092e+05  842688.8750       0    Soluble  \n",
      "3  816827.31250  7.161374e+05  7.487942e+05  981587.6875       0    Soluble  \n",
      "4  850730.37500  1.410922e+06  1.148917e+06  862909.1250       1    Soluble  \n",
      "\n",
      "[5 rows x 13693 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# -----------------------------\n",
    "# Config\n",
    "# -----------------------------\n",
    "CSV_PATH = r\"F:\\binary\\final\\ms1_per_sample.csv\"   # change to your CSV file path\n",
    "OUT_PATH = r\"F:\\binary\\final\\ms1_per_sample1.csv\"\n",
    "\n",
    "# -----------------------------\n",
    "# Load CSV\n",
    "# -----------------------------\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "\n",
    "# -----------------------------\n",
    "# Create 'target' column\n",
    "# -----------------------------\n",
    "# 0 for _NC_, 1 for _PD_ or _PDAD_\n",
    "df[\"target\"] = df[\"sample_name\"].apply(\n",
    "    lambda x: 1 if any(tag in str(x) for tag in [\"_PD_\", \"_PDAD_\"]) else 0\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# Create 'fractions' column\n",
    "# -----------------------------\n",
    "def get_fraction(name: str) -> str:\n",
    "    name = str(name)\n",
    "    if \"_Pellet_\" in name:\n",
    "        return \"Pellet\"\n",
    "    elif \"_Soluble_\" in name:\n",
    "        return \"Soluble\"\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "df[\"fractions\"] = df[\"sample_name\"].apply(get_fraction)\n",
    "\n",
    "# -----------------------------\n",
    "# Save result\n",
    "# -----------------------------\n",
    "df.to_csv(OUT_PATH, index=False)\n",
    "print(f\"✅ Done. Saved with new columns → {OUT_PATH}\")\n",
    "\n",
    "# (Optional) preview\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "45caedcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MS1 CSV: F:\\binary\\final\\ms1_per_sample.csv\n",
      "MS2 CSV: F:\\binary\\final\\ms2_per_scan.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing scans: 100%|████████████████████████████████████| 19307/19307 [00:00<00:00, 23493.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set()\n",
      "Final MS2+IDs CSV: F:\\binary\\final\\ms2_per_scan_with_ids.csv\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "End-to-end:\n",
    "1) Process Thermo .raw in a folder -> MS1 and MS2 CSVs\n",
    "2) Make MS2 compatible with ID_import (scan column, sample_name alignment)\n",
    "3) Run ID_import to attach IDs from a tdportal table\n",
    "\n",
    "Requires: fisher-py, numpy, pandas, tqdm\n",
    "  pip install fisher-py numpy pandas tqdm\n",
    "\"\"\"\n",
    "\n",
    "import os, glob, re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import List, Tuple, Dict\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ---- fisher-py imports (RAW access) ----\n",
    "from fisher_py.data.business import Scan\n",
    "from fisher_py import RawFile\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Binning configuration\n",
    "# -----------------------------\n",
    "# MS1: 0.1 m/z bins, 600.0 .. ~1968.9 (index = round(mz * 10))\n",
    "MS1_MIN_IDX, MS1_LEN = 6000, 13690\n",
    "MS1_MAX_EXC = MS1_MIN_IDX + MS1_LEN\n",
    "\n",
    "# MS2: 1.0 m/z bins, 400 .. 1999 (index = round(mz))\n",
    "MS2_MIN_IDX, MS2_LEN = 400, 1600\n",
    "MS2_MAX_EXC = MS2_MIN_IDX + MS2_LEN\n",
    "\n",
    "# Normalize each MS2 scan to its max intensity (per-scan TIC-like)\n",
    "MS2_NORMALIZE = True\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Helpers\n",
    "# -----------------------------\n",
    "def _scan_type_label(text: str) -> str:\n",
    "    m = re.search(r\"Full\\s+(\\w+)\", str(text), flags=re.IGNORECASE)\n",
    "    return m.group(1).lower() if m else \"\"\n",
    "\n",
    "def _as_float_array(x):\n",
    "    if x is None:\n",
    "        return np.array([], dtype=float)\n",
    "    a = np.asarray(x)\n",
    "    return a.astype(float, copy=False) if a.size else np.array([], dtype=float)\n",
    "\n",
    "def _raw_files_in(folder: str) -> List[str]:\n",
    "    folder = os.path.abspath(folder)\n",
    "    if not os.path.isdir(folder):\n",
    "        raise FileNotFoundError(f'Folder not found: \"{folder}\"')\n",
    "    out = sorted(set(glob.glob(os.path.join(folder, \"*.raw\")) +\n",
    "                     glob.glob(os.path.join(folder, \"*.RAW\"))))\n",
    "    if not out:\n",
    "        raise FileNotFoundError(f'No .raw files in: {folder}')\n",
    "    return out\n",
    "\n",
    "def _precursor_from_scan(raw_scan) -> float:\n",
    "    for attr in (\"precursor_mz\", \"master_precursor_mz\", \"isolation_mz\"):\n",
    "        if hasattr(raw_scan, attr):\n",
    "            try:\n",
    "                return float(getattr(raw_scan, attr))\n",
    "            except Exception:\n",
    "                pass\n",
    "    try:\n",
    "        nums = re.findall(r'\\d+\\.\\d+', str(raw_scan.scan_type))\n",
    "        return float(nums[1]) if len(nums) > 1 else np.nan\n",
    "    except Exception:\n",
    "        return np.nan\n",
    "\n",
    "def _make_cast_headers(prefix: str, length: int) -> List[str]:\n",
    "    width = max(5, len(str(length)))\n",
    "    return [f\"{prefix}_{i:0{width}d}\" for i in range(length)]\n",
    "\n",
    "def _strip_ext_basename(x: str) -> str:\n",
    "    base = os.path.basename(str(x))\n",
    "    return base[:-4] if base.lower().endswith(\".raw\") else base\n",
    "\n",
    "def _norm_key(x: str) -> str:\n",
    "    \"\"\"Case-insensitive, extension-agnostic normalization.\"\"\"\n",
    "    return _strip_ext_basename(x).lower()\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# RAW processing\n",
    "# -----------------------------\n",
    "def process_raw_folder(raw_folder: str,\n",
    "                       out_ms1_csv: str,\n",
    "                       out_ms2_csv: str) -> Tuple[str, str]:\n",
    "    \"\"\"\n",
    "    1) MS1 CSV: one row per sample_name; columns: sample_name, target, fractions, cast_.....\n",
    "    2) MS2 CSV: one row per MS2 scan; columns: sample_name, scan_number, retention_time, precursor_mz, cast_...\n",
    "\n",
    "    Returns (ms1_csv_path, ms2_csv_path).\n",
    "    \"\"\"\n",
    "    # --- helpers for new columns (derived from sample_name) ---\n",
    "    def _target_from_name(name: str) -> int:\n",
    "        s = str(name)\n",
    "        # 0 for _NC_, 1 for _PD_ or _PDAD_\n",
    "        if \"_PD_\" in s or \"_PDAD_\" in s:\n",
    "            return 1\n",
    "        return 0  # includes _NC_ and everything else by default\n",
    "\n",
    "    def _fraction_from_name(name: str):\n",
    "        s = str(name)\n",
    "        if \"_Pellet_\" in s:\n",
    "            return \"Pellet\"\n",
    "        if \"_Soluble_\" in s:\n",
    "            return \"Soluble\"\n",
    "        return None\n",
    "\n",
    "    raw_paths = _raw_files_in(raw_folder)\n",
    "\n",
    "    # MS1 accumulation per sample\n",
    "    ms1_acc: Dict[str, np.ndarray] = {}\n",
    "\n",
    "    # MS2 rows\n",
    "    ms2_rows: List[List[float]] = []\n",
    "\n",
    "    for raw_path in raw_paths:\n",
    "        sample_name = os.path.splitext(os.path.basename(raw_path))[0]\n",
    "        if sample_name not in ms1_acc:\n",
    "            ms1_acc[sample_name] = np.zeros(MS1_LEN, dtype=np.float32)\n",
    "\n",
    "        try:\n",
    "            raw = RawFile(raw_path)\n",
    "        except Exception as e:\n",
    "            print(f'[skip] Cannot open RAW: {raw_path} ({e})')\n",
    "            continue\n",
    "\n",
    "        total_scans = int(getattr(raw, \"number_of_scans\", 0) or 0)\n",
    "\n",
    "        for scan_number in range(1, total_scans + 1):\n",
    "            try:\n",
    "                raw_scan = Scan.from_file(raw._raw_file_access, scan_number=scan_number)\n",
    "            except Exception:\n",
    "                continue\n",
    "\n",
    "            stype = _scan_type_label(raw_scan.scan_type)\n",
    "            sc_num = getattr(raw_scan.scan_statistics, \"scan_number\", scan_number)\n",
    "            try:\n",
    "                rt = float(raw.get_retention_time_from_scan_number(sc_num))\n",
    "            except Exception:\n",
    "                rt = np.nan\n",
    "\n",
    "            mz = _as_float_array(getattr(raw_scan, \"preferred_masses\", None))\n",
    "            it = _as_float_array(getattr(raw_scan, \"preferred_intensities\", None))\n",
    "            if mz.size == 0 or it.size == 0:\n",
    "                continue\n",
    "\n",
    "            if stype == \"ms\":\n",
    "                idx = np.rint(mz * 10.0).astype(np.int32)\n",
    "                mask = (idx >= MS1_MIN_IDX) & (idx < MS1_MAX_EXC)\n",
    "                if mask.any():\n",
    "                    np.add.at(ms1_acc[sample_name],\n",
    "                              idx[mask] - MS1_MIN_IDX,\n",
    "                              it[mask].astype(np.float32, copy=False))\n",
    "\n",
    "            elif stype == \"ms2\":\n",
    "                idx = np.rint(mz).astype(np.int32)\n",
    "                mask = (idx >= MS2_MIN_IDX) & (idx < MS2_MAX_EXC)\n",
    "                if not mask.any():\n",
    "                    continue\n",
    "                v = np.zeros(MS2_LEN, dtype=np.float32)\n",
    "                np.add.at(v, idx[mask] - MS2_MIN_IDX, it[mask].astype(np.float32, copy=False))\n",
    "                if MS2_NORMALIZE:\n",
    "                    vmax = float(v.max())\n",
    "                    if vmax > 0:\n",
    "                        v /= vmax\n",
    "                prec = _precursor_from_scan(raw_scan)\n",
    "                ms2_rows.append([sample_name, int(sc_num), float(rt), float(prec)] + v.astype(np.float32).tolist())\n",
    "\n",
    "        try:\n",
    "            raw.dispose()\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    # ---- Write MS1 CSV with new columns ----\n",
    "    cast_headers = _make_cast_headers(\"cast\", MS1_LEN)\n",
    "    ms1_headers = [\"sample_name\", \"target\", \"fractions\"] + cast_headers\n",
    "\n",
    "    ms1_records = []\n",
    "    for sn, vec in ms1_acc.items():\n",
    "        ms1_records.append([\n",
    "            sn,\n",
    "            _target_from_name(sn),\n",
    "            _fraction_from_name(sn),\n",
    "            *vec.tolist()\n",
    "        ])\n",
    "\n",
    "    ms1_df = pd.DataFrame(ms1_records, columns=ms1_headers)\n",
    "    os.makedirs(os.path.dirname(os.path.abspath(out_ms1_csv)) or \".\", exist_ok=True)\n",
    "    ms1_df.to_csv(out_ms1_csv, index=False)\n",
    "\n",
    "    # ---- Write MS2 CSV (unchanged) ----\n",
    "    ms2_headers = [\"sample_name\", \"scan_number\", \"retention_time\", \"precursor_mz\"] + _make_cast_headers(\"cast\", MS2_LEN)\n",
    "    ms2_df = pd.DataFrame(ms2_rows, columns=ms2_headers)\n",
    "    os.makedirs(os.path.dirname(os.path.abspath(out_ms2_csv)) or \".\", exist_ok=True)\n",
    "    ms2_df.to_csv(out_ms2_csv, index=False)\n",
    "\n",
    "    return out_ms1_csv, out_ms2_csv\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Make MS2 compatible with ID_import\n",
    "# -----------------------------\n",
    "def prepare_ms2_for_id_import(ms2_df: pd.DataFrame, tdportal_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    - Renames 'scan_number' -> 'scan'\n",
    "    - Coerces 'scan' to int\n",
    "    - Aligns 'sample_name' to EXACT keys present in tdportal['File Name'] (case/extension robust)\n",
    "    \"\"\"\n",
    "    if \"scan_number\" in ms2_df.columns:\n",
    "        ms2_df = ms2_df.rename(columns={\"scan_number\": \"scan\"})\n",
    "\n",
    "    # enforce int scans\n",
    "    ms2_df[\"scan\"] = pd.to_numeric(ms2_df[\"scan\"], errors=\"coerce\").astype(\"Int64\")\n",
    "    ms2_df = ms2_df.dropna(subset=[\"scan\"]).copy()\n",
    "    ms2_df[\"scan\"] = ms2_df[\"scan\"].astype(int)\n",
    "\n",
    "    # Build a resolver from normalized forms -> canonical td key\n",
    "    if \"File Name\" not in tdportal_df.columns:\n",
    "        raise KeyError(\"tdportal must contain 'File Name' column.\")\n",
    "    td_names = list(tdportal_df[\"File Name\"].astype(str).values)\n",
    "\n",
    "    resolver: Dict[str, str] = {}\n",
    "    for name in td_names:\n",
    "        resolver[_norm_key(name)] = name  # canonical map\n",
    "\n",
    "    # map ms2 sample_name to the canonical td key if possible\n",
    "    def _resolve_sample(s: str) -> str:\n",
    "        k = _norm_key(s)\n",
    "        return resolver.get(k, s)  # if unmatched, keep original (ID_import will handle KeyError)\n",
    "\n",
    "    ms2_df[\"sample_name\"] = ms2_df[\"sample_name\"].astype(str).map(_resolve_sample)\n",
    "\n",
    "    return ms2_df\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Your original ID_import (unchanged)\n",
    "# -----------------------------\n",
    "def ID_import(tdportal, databank, cast_path):\n",
    "    def str_to_int(st):\n",
    "        internal = []\n",
    "        digits = re.findall(r'\\d+', st)\n",
    "        for i in range(0, len(digits)):\n",
    "            internal.append(int(digits[i]))\n",
    "        return(internal)\n",
    "\n",
    "    scan_number = [0]*len(tdportal['File Name'])\n",
    "    td_samples = []\n",
    "\n",
    "    for i in range(0, len(tdportal['File Name'])):\n",
    "        scan_number[i] = str_to_int(str(tdportal['Fragment Scans'][i]))\n",
    "        if tdportal['File Name'][i] not in td_samples:\n",
    "            td_samples.append(tdportal['File Name'][i])\n",
    "\n",
    "    my_dic_scan = {key: [] for key in td_samples}\n",
    "    my_dic_index = {key: [] for key in td_samples}\n",
    "\n",
    "    for i in range(0, len(tdportal['File Name'])):\n",
    "        my_dic_scan[tdportal['File Name'][i]].append(scan_number[i])\n",
    "        my_dic_index[tdportal['File Name'][i]].append([i]*len(scan_number[i]))\n",
    "\n",
    "    for i in range(0, len(td_samples)):\n",
    "        nested_list = my_dic_scan[td_samples[i]]\n",
    "        flat_list = []\n",
    "        for item in nested_list:\n",
    "            if isinstance(item, list):\n",
    "                flat_list.extend(item)\n",
    "            else:\n",
    "                flat_list.append(item)\n",
    "        my_dic_scan[td_samples[i]] = [elem for sublist in flat_list for elem in (sublist if isinstance(sublist, list) else [sublist])]\n",
    "\n",
    "    for i in range(0, len(td_samples)):\n",
    "        nested_list = my_dic_index[td_samples[i]]\n",
    "        flat_list = []\n",
    "        for item in nested_list:\n",
    "            if isinstance(item, list):\n",
    "                flat_list.extend(item)\n",
    "            else:\n",
    "                flat_list.append(item)\n",
    "        my_dic_index[td_samples[i]] = [elem for sublist in flat_list for elem in (sublist if isinstance(sublist, list) else [sublist])]\n",
    "\n",
    "    sequence, MASS, Accession, missing, PFR = [], [], [], [], []\n",
    "\n",
    "    for i in tqdm(range(len(databank['scan'])), desc=\"Processing scans\", ncols=100):\n",
    "        try:\n",
    "            sample = databank['sample_name'][i]\n",
    "            scan   = databank['scan'][i]\n",
    "\n",
    "            if scan in my_dic_scan[sample]:\n",
    "                tt = my_dic_index[sample][my_dic_scan[sample].index(scan)]\n",
    "                sequence.append(tdportal.at[tt, 'Sequence'])\n",
    "                MASS.append(tdportal.at[tt, 'Average Mass'])\n",
    "                Accession.append(tdportal.at[tt, 'Accession'])\n",
    "                PFR.append(tdportal.at[tt, 'PFR'])\n",
    "            else:\n",
    "                sequence.append(None)\n",
    "                MASS.append(None)\n",
    "                Accession.append(None)\n",
    "                PFR.append(None)\n",
    "\n",
    "        except KeyError as e:\n",
    "            missing.append(sample)\n",
    "            sequence.append(None)\n",
    "            MASS.append(None)\n",
    "            Accession.append(None)\n",
    "            PFR.append(None)\n",
    "\n",
    "        except Exception as e:\n",
    "            sequence.append(None)\n",
    "            MASS.append(None)\n",
    "            Accession.append(None)\n",
    "            PFR.append(None)\n",
    "\n",
    "    print(set(missing))\n",
    "\n",
    "    databank['sequence'] = sequence\n",
    "    databank['MASS'] = MASS\n",
    "    databank['Accession'] = Accession\n",
    "    databank['PFR'] = PFR\n",
    "\n",
    "    databank = pd.DataFrame(databank)\n",
    "    databank.to_csv(cast_path, index=False)\n",
    "    return()\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Orchestrator\n",
    "# -----------------------------\n",
    "def run_all(raw_folder: str,\n",
    "            out_ms1_csv: str,\n",
    "            out_ms2_csv: str,\n",
    "            tdportal_csv: str,\n",
    "            cast_out_csv: str):\n",
    "    # 1) Process RAWs -> MS1 & MS2\n",
    "    ms1_csv, ms2_csv = process_raw_folder(raw_folder, out_ms1_csv, out_ms2_csv)\n",
    "    print(\"MS1 CSV:\", ms1_csv)\n",
    "    print(\"MS2 CSV:\", ms2_csv)\n",
    "\n",
    "    # 2) Load tdportal + ms2, make ms2 compatible\n",
    "    tdportal = pd.read_csv(tdportal_csv)\n",
    "    ms2_df = pd.read_csv(ms2_csv)\n",
    "    ms2_df = prepare_ms2_for_id_import(ms2_df, tdportal)\n",
    "\n",
    "    # 3) ID_import\n",
    "    #    Pass DataFrame (dict-like) so ID_import can index as databank['col']\n",
    "    ID_import(tdportal=tdportal, databank=ms2_df, cast_path=cast_out_csv)\n",
    "    print(\"Final MS2+IDs CSV:\", cast_out_csv)\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Example usage\n",
    "# -----------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    RAW_FOLDER   = r\"F:\\old_data\\usb1\\samples\"         # folder with .raw files\n",
    "    OUT_MS1      = r\"F:\\binary\\final\\ms1_per_sample.csv\"\n",
    "    OUT_MS2      = r\"F:\\binary\\final\\ms2_per_scan.csv\"\n",
    "    TDPORTAL_CSV = r\"F:\\binary\\tdreport.csv\"    # must have 'File Name' and 'Fragment Scans'\n",
    "    CAST_OUT     = r\"F:\\binary\\final\\ms2_per_scan_with_ids.csv\"\n",
    "\n",
    "    run_all(RAW_FOLDER, OUT_MS1, OUT_MS2, TDPORTAL_CSV, CAST_OUT)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "casting",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
