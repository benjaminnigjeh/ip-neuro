{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8104e0c9",
   "metadata": {},
   "source": [
    "Generating deconvoluted spectra from the informative features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "975db4cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: frac_pellet_grads_AB__pos_1__neg_0_negabs_runA.csv ‚Üí F:\\binary\\final\\raw\\result\\frac_pellet_grads_AB__pos_1__neg_0_negabs_runA\n",
      "Processing: frac_pellet_grads_AB__pos_1__neg_0_negabs_runB.csv ‚Üí F:\\binary\\final\\raw\\result\\frac_pellet_grads_AB__pos_1__neg_0_negabs_runB\n",
      "Processing: frac_pellet_grads_AB__pos_1__neg_0_pos_runA.csv ‚Üí F:\\binary\\final\\raw\\result\\frac_pellet_grads_AB__pos_1__neg_0_pos_runA\n",
      "Processing: frac_pellet_grads_AB__pos_1__neg_0_pos_runB.csv ‚Üí F:\\binary\\final\\raw\\result\\frac_pellet_grads_AB__pos_1__neg_0_pos_runB\n",
      "Processing: frac_soluble_grads_AB__pos_1__neg_0_negabs_runA.csv ‚Üí F:\\binary\\final\\raw\\result\\frac_soluble_grads_AB__pos_1__neg_0_negabs_runA\n",
      "Processing: frac_soluble_grads_AB__pos_1__neg_0_negabs_runB.csv ‚Üí F:\\binary\\final\\raw\\result\\frac_soluble_grads_AB__pos_1__neg_0_negabs_runB\n",
      "Processing: frac_soluble_grads_AB__pos_1__neg_0_pos_runA.csv ‚Üí F:\\binary\\final\\raw\\result\\frac_soluble_grads_AB__pos_1__neg_0_pos_runA\n",
      "Processing: frac_soluble_grads_AB__pos_1__neg_0_pos_runB.csv ‚Üí F:\\binary\\final\\raw\\result\\frac_soluble_grads_AB__pos_1__neg_0_pos_runB\n",
      "‚úÖ All files processed. Results saved in: F:\\binary\\final\\raw\\result\n",
      "Staging: F:\\binary\\final\\raw\\result\\frac_pellet_grads_AB__pos_1__neg_0_negabs_runA_mass.txt ‚Üí C:\\Users\\benja\\AppData\\Local\\Temp\\mass_collect_682b2yn0\\frac_pellet_grads_AB__pos_1__neg_0_negabs_runA_mass.txt\n",
      "Staging: F:\\binary\\final\\raw\\result\\frac_pellet_grads_AB__pos_1__neg_0_negabs_runB_mass.txt ‚Üí C:\\Users\\benja\\AppData\\Local\\Temp\\mass_collect_682b2yn0\\frac_pellet_grads_AB__pos_1__neg_0_negabs_runB_mass.txt\n",
      "Staging: F:\\binary\\final\\raw\\result\\frac_pellet_grads_AB__pos_1__neg_0_pos_runA_mass.txt ‚Üí C:\\Users\\benja\\AppData\\Local\\Temp\\mass_collect_682b2yn0\\frac_pellet_grads_AB__pos_1__neg_0_pos_runA_mass.txt\n",
      "Staging: F:\\binary\\final\\raw\\result\\frac_pellet_grads_AB__pos_1__neg_0_pos_runB_mass.txt ‚Üí C:\\Users\\benja\\AppData\\Local\\Temp\\mass_collect_682b2yn0\\frac_pellet_grads_AB__pos_1__neg_0_pos_runB_mass.txt\n",
      "Staging: F:\\binary\\final\\raw\\result\\frac_soluble_grads_AB__pos_1__neg_0_negabs_runA_mass.txt ‚Üí C:\\Users\\benja\\AppData\\Local\\Temp\\mass_collect_682b2yn0\\frac_soluble_grads_AB__pos_1__neg_0_negabs_runA_mass.txt\n",
      "Staging: F:\\binary\\final\\raw\\result\\frac_soluble_grads_AB__pos_1__neg_0_negabs_runB_mass.txt ‚Üí C:\\Users\\benja\\AppData\\Local\\Temp\\mass_collect_682b2yn0\\frac_soluble_grads_AB__pos_1__neg_0_negabs_runB_mass.txt\n",
      "Staging: F:\\binary\\final\\raw\\result\\frac_soluble_grads_AB__pos_1__neg_0_pos_runA_mass.txt ‚Üí C:\\Users\\benja\\AppData\\Local\\Temp\\mass_collect_682b2yn0\\frac_soluble_grads_AB__pos_1__neg_0_pos_runA_mass.txt\n",
      "Staging: F:\\binary\\final\\raw\\result\\frac_soluble_grads_AB__pos_1__neg_0_pos_runB_mass.txt ‚Üí C:\\Users\\benja\\AppData\\Local\\Temp\\mass_collect_682b2yn0\\frac_soluble_grads_AB__pos_1__neg_0_pos_runB_mass.txt\n",
      "Finalizing: C:\\Users\\benja\\AppData\\Local\\Temp\\mass_collect_682b2yn0\\frac_pellet_grads_AB__pos_1__neg_0_negabs_runA_mass.txt ‚Üí F:\\binary\\final\\raw\\result\\frac_pellet_grads_AB__pos_1__neg_0_negabs_runA_mass.txt\n",
      "Finalizing: C:\\Users\\benja\\AppData\\Local\\Temp\\mass_collect_682b2yn0\\frac_pellet_grads_AB__pos_1__neg_0_negabs_runB_mass.txt ‚Üí F:\\binary\\final\\raw\\result\\frac_pellet_grads_AB__pos_1__neg_0_negabs_runB_mass.txt\n",
      "Finalizing: C:\\Users\\benja\\AppData\\Local\\Temp\\mass_collect_682b2yn0\\frac_pellet_grads_AB__pos_1__neg_0_pos_runA_mass.txt ‚Üí F:\\binary\\final\\raw\\result\\frac_pellet_grads_AB__pos_1__neg_0_pos_runA_mass.txt\n",
      "Finalizing: C:\\Users\\benja\\AppData\\Local\\Temp\\mass_collect_682b2yn0\\frac_pellet_grads_AB__pos_1__neg_0_pos_runB_mass.txt ‚Üí F:\\binary\\final\\raw\\result\\frac_pellet_grads_AB__pos_1__neg_0_pos_runB_mass.txt\n",
      "Finalizing: C:\\Users\\benja\\AppData\\Local\\Temp\\mass_collect_682b2yn0\\frac_soluble_grads_AB__pos_1__neg_0_negabs_runA_mass.txt ‚Üí F:\\binary\\final\\raw\\result\\frac_soluble_grads_AB__pos_1__neg_0_negabs_runA_mass.txt\n",
      "Finalizing: C:\\Users\\benja\\AppData\\Local\\Temp\\mass_collect_682b2yn0\\frac_soluble_grads_AB__pos_1__neg_0_negabs_runB_mass.txt ‚Üí F:\\binary\\final\\raw\\result\\frac_soluble_grads_AB__pos_1__neg_0_negabs_runB_mass.txt\n",
      "Finalizing: C:\\Users\\benja\\AppData\\Local\\Temp\\mass_collect_682b2yn0\\frac_soluble_grads_AB__pos_1__neg_0_pos_runA_mass.txt ‚Üí F:\\binary\\final\\raw\\result\\frac_soluble_grads_AB__pos_1__neg_0_pos_runA_mass.txt\n",
      "Finalizing: C:\\Users\\benja\\AppData\\Local\\Temp\\mass_collect_682b2yn0\\frac_soluble_grads_AB__pos_1__neg_0_pos_runB_mass.txt ‚Üí F:\\binary\\final\\raw\\result\\frac_soluble_grads_AB__pos_1__neg_0_pos_runB_mass.txt\n",
      "üìÇ Clean result folder ready with only *_mass.txt files: F:\\binary\\final\\raw\\result\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import os\n",
    "import subprocess\n",
    "import shutil\n",
    "import tempfile\n",
    "\n",
    "def _unique_dst_path(dst_dir, fname):\n",
    "    \"\"\"Return a unique path in dst_dir for fname, adding a numeric suffix if needed.\"\"\"\n",
    "    base, ext = os.path.splitext(fname)\n",
    "    candidate = os.path.join(dst_dir, fname)\n",
    "    i = 1\n",
    "    while os.path.exists(candidate):\n",
    "        candidate = os.path.join(dst_dir, f\"{base}__{i}{ext}\")\n",
    "        i += 1\n",
    "    return candidate\n",
    "\n",
    "def _prefixed_name(src_path, result_root):\n",
    "    \"\"\"\n",
    "    Build a safer filename using the immediate parent folder under result/ as a prefix\n",
    "    to reduce collisions: e.g., result/sampleA/sampleA_mass.txt -> sampleA__sampleA_mass.txt\n",
    "    \"\"\"\n",
    "    # src_path like .../result/<parent>/<file>\n",
    "    parent = os.path.basename(os.path.dirname(src_path))\n",
    "    fname = os.path.basename(src_path)\n",
    "    return f\"{parent}__{fname}\" if parent and parent != \"result\" else fname\n",
    "\n",
    "def run_unidec_on_folder(folder_path):\n",
    "    # Ensure result root folder exists\n",
    "    result_root = os.path.join(folder_path, \"result\")\n",
    "    os.makedirs(result_root, exist_ok=True)\n",
    "\n",
    "    # Loop through files in the folder (top-level only)\n",
    "    for file_name in os.listdir(folder_path):\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "\n",
    "        # Skip directories\n",
    "        if not os.path.isfile(file_path):\n",
    "            continue\n",
    "\n",
    "        # Create a unique subfolder named after the file (without extension)\n",
    "        base_name = os.path.splitext(file_name)[0]\n",
    "        file_result_folder = os.path.join(result_root, base_name)\n",
    "        os.makedirs(file_result_folder, exist_ok=True)\n",
    "\n",
    "        # Run UniDec for this file, send outputs to its subfolder\n",
    "        print(f\"Processing: {file_name} ‚Üí {file_result_folder}\")\n",
    "        subprocess.run([\"python\", \"-m\", \"unidec\", \"-f\", file_path, \"-o\", file_result_folder])\n",
    "\n",
    "    print(\"‚úÖ All files processed. Results saved in:\", result_root)\n",
    "\n",
    "    # 1) Collect *_mass.txt paths from result_root (including subfolders)\n",
    "    collected = []\n",
    "    for root, _, files in os.walk(result_root):\n",
    "        for f in files:\n",
    "            if f.endswith(\"_mass.txt\"):\n",
    "                collected.append(os.path.join(root, f))\n",
    "\n",
    "    if not collected:\n",
    "        print(\"‚ö†Ô∏è No *_mass.txt files found under:\", result_root)\n",
    "        return\n",
    "\n",
    "    # 2) Copy them to a temp folder FIRST (so deleting result/ content won't break src paths)\n",
    "    temp_dir = tempfile.mkdtemp(prefix=\"mass_collect_\")\n",
    "    copied = []\n",
    "    for src in collected:\n",
    "        try:\n",
    "            # Prefix with subfolder name to avoid collisions\n",
    "            safe_name = _prefixed_name(src, result_root)\n",
    "            dst = os.path.join(temp_dir, safe_name)\n",
    "            dst = _unique_dst_path(temp_dir, os.path.basename(dst))  # ensure uniqueness\n",
    "            print(f\"Staging: {src} ‚Üí {dst}\")\n",
    "            shutil.copy2(src, dst)\n",
    "            copied.append(dst)\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Skip (copy error): {src} ‚Äî {e}\")\n",
    "\n",
    "    # 3) Clean the result_root completely\n",
    "    for item in os.listdir(result_root):\n",
    "        item_path = os.path.join(result_root, item)\n",
    "        try:\n",
    "            if os.path.isfile(item_path) or os.path.islink(item_path):\n",
    "                os.remove(item_path)\n",
    "            else:\n",
    "                shutil.rmtree(item_path)\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Could not remove {item_path}: {e}\")\n",
    "\n",
    "    # 4) Move staged files back into a clean result_root\n",
    "    for staged in copied:\n",
    "        try:\n",
    "            final_dst = os.path.join(result_root, os.path.basename(staged))\n",
    "            final_dst = _unique_dst_path(result_root, os.path.basename(final_dst))\n",
    "            print(f\"Finalizing: {staged} ‚Üí {final_dst}\")\n",
    "            shutil.move(staged, final_dst)\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Move error for {staged}: {e}\")\n",
    "\n",
    "    # 5) Remove temp dir (ignore errors)\n",
    "    try:\n",
    "        shutil.rmtree(temp_dir)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    print(\"üìÇ Clean result folder ready with only *_mass.txt files:\", result_root)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Example usage\n",
    "    folder_path = r\"F:\\binary\\final\\raw\"  # <-- replace with your folder\n",
    "    run_unidec_on_folder(folder_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c693eba",
   "metadata": {},
   "source": [
    "Visualize the deconvolution graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcdcd2fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mirror Plots (Run A vs Run B) ‚Äî strong-color version with up/downregulated labels\n",
    "\n",
    "import re, numpy as np, pandas as pd, matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from IPython.display import display, Image\n",
    "\n",
    "# ==== CONFIG ====\n",
    "FOLDER   = Path(r\"F:/binary/final/decon\")  # üëà change this to your folder\n",
    "OUT_DIR  = None                            # or set to Path(r\"F:/binary/mirror_plots\")\n",
    "MASS_MIN, MASS_MAX = 10000, 20000          # mass range (Da)\n",
    "BAR_WIDTH = 5                              # narrow bars\n",
    "# =================\n",
    "\n",
    "plt.rcParams.update({\n",
    "    \"figure.figsize\": (10, 5),\n",
    "    \"axes.titlesize\": 16,\n",
    "    \"axes.labelsize\": 14,\n",
    "    \"xtick.labelsize\": 11,\n",
    "    \"ytick.labelsize\": 11,\n",
    "    \"axes.titleweight\": \"bold\",\n",
    "    \"axes.labelweight\": \"bold\",\n",
    "    \"axes.edgecolor\": \"black\",\n",
    "    \"axes.linewidth\": 1.0\n",
    "})\n",
    "\n",
    "PATTERN = re.compile(\n",
    "    r'^frac_(?P<fraction>[A-Za-z0-9]+)_grads_AB__pos_1__neg_0_(?P<sign>negabs|pos)_run(?P<run>[AB])_mass\\.txt$'\n",
    ")\n",
    "\n",
    "def read_two_col_txt(path: Path) -> pd.DataFrame:\n",
    "    df = pd.read_csv(path, sep=r\"\\s+\", header=None, names=[\"mass\", \"value\"], engine=\"python\")\n",
    "    df = df[np.isfinite(df[\"mass\"]) & np.isfinite(df[\"value\"])]\n",
    "    df = df[(df[\"mass\"] >= MASS_MIN) & (df[\"mass\"] <= MASS_MAX)]\n",
    "    return df.sort_values(\"mass\").reset_index(drop=True)\n",
    "\n",
    "def discover_pairs(folder: Path):\n",
    "    found = {}\n",
    "    for p in folder.glob(\"*.txt\"):\n",
    "        m = PATTERN.match(p.name)\n",
    "        if not m:\n",
    "            continue\n",
    "        k = (m[\"fraction\"], m[\"sign\"])\n",
    "        found.setdefault(k, {})[m[\"run\"]] = p\n",
    "    return {k: v for k, v in found.items() if \"A\" in v and \"B\" in v}\n",
    "\n",
    "def align_on_union_mass(a: pd.DataFrame, b: pd.DataFrame):\n",
    "    if len(a) == 0 or len(b) == 0:\n",
    "        return a.copy(), b.copy()\n",
    "    if len(a) == len(b) and np.allclose(a[\"mass\"], b[\"mass\"]):\n",
    "        return a, b\n",
    "    union = pd.DataFrame({\"mass\": np.unique(np.r_[a[\"mass\"].values, b[\"mass\"].values])})\n",
    "    a2 = pd.merge_asof(union.sort_values(\"mass\"), a.sort_values(\"mass\"), on=\"mass\", direction=\"nearest\")\n",
    "    b2 = pd.merge_asof(union.sort_values(\"mass\"), b.sort_values(\"mass\"), on=\"mass\", direction=\"nearest\")\n",
    "    a2[\"value\"] = a2[\"value\"].fillna(0.0)\n",
    "    b2[\"value\"] = b2[\"value\"].fillna(0.0)\n",
    "    return a2, b2\n",
    "\n",
    "def cosine_similarity(a_vals: np.ndarray, b_vals: np.ndarray) -> float:\n",
    "    denom = (np.linalg.norm(a_vals) * np.linalg.norm(b_vals))\n",
    "    return 0.0 if denom == 0 else float(np.dot(a_vals, b_vals) / denom)\n",
    "\n",
    "def mirror_plot_bars(a: pd.DataFrame, b: pd.DataFrame, out: Path, title: str, cos_sim: float):\n",
    "    if len(a) == 0 or len(b) == 0:\n",
    "        print(f\"[skip] Empty after filtering {MASS_MIN}-{MASS_MAX} Da ‚Üí {out.name}\")\n",
    "        return\n",
    "\n",
    "    A, B = align_on_union_mass(a, b)\n",
    "    x = A[\"mass\"].values\n",
    "    yA = A[\"value\"].values\n",
    "    yB = -B[\"value\"].values  # mirrored\n",
    "\n",
    "    # Strong, saturated colors\n",
    "    color_A = \"#000000\"  # deep navy blue\n",
    "    color_B = \"#CC0000\"  # vivid orange-red\n",
    "\n",
    "    plt.figure(facecolor=\"white\")\n",
    "    plt.bar(x, yA, width=BAR_WIDTH, color=color_A, alpha=1.0, linewidth=0, label=\"Run A\", zorder=3)\n",
    "    plt.bar(x, yB, width=BAR_WIDTH, color=color_B, alpha=1.0, linewidth=0, label=\"Run B (mirrored)\", zorder=3)\n",
    "    plt.axhline(0, color=\"black\", linewidth=1.4, zorder=1)\n",
    "    plt.title(f\"{title}  |  cos(A,B) = {cos_sim:.3f}\")\n",
    "    plt.xlabel(\"Mass (Da)\")\n",
    "    plt.ylabel(\"Gradient / Intensity\")\n",
    "    plt.xlim(MASS_MIN, MASS_MAX)\n",
    "    plt.legend(frameon=False)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    out.parent.mkdir(exist_ok=True, parents=True)\n",
    "    plt.savefig(out, dpi=220)\n",
    "    plt.close()\n",
    "\n",
    "# ==== RUN ====\n",
    "pairs = discover_pairs(FOLDER)\n",
    "out_dir = OUT_DIR or (FOLDER / \"mirror_plots\")\n",
    "out_dir.mkdir(exist_ok=True, parents=True)\n",
    "print(f\"Found {len(pairs)} A/B pairs in {FOLDER}. Saving to {out_dir}\\n\")\n",
    "\n",
    "for (frac, sign), runs in sorted(pairs.items()):\n",
    "    dfA = read_two_col_txt(runs[\"A\"])\n",
    "    dfB = read_two_col_txt(runs[\"B\"])\n",
    "    A_aligned, B_aligned = align_on_union_mass(dfA, dfB)\n",
    "    cos = cosine_similarity(A_aligned[\"value\"].to_numpy(), B_aligned[\"value\"].to_numpy())\n",
    "\n",
    "    # Replace \"negabs\" ‚Üí \"downregulated\", \"pos\" ‚Üí \"upregulated\"\n",
    "    sign_label = \"downregulated\" if sign == \"negabs\" else \"upregulated\"\n",
    "\n",
    "    name = f\"mirror_{frac}_{sign}_A_vs_B_bars_strong.png\"\n",
    "    title = f\"{frac.capitalize()} ‚Äî {sign_label} (Run A vs Run B)\"\n",
    "    mirror_plot_bars(dfA, dfB, out_dir / name, title, cos)\n",
    "    print(f\"  ‚úì {frac}/{sign_label}: cos(A,B) = {cos:.4f}  ‚Üí  {name}\")\n",
    "\n",
    "# ==== PREVIEW ====\n",
    "print(\"\\nPreview:\")\n",
    "for p in sorted(out_dir.glob(\"*_strong.png\")):\n",
    "    print(\" \", p.name)\n",
    "    display(Image(filename=str(p)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e7ceca",
   "metadata": {},
   "source": [
    "Features less than 25K Da"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5563a0a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8 decon file(s) in F:\\binary\\final\\decon\n",
      "Found 8 raw CSV file(s) in F:\\binary\\final\\raw\n",
      "Applying maximum protein mass filter: ‚â§ 25.000 kDa\n",
      "[frac_pellet_grads_AB__pos_1__neg_0_negabs_runA] Applied max mass filter ‚â§ 25.000 kDa (25000.0 Da): kept 2005/7371 points.\n",
      "[frac_pellet_grads_AB__pos_1__neg_0_negabs_runA] Neutral-mass detection: 18 peaks ‚Üí F:\\binary\\final\\firstpass\\frac_pellet_grads_AB__pos_1__neg_0_negabs_runA\\frac_pellet_grads_AB__pos_1__neg_0_negabs_runA_detected_signals.csv\n",
      "[frac_pellet_grads_AB__pos_1__neg_0_negabs_runA] Detection plot saved ‚Üí F:\\binary\\final\\firstpass\\frac_pellet_grads_AB__pos_1__neg_0_negabs_runA\\frac_pellet_grads_AB__pos_1__neg_0_negabs_runA_detected_signals.png\n",
      "[frac_pellet_grads_AB__pos_1__neg_0_negabs_runA] Parsed filename metadata: {'bin': None, 'experiments': 1, 'controls': 1, 'experiments_ids': '1', 'controls_ids': '0', 'experiments_n': 1, 'controls_n': 1, 'regulation': 'downregulated', 'replicate': 'A', 'source_file': 'frac_pellet_grads_AB__pos_1__neg_0_negabs_runA_mass.txt'}\n",
      "Saved plot: F:\\binary\\final\\firstpass\\frac_pellet_grads_AB__pos_1__neg_0_negabs_runA\\frac_pellet_grads_AB__pos_1__neg_0_negabs_runA_neutral_mass_spectrum.png\n",
      "Saved plot: F:\\binary\\final\\firstpass\\frac_pellet_grads_AB__pos_1__neg_0_negabs_runA\\frac_pellet_grads_AB__pos_1__neg_0_negabs_runA_mirror_assigned_vs_total.png\n",
      "Saved plot: F:\\binary\\final\\firstpass\\frac_pellet_grads_AB__pos_1__neg_0_negabs_runA\\frac_pellet_grads_AB__pos_1__neg_0_negabs_runA_mirror_unassigned_vs_total.png\n",
      "Saved plot: F:\\binary\\final\\firstpass\\frac_pellet_grads_AB__pos_1__neg_0_negabs_runA\\frac_pellet_grads_AB__pos_1__neg_0_negabs_runA_mirror_assigned_by_protein_vs_total.png\n",
      "\n",
      "=== [frac_pellet_grads_AB__pos_1__neg_0_negabs_runA] Summary ===\n",
      "Raw MS1 peaks (rows): 4,330\n",
      "Detected neutral-mass peaks: 18\n",
      "Assigned raw peaks: 349\n",
      "Non-assigned raw peaks: 3,981\n",
      "Max mass filter: ‚â§ 25.000 kDa (25000.0 Da)\n",
      "Saved: F:\\binary\\final\\firstpass\\frac_pellet_grads_AB__pos_1__neg_0_negabs_runA\\frac_pellet_grads_AB__pos_1__neg_0_negabs_runA_assigned_ms1_with_peaks.csv\n",
      "Saved: F:\\binary\\final\\firstpass\\frac_pellet_grads_AB__pos_1__neg_0_negabs_runA\\frac_pellet_grads_AB__pos_1__neg_0_negabs_runA_assignments_summary.csv\n",
      "------------------------------------------------------------\n",
      "[frac_pellet_grads_AB__pos_1__neg_0_negabs_runB] Applied max mass filter ‚â§ 25.000 kDa (25000.0 Da): kept 2005/7370 points.\n",
      "[frac_pellet_grads_AB__pos_1__neg_0_negabs_runB] Neutral-mass detection: 16 peaks ‚Üí F:\\binary\\final\\firstpass\\frac_pellet_grads_AB__pos_1__neg_0_negabs_runB\\frac_pellet_grads_AB__pos_1__neg_0_negabs_runB_detected_signals.csv\n",
      "[frac_pellet_grads_AB__pos_1__neg_0_negabs_runB] Detection plot saved ‚Üí F:\\binary\\final\\firstpass\\frac_pellet_grads_AB__pos_1__neg_0_negabs_runB\\frac_pellet_grads_AB__pos_1__neg_0_negabs_runB_detected_signals.png\n",
      "[frac_pellet_grads_AB__pos_1__neg_0_negabs_runB] Parsed filename metadata: {'bin': None, 'experiments': 1, 'controls': 1, 'experiments_ids': '1', 'controls_ids': '0', 'experiments_n': 1, 'controls_n': 1, 'regulation': 'downregulated', 'replicate': 'B', 'source_file': 'frac_pellet_grads_AB__pos_1__neg_0_negabs_runB_mass.txt'}\n",
      "Saved plot: F:\\binary\\final\\firstpass\\frac_pellet_grads_AB__pos_1__neg_0_negabs_runB\\frac_pellet_grads_AB__pos_1__neg_0_negabs_runB_neutral_mass_spectrum.png\n",
      "Saved plot: F:\\binary\\final\\firstpass\\frac_pellet_grads_AB__pos_1__neg_0_negabs_runB\\frac_pellet_grads_AB__pos_1__neg_0_negabs_runB_mirror_assigned_vs_total.png\n",
      "Saved plot: F:\\binary\\final\\firstpass\\frac_pellet_grads_AB__pos_1__neg_0_negabs_runB\\frac_pellet_grads_AB__pos_1__neg_0_negabs_runB_mirror_unassigned_vs_total.png\n",
      "Saved plot: F:\\binary\\final\\firstpass\\frac_pellet_grads_AB__pos_1__neg_0_negabs_runB\\frac_pellet_grads_AB__pos_1__neg_0_negabs_runB_mirror_assigned_by_protein_vs_total.png\n",
      "\n",
      "=== [frac_pellet_grads_AB__pos_1__neg_0_negabs_runB] Summary ===\n",
      "Raw MS1 peaks (rows): 4,634\n",
      "Detected neutral-mass peaks: 16\n",
      "Assigned raw peaks: 305\n",
      "Non-assigned raw peaks: 4,329\n",
      "Max mass filter: ‚â§ 25.000 kDa (25000.0 Da)\n",
      "Saved: F:\\binary\\final\\firstpass\\frac_pellet_grads_AB__pos_1__neg_0_negabs_runB\\frac_pellet_grads_AB__pos_1__neg_0_negabs_runB_assigned_ms1_with_peaks.csv\n",
      "Saved: F:\\binary\\final\\firstpass\\frac_pellet_grads_AB__pos_1__neg_0_negabs_runB\\frac_pellet_grads_AB__pos_1__neg_0_negabs_runB_assignments_summary.csv\n",
      "------------------------------------------------------------\n",
      "[frac_pellet_grads_AB__pos_1__neg_0_pos_runA] Applied max mass filter ‚â§ 25.000 kDa (25000.0 Da): kept 2001/7338 points.\n",
      "[frac_pellet_grads_AB__pos_1__neg_0_pos_runA] Neutral-mass detection: 32 peaks ‚Üí F:\\binary\\final\\firstpass\\frac_pellet_grads_AB__pos_1__neg_0_pos_runA\\frac_pellet_grads_AB__pos_1__neg_0_pos_runA_detected_signals.csv\n",
      "[frac_pellet_grads_AB__pos_1__neg_0_pos_runA] Detection plot saved ‚Üí F:\\binary\\final\\firstpass\\frac_pellet_grads_AB__pos_1__neg_0_pos_runA\\frac_pellet_grads_AB__pos_1__neg_0_pos_runA_detected_signals.png\n",
      "[frac_pellet_grads_AB__pos_1__neg_0_pos_runA] Parsed filename metadata: {'bin': None, 'experiments': 1, 'controls': 1, 'experiments_ids': '1', 'controls_ids': '0', 'experiments_n': 1, 'controls_n': 1, 'regulation': 'upregulated', 'replicate': 'A', 'source_file': 'frac_pellet_grads_AB__pos_1__neg_0_pos_runA_mass.txt'}\n",
      "Saved plot: F:\\binary\\final\\firstpass\\frac_pellet_grads_AB__pos_1__neg_0_pos_runA\\frac_pellet_grads_AB__pos_1__neg_0_pos_runA_neutral_mass_spectrum.png\n",
      "Saved plot: F:\\binary\\final\\firstpass\\frac_pellet_grads_AB__pos_1__neg_0_pos_runA\\frac_pellet_grads_AB__pos_1__neg_0_pos_runA_mirror_assigned_vs_total.png\n",
      "Saved plot: F:\\binary\\final\\firstpass\\frac_pellet_grads_AB__pos_1__neg_0_pos_runA\\frac_pellet_grads_AB__pos_1__neg_0_pos_runA_mirror_unassigned_vs_total.png\n",
      "Saved plot: F:\\binary\\final\\firstpass\\frac_pellet_grads_AB__pos_1__neg_0_pos_runA\\frac_pellet_grads_AB__pos_1__neg_0_pos_runA_mirror_assigned_by_protein_vs_total.png\n",
      "\n",
      "=== [frac_pellet_grads_AB__pos_1__neg_0_pos_runA] Summary ===\n",
      "Raw MS1 peaks (rows): 5,670\n",
      "Detected neutral-mass peaks: 32\n",
      "Assigned raw peaks: 513\n",
      "Non-assigned raw peaks: 5,157\n",
      "Max mass filter: ‚â§ 25.000 kDa (25000.0 Da)\n",
      "Saved: F:\\binary\\final\\firstpass\\frac_pellet_grads_AB__pos_1__neg_0_pos_runA\\frac_pellet_grads_AB__pos_1__neg_0_pos_runA_assigned_ms1_with_peaks.csv\n",
      "Saved: F:\\binary\\final\\firstpass\\frac_pellet_grads_AB__pos_1__neg_0_pos_runA\\frac_pellet_grads_AB__pos_1__neg_0_pos_runA_assignments_summary.csv\n",
      "------------------------------------------------------------\n",
      "[frac_pellet_grads_AB__pos_1__neg_0_pos_runB] Applied max mass filter ‚â§ 25.000 kDa (25000.0 Da): kept 2002/7349 points.\n",
      "[frac_pellet_grads_AB__pos_1__neg_0_pos_runB] Neutral-mass detection: 25 peaks ‚Üí F:\\binary\\final\\firstpass\\frac_pellet_grads_AB__pos_1__neg_0_pos_runB\\frac_pellet_grads_AB__pos_1__neg_0_pos_runB_detected_signals.csv\n",
      "[frac_pellet_grads_AB__pos_1__neg_0_pos_runB] Detection plot saved ‚Üí F:\\binary\\final\\firstpass\\frac_pellet_grads_AB__pos_1__neg_0_pos_runB\\frac_pellet_grads_AB__pos_1__neg_0_pos_runB_detected_signals.png\n",
      "[frac_pellet_grads_AB__pos_1__neg_0_pos_runB] Parsed filename metadata: {'bin': None, 'experiments': 1, 'controls': 1, 'experiments_ids': '1', 'controls_ids': '0', 'experiments_n': 1, 'controls_n': 1, 'regulation': 'upregulated', 'replicate': 'B', 'source_file': 'frac_pellet_grads_AB__pos_1__neg_0_pos_runB_mass.txt'}\n",
      "Saved plot: F:\\binary\\final\\firstpass\\frac_pellet_grads_AB__pos_1__neg_0_pos_runB\\frac_pellet_grads_AB__pos_1__neg_0_pos_runB_neutral_mass_spectrum.png\n",
      "Saved plot: F:\\binary\\final\\firstpass\\frac_pellet_grads_AB__pos_1__neg_0_pos_runB\\frac_pellet_grads_AB__pos_1__neg_0_pos_runB_mirror_assigned_vs_total.png\n",
      "Saved plot: F:\\binary\\final\\firstpass\\frac_pellet_grads_AB__pos_1__neg_0_pos_runB\\frac_pellet_grads_AB__pos_1__neg_0_pos_runB_mirror_unassigned_vs_total.png\n",
      "Saved plot: F:\\binary\\final\\firstpass\\frac_pellet_grads_AB__pos_1__neg_0_pos_runB\\frac_pellet_grads_AB__pos_1__neg_0_pos_runB_mirror_assigned_by_protein_vs_total.png\n",
      "\n",
      "=== [frac_pellet_grads_AB__pos_1__neg_0_pos_runB] Summary ===\n",
      "Raw MS1 peaks (rows): 5,366\n",
      "Detected neutral-mass peaks: 25\n",
      "Assigned raw peaks: 369\n",
      "Non-assigned raw peaks: 4,997\n",
      "Max mass filter: ‚â§ 25.000 kDa (25000.0 Da)\n",
      "Saved: F:\\binary\\final\\firstpass\\frac_pellet_grads_AB__pos_1__neg_0_pos_runB\\frac_pellet_grads_AB__pos_1__neg_0_pos_runB_assigned_ms1_with_peaks.csv\n",
      "Saved: F:\\binary\\final\\firstpass\\frac_pellet_grads_AB__pos_1__neg_0_pos_runB\\frac_pellet_grads_AB__pos_1__neg_0_pos_runB_assignments_summary.csv\n",
      "------------------------------------------------------------\n",
      "[frac_soluble_grads_AB__pos_1__neg_0_negabs_runA] Applied max mass filter ‚â§ 25.000 kDa (25000.0 Da): kept 2002/7342 points.\n",
      "[frac_soluble_grads_AB__pos_1__neg_0_negabs_runA] Neutral-mass detection: 21 peaks ‚Üí F:\\binary\\final\\firstpass\\frac_soluble_grads_AB__pos_1__neg_0_negabs_runA\\frac_soluble_grads_AB__pos_1__neg_0_negabs_runA_detected_signals.csv\n",
      "[frac_soluble_grads_AB__pos_1__neg_0_negabs_runA] Detection plot saved ‚Üí F:\\binary\\final\\firstpass\\frac_soluble_grads_AB__pos_1__neg_0_negabs_runA\\frac_soluble_grads_AB__pos_1__neg_0_negabs_runA_detected_signals.png\n",
      "[frac_soluble_grads_AB__pos_1__neg_0_negabs_runA] Parsed filename metadata: {'bin': None, 'experiments': 1, 'controls': 1, 'experiments_ids': '1', 'controls_ids': '0', 'experiments_n': 1, 'controls_n': 1, 'regulation': 'downregulated', 'replicate': 'A', 'source_file': 'frac_soluble_grads_AB__pos_1__neg_0_negabs_runA_mass.txt'}\n",
      "Saved plot: F:\\binary\\final\\firstpass\\frac_soluble_grads_AB__pos_1__neg_0_negabs_runA\\frac_soluble_grads_AB__pos_1__neg_0_negabs_runA_neutral_mass_spectrum.png\n",
      "Saved plot: F:\\binary\\final\\firstpass\\frac_soluble_grads_AB__pos_1__neg_0_negabs_runA\\frac_soluble_grads_AB__pos_1__neg_0_negabs_runA_mirror_assigned_vs_total.png\n",
      "Saved plot: F:\\binary\\final\\firstpass\\frac_soluble_grads_AB__pos_1__neg_0_negabs_runA\\frac_soluble_grads_AB__pos_1__neg_0_negabs_runA_mirror_unassigned_vs_total.png\n",
      "Saved plot: F:\\binary\\final\\firstpass\\frac_soluble_grads_AB__pos_1__neg_0_negabs_runA\\frac_soluble_grads_AB__pos_1__neg_0_negabs_runA_mirror_assigned_by_protein_vs_total.png\n",
      "\n",
      "=== [frac_soluble_grads_AB__pos_1__neg_0_negabs_runA] Summary ===\n",
      "Raw MS1 peaks (rows): 5,472\n",
      "Detected neutral-mass peaks: 21\n",
      "Assigned raw peaks: 383\n",
      "Non-assigned raw peaks: 5,089\n",
      "Max mass filter: ‚â§ 25.000 kDa (25000.0 Da)\n",
      "Saved: F:\\binary\\final\\firstpass\\frac_soluble_grads_AB__pos_1__neg_0_negabs_runA\\frac_soluble_grads_AB__pos_1__neg_0_negabs_runA_assigned_ms1_with_peaks.csv\n",
      "Saved: F:\\binary\\final\\firstpass\\frac_soluble_grads_AB__pos_1__neg_0_negabs_runA\\frac_soluble_grads_AB__pos_1__neg_0_negabs_runA_assignments_summary.csv\n",
      "------------------------------------------------------------\n",
      "[frac_soluble_grads_AB__pos_1__neg_0_negabs_runB] Applied max mass filter ‚â§ 25.000 kDa (25000.0 Da): kept 2002/7344 points.\n",
      "[frac_soluble_grads_AB__pos_1__neg_0_negabs_runB] Neutral-mass detection: 20 peaks ‚Üí F:\\binary\\final\\firstpass\\frac_soluble_grads_AB__pos_1__neg_0_negabs_runB\\frac_soluble_grads_AB__pos_1__neg_0_negabs_runB_detected_signals.csv\n",
      "[frac_soluble_grads_AB__pos_1__neg_0_negabs_runB] Detection plot saved ‚Üí F:\\binary\\final\\firstpass\\frac_soluble_grads_AB__pos_1__neg_0_negabs_runB\\frac_soluble_grads_AB__pos_1__neg_0_negabs_runB_detected_signals.png\n",
      "[frac_soluble_grads_AB__pos_1__neg_0_negabs_runB] Parsed filename metadata: {'bin': None, 'experiments': 1, 'controls': 1, 'experiments_ids': '1', 'controls_ids': '0', 'experiments_n': 1, 'controls_n': 1, 'regulation': 'downregulated', 'replicate': 'B', 'source_file': 'frac_soluble_grads_AB__pos_1__neg_0_negabs_runB_mass.txt'}\n",
      "Saved plot: F:\\binary\\final\\firstpass\\frac_soluble_grads_AB__pos_1__neg_0_negabs_runB\\frac_soluble_grads_AB__pos_1__neg_0_negabs_runB_neutral_mass_spectrum.png\n",
      "Saved plot: F:\\binary\\final\\firstpass\\frac_soluble_grads_AB__pos_1__neg_0_negabs_runB\\frac_soluble_grads_AB__pos_1__neg_0_negabs_runB_mirror_assigned_vs_total.png\n",
      "Saved plot: F:\\binary\\final\\firstpass\\frac_soluble_grads_AB__pos_1__neg_0_negabs_runB\\frac_soluble_grads_AB__pos_1__neg_0_negabs_runB_mirror_unassigned_vs_total.png\n",
      "Saved plot: F:\\binary\\final\\firstpass\\frac_soluble_grads_AB__pos_1__neg_0_negabs_runB\\frac_soluble_grads_AB__pos_1__neg_0_negabs_runB_mirror_assigned_by_protein_vs_total.png\n",
      "\n",
      "=== [frac_soluble_grads_AB__pos_1__neg_0_negabs_runB] Summary ===\n",
      "Raw MS1 peaks (rows): 5,074\n",
      "Detected neutral-mass peaks: 20\n",
      "Assigned raw peaks: 378\n",
      "Non-assigned raw peaks: 4,696\n",
      "Max mass filter: ‚â§ 25.000 kDa (25000.0 Da)\n",
      "Saved: F:\\binary\\final\\firstpass\\frac_soluble_grads_AB__pos_1__neg_0_negabs_runB\\frac_soluble_grads_AB__pos_1__neg_0_negabs_runB_assigned_ms1_with_peaks.csv\n",
      "Saved: F:\\binary\\final\\firstpass\\frac_soluble_grads_AB__pos_1__neg_0_negabs_runB\\frac_soluble_grads_AB__pos_1__neg_0_negabs_runB_assignments_summary.csv\n",
      "------------------------------------------------------------\n",
      "[frac_soluble_grads_AB__pos_1__neg_0_pos_runA] Applied max mass filter ‚â§ 25.000 kDa (25000.0 Da): kept 2004/7365 points.\n",
      "[frac_soluble_grads_AB__pos_1__neg_0_pos_runA] Neutral-mass detection: 18 peaks ‚Üí F:\\binary\\final\\firstpass\\frac_soluble_grads_AB__pos_1__neg_0_pos_runA\\frac_soluble_grads_AB__pos_1__neg_0_pos_runA_detected_signals.csv\n",
      "[frac_soluble_grads_AB__pos_1__neg_0_pos_runA] Detection plot saved ‚Üí F:\\binary\\final\\firstpass\\frac_soluble_grads_AB__pos_1__neg_0_pos_runA\\frac_soluble_grads_AB__pos_1__neg_0_pos_runA_detected_signals.png\n",
      "[frac_soluble_grads_AB__pos_1__neg_0_pos_runA] Parsed filename metadata: {'bin': None, 'experiments': 1, 'controls': 1, 'experiments_ids': '1', 'controls_ids': '0', 'experiments_n': 1, 'controls_n': 1, 'regulation': 'upregulated', 'replicate': 'A', 'source_file': 'frac_soluble_grads_AB__pos_1__neg_0_pos_runA_mass.txt'}\n",
      "Saved plot: F:\\binary\\final\\firstpass\\frac_soluble_grads_AB__pos_1__neg_0_pos_runA\\frac_soluble_grads_AB__pos_1__neg_0_pos_runA_neutral_mass_spectrum.png\n",
      "Saved plot: F:\\binary\\final\\firstpass\\frac_soluble_grads_AB__pos_1__neg_0_pos_runA\\frac_soluble_grads_AB__pos_1__neg_0_pos_runA_mirror_assigned_vs_total.png\n",
      "Saved plot: F:\\binary\\final\\firstpass\\frac_soluble_grads_AB__pos_1__neg_0_pos_runA\\frac_soluble_grads_AB__pos_1__neg_0_pos_runA_mirror_unassigned_vs_total.png\n",
      "Saved plot: F:\\binary\\final\\firstpass\\frac_soluble_grads_AB__pos_1__neg_0_pos_runA\\frac_soluble_grads_AB__pos_1__neg_0_pos_runA_mirror_assigned_by_protein_vs_total.png\n",
      "\n",
      "=== [frac_soluble_grads_AB__pos_1__neg_0_pos_runA] Summary ===\n",
      "Raw MS1 peaks (rows): 4,528\n",
      "Detected neutral-mass peaks: 18\n",
      "Assigned raw peaks: 337\n",
      "Non-assigned raw peaks: 4,191\n",
      "Max mass filter: ‚â§ 25.000 kDa (25000.0 Da)\n",
      "Saved: F:\\binary\\final\\firstpass\\frac_soluble_grads_AB__pos_1__neg_0_pos_runA\\frac_soluble_grads_AB__pos_1__neg_0_pos_runA_assigned_ms1_with_peaks.csv\n",
      "Saved: F:\\binary\\final\\firstpass\\frac_soluble_grads_AB__pos_1__neg_0_pos_runA\\frac_soluble_grads_AB__pos_1__neg_0_pos_runA_assignments_summary.csv\n",
      "------------------------------------------------------------\n",
      "[frac_soluble_grads_AB__pos_1__neg_0_pos_runB] Applied max mass filter ‚â§ 25.000 kDa (25000.0 Da): kept 2004/7362 points.\n",
      "[frac_soluble_grads_AB__pos_1__neg_0_pos_runB] Neutral-mass detection: 20 peaks ‚Üí F:\\binary\\final\\firstpass\\frac_soluble_grads_AB__pos_1__neg_0_pos_runB\\frac_soluble_grads_AB__pos_1__neg_0_pos_runB_detected_signals.csv\n",
      "[frac_soluble_grads_AB__pos_1__neg_0_pos_runB] Detection plot saved ‚Üí F:\\binary\\final\\firstpass\\frac_soluble_grads_AB__pos_1__neg_0_pos_runB\\frac_soluble_grads_AB__pos_1__neg_0_pos_runB_detected_signals.png\n",
      "[frac_soluble_grads_AB__pos_1__neg_0_pos_runB] Parsed filename metadata: {'bin': None, 'experiments': 1, 'controls': 1, 'experiments_ids': '1', 'controls_ids': '0', 'experiments_n': 1, 'controls_n': 1, 'regulation': 'upregulated', 'replicate': 'B', 'source_file': 'frac_soluble_grads_AB__pos_1__neg_0_pos_runB_mass.txt'}\n",
      "Saved plot: F:\\binary\\final\\firstpass\\frac_soluble_grads_AB__pos_1__neg_0_pos_runB\\frac_soluble_grads_AB__pos_1__neg_0_pos_runB_neutral_mass_spectrum.png\n",
      "Saved plot: F:\\binary\\final\\firstpass\\frac_soluble_grads_AB__pos_1__neg_0_pos_runB\\frac_soluble_grads_AB__pos_1__neg_0_pos_runB_mirror_assigned_vs_total.png\n",
      "Saved plot: F:\\binary\\final\\firstpass\\frac_soluble_grads_AB__pos_1__neg_0_pos_runB\\frac_soluble_grads_AB__pos_1__neg_0_pos_runB_mirror_unassigned_vs_total.png\n",
      "Saved plot: F:\\binary\\final\\firstpass\\frac_soluble_grads_AB__pos_1__neg_0_pos_runB\\frac_soluble_grads_AB__pos_1__neg_0_pos_runB_mirror_assigned_by_protein_vs_total.png\n",
      "\n",
      "=== [frac_soluble_grads_AB__pos_1__neg_0_pos_runB] Summary ===\n",
      "Raw MS1 peaks (rows): 4,926\n",
      "Detected neutral-mass peaks: 20\n",
      "Assigned raw peaks: 343\n",
      "Non-assigned raw peaks: 4,583\n",
      "Max mass filter: ‚â§ 25.000 kDa (25000.0 Da)\n",
      "Saved: F:\\binary\\final\\firstpass\\frac_soluble_grads_AB__pos_1__neg_0_pos_runB\\frac_soluble_grads_AB__pos_1__neg_0_pos_runB_assigned_ms1_with_peaks.csv\n",
      "Saved: F:\\binary\\final\\firstpass\\frac_soluble_grads_AB__pos_1__neg_0_pos_runB\\frac_soluble_grads_AB__pos_1__neg_0_pos_runB_assignments_summary.csv\n",
      "------------------------------------------------------------\n",
      "‚úÖ Batch processing complete.\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "(‚Ä¶ header unchanged ‚Ä¶)\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import glob\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from bisect import bisect_left\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.signal import find_peaks\n",
    "\n",
    "# ============================================================\n",
    "# --------------------  PEAK DETECTION  ----------------------\n",
    "# ============================================================\n",
    "@dataclass\n",
    "class PeakFindingParams:\n",
    "    min_prominence: float | None = None\n",
    "    min_height: float | None = None\n",
    "    min_distance_pts: int = 10\n",
    "    smooth_window: int = 0\n",
    "    min_snr: float = 0.0\n",
    "\n",
    "def _mad_sigma(y: np.ndarray) -> float:\n",
    "    if y.size == 0:\n",
    "        return 0.0\n",
    "    med = np.median(y)\n",
    "    mad = np.median(np.abs(y - med))\n",
    "    return 1.4826 * mad\n",
    "\n",
    "def _smooth(y: np.ndarray, window: int) -> np.ndarray:\n",
    "    if window < 3 or window % 2 == 0:\n",
    "        return y\n",
    "    kernel = np.ones(window, dtype=float) / window\n",
    "    return np.convolve(y, kernel, mode=\"same\")\n",
    "\n",
    "def _extract_id_list(name: str, key: str) -> list[int] | None:\n",
    "    m = re.search(rf\"(?:^|[_-]){key}((?:[_-]\\d+)+)(?=[_-]|$)\", name, flags=re.I)\n",
    "    if not m:\n",
    "        m1 = re.search(rf\"(?:^|[_-]){key}[_-]?(\\d+)(?=[_-]|$)\", name, flags=re.I)\n",
    "        if m1:\n",
    "            return [int(m1.group(1))]\n",
    "        return None\n",
    "    parts = re.findall(r\"\\d+\", m.group(1))\n",
    "    return [int(x) for x in parts] if parts else None\n",
    "\n",
    "def parse_metadata_from_filename(path: str | Path) -> dict:\n",
    "    p = Path(path)\n",
    "    name = p.stem\n",
    "    meta = {\n",
    "        \"bin\": None, \"experiments\": None, \"controls\": None,\n",
    "        \"experiments_ids\": None, \"controls_ids\": None,\n",
    "        \"experiments_n\": None, \"controls_n\": None,\n",
    "        \"regulation\": None, \"replicate\": None, \"source_file\": p.name,\n",
    "    }\n",
    "    m = re.search(r\"(?:^|[_-])bin[_-]?(\\d+)(?=[_-]|$)\", name, flags=re.I)\n",
    "    if m:\n",
    "        meta[\"bin\"] = int(m.group(1))\n",
    "    else:\n",
    "        m2 = re.match(r\"^(\\d+)(?=[_-])\", name)\n",
    "        if m2:\n",
    "            meta[\"bin\"] = int(m2.group(1))\n",
    "    exp_ids = _extract_id_list(name, \"pos\")\n",
    "    ctl_ids = _extract_id_list(name, \"neg\")\n",
    "    if exp_ids is not None:\n",
    "        meta[\"experiments_ids\"] = \",\".join(str(x) for x in exp_ids)\n",
    "        meta[\"experiments_n\"] = len(exp_ids)\n",
    "        meta[\"experiments\"] = len(exp_ids)\n",
    "    if ctl_ids is not None:\n",
    "        meta[\"controls_ids\"] = \",\".join(str(x) for x in ctl_ids)\n",
    "        meta[\"controls_n\"] = len(ctl_ids)\n",
    "        meta[\"controls\"] = len(ctl_ids)\n",
    "    reg_tokens = [m.group(1).lower() for m in re.finditer(\n",
    "        r\"(?:^|[_-])(negabs|posabs|neg|pos)(?=[_-]|$)\", name, flags=re.I\n",
    "    )]\n",
    "    if reg_tokens:\n",
    "        token = reg_tokens[-1]\n",
    "        reg_map = {\"negabs\": \"downregulated\", \"neg\": \"downregulated\",\n",
    "                   \"posabs\": \"upregulated\", \"pos\": \"upregulated\"}\n",
    "        meta[\"regulation\"] = reg_map.get(token)\n",
    "    m = re.search(r\"(?:^|[_-])run([A-Za-z])(?=[_-]|$)\", name)\n",
    "    if m:\n",
    "        meta[\"replicate\"] = m.group(1).upper()\n",
    "    return meta\n",
    "\n",
    "def load_space_separated(path: str | Path) -> pd.DataFrame:\n",
    "    path = Path(path)\n",
    "    try:\n",
    "        df = pd.read_csv(path, sep=r\"\\s+\", engine=\"python\", header=None,\n",
    "                         names=[\"mass\", \"intensity\"], comment=\"#\")\n",
    "    except Exception:\n",
    "        df = pd.read_csv(path, header=None)\n",
    "        if df.shape[1] >= 2:\n",
    "            df = df.iloc[:, :2]; df.columns = [\"mass\", \"intensity\"]\n",
    "        else:\n",
    "            raise ValueError(\"Deconvoluted file must have at least two columns: mass intensity\")\n",
    "    return df\n",
    "\n",
    "def detect_signals(df: pd.DataFrame, params: PeakFindingParams = PeakFindingParams()) -> pd.DataFrame:\n",
    "    if not {\"mass\", \"intensity\"}.issubset(df.columns):\n",
    "        if df.shape[1] >= 2:\n",
    "            df = df.copy()\n",
    "            df.columns = [\"mass\", \"intensity\"] + [f\"col{i}\" for i in range(2, df.shape[1])]\n",
    "        else:\n",
    "            raise ValueError(\"Input DataFrame must have columns ['mass','intensity'].\")\n",
    "    df = df.replace([np.inf, -np.inf], np.nan).dropna(subset=[\"mass\", \"intensity\"])\n",
    "    df = df.sort_values(\"mass\").reset_index(drop=True)\n",
    "    x = df[\"mass\"].to_numpy(float); y = df[\"intensity\"].to_numpy(float)\n",
    "    y_proc = _smooth(y, params.smooth_window)\n",
    "    sigma = _mad_sigma(y_proc); ymax = float(np.max(y_proc)) if y_proc.size else 0.0\n",
    "    min_prom = params.min_prominence or max(6.0 * sigma, 0.001 * ymax)\n",
    "    min_h    = params.min_height     or max(4.0 * sigma, 0.0005 * ymax)\n",
    "    peaks, props = find_peaks(y_proc, prominence=min_prom, height=min_h,\n",
    "                              distance=max(1, int(params.min_distance_pts)))\n",
    "    out = pd.DataFrame({\n",
    "        \"mass\": x[peaks],\n",
    "        \"intensity\": y[peaks],\n",
    "        \"prominence\": props.get(\"prominences\", np.full(peaks.shape, np.nan)),\n",
    "        \"left_base_idx\": props.get(\"left_bases\", np.full(peaks.shape, -1)),\n",
    "        \"right_base_idx\": props.get(\"right_bases\", np.full(peaks.shape, -1)),\n",
    "    })\n",
    "    snr_den = sigma if sigma > 0 else (np.std(y_proc) if y_proc.size else 1.0)\n",
    "    snr_den = snr_den if snr_den > 0 else 1.0\n",
    "    out[\"snr\"] = out[\"intensity\"] / snr_den\n",
    "    if params.min_snr > 0:\n",
    "        out = out[out[\"snr\"] >= params.min_snr].reset_index(drop=True)\n",
    "    return out.sort_values(\"intensity\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "def plot_spectrum_with_peaks(df: pd.DataFrame, peaks_df: pd.DataFrame,\n",
    "                             out_png: str | Path | None = None,\n",
    "                             title: str = \"Detected Neutral-Mass Signals\") -> None:\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.plot(df[\"mass\"].to_numpy(), df[\"intensity\"].to_numpy(), linewidth=1)\n",
    "    if peaks_df is not None and not peaks_df.empty:\n",
    "        plt.scatter(peaks_df[\"mass\"].to_numpy(),\n",
    "                    peaks_df[\"intensity\"].to_numpy(), s=18)\n",
    "    plt.xlabel(\"Neutral mass (Da)\")\n",
    "    plt.ylabel(\"Intensity (arb.)\")\n",
    "    plt.title(title)\n",
    "    plt.tight_layout()\n",
    "    if out_png:\n",
    "        plt.savefig(out_png, dpi=150)\n",
    "    plt.close()\n",
    "\n",
    "# ============================================================\n",
    "# -----------------  CHARGE-SERIES MATCHING  -----------------\n",
    "# ============================================================\n",
    "PROTON_MASS = 1.007276466812  # Da\n",
    "Z_MIN, Z_MAX = 5, 50\n",
    "PPM_TOL = 1000.0\n",
    "ABS_DA_TOL = 1.0\n",
    "MIN_MATCHED_CHARGE_STATES = 4\n",
    "\n",
    "def _read_raw_ms1(path: str) -> pd.DataFrame:\n",
    "    try:\n",
    "        df = pd.read_csv(path)\n",
    "    except Exception:\n",
    "        df = pd.read_csv(path, header=None)\n",
    "    if df.shape[1] == 2:\n",
    "        df.columns = [\"mz\", \"intensity\"]\n",
    "    else:\n",
    "        cols_lower = [str(c).lower() for c in df.columns]\n",
    "        mz_candidates = [i for i, c in enumerate(cols_lower)\n",
    "                         if (\"mz\" in c) or (\"m/z\" in c) or (\"mass/charge\" in c) or (c.strip() == \"m z\")]\n",
    "        int_candidates = [i for i, c in enumerate(cols_lower)\n",
    "                          if (\"int\" in c) or (\"abund\" in c) or (\"height\" in c) or (\"signal\" in c)]\n",
    "        if not mz_candidates: mz_candidates = [0]\n",
    "        if not int_candidates: int_candidates = [1 if df.shape[1] > 1 else 0]\n",
    "        df = df.iloc[:, [mz_candidates[0], int_candidates[0]]].copy()\n",
    "        df.columns = [\"mz\", \"intensity\"]\n",
    "    df = df.replace([np.inf, -np.inf], np.nan).dropna()\n",
    "    df = df[df[\"intensity\"] > 0].copy()\n",
    "    df[\"mz\"] = pd.to_numeric(df[\"mz\"], errors=\"coerce\")\n",
    "    df[\"intensity\"] = pd.to_numeric(df[\"intensity\"], errors=\"coerce\")\n",
    "    df = df.dropna().sort_values(\"mz\").reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "def _ppm_window(target_mz: float, ppm: float, abs_da: float) -> tuple[float, float]:\n",
    "    da = target_mz * ppm * 1e-6\n",
    "    tol = max(da, abs_da)\n",
    "    return target_mz - tol, target_mz + tol\n",
    "\n",
    "def _match_targets(sorted_mz: np.ndarray, targets: np.ndarray,\n",
    "                   ppm: float, abs_da: float, available_mask: np.ndarray) -> dict[int, int | None]:\n",
    "    results: dict[int, int | None] = {}\n",
    "    for ti, t in enumerate(targets):\n",
    "        lo, hi = _ppm_window(t, ppm, abs_da)\n",
    "        j = bisect_left(sorted_mz, t)\n",
    "        best_idx = None; best_delta = float(\"inf\")\n",
    "        for k in (j, j-1, j+1, j-2, j+2, j-3, j+3):\n",
    "            if 0 <= k < len(sorted_mz):\n",
    "                mz_k = sorted_mz[k]\n",
    "                if available_mask[k] and (lo <= mz_k <= hi):\n",
    "                    delta = abs(mz_k - t)\n",
    "                    if delta < best_delta:\n",
    "                        best_delta = delta; best_idx = k\n",
    "        results[ti] = best_idx\n",
    "    return results\n",
    "\n",
    "def _generate_charge_series(neutral_mass: float, z_min: int, z_max: int) -> pd.DataFrame:\n",
    "    z = np.arange(z_min, z_max + 1, dtype=int)\n",
    "    mz = (neutral_mass + z * PROTON_MASS) / z\n",
    "    return pd.DataFrame({\"z\": z, \"target_mz\": mz})\n",
    "\n",
    "def assign_ms1_peaks(raw_df: pd.DataFrame, deconv_peaks_df: pd.DataFrame,\n",
    "                     meta: dict | None = None) -> tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    raw_df = raw_df.sort_values(\"mz\").reset_index(drop=True)\n",
    "    mz_arr = raw_df[\"mz\"].to_numpy()\n",
    "    inten_arr = raw_df[\"intensity\"].to_numpy()\n",
    "    available = np.ones(len(raw_df), dtype=bool)\n",
    "\n",
    "    assigned_mass = np.full(len(raw_df), np.nan)\n",
    "    assigned_z    = np.full(len(raw_df), np.nan)\n",
    "\n",
    "    summary_rows = []\n",
    "\n",
    "    for r in deconv_peaks_df.itertuples(index=False):\n",
    "        mass = float(r.mass)\n",
    "        mass_intensity = float(r.intensity)\n",
    "        mass_snr = float(getattr(r, \"snr\", np.nan))\n",
    "\n",
    "        series = _generate_charge_series(mass, Z_MIN, Z_MAX)\n",
    "        targets = series[\"target_mz\"].to_numpy()\n",
    "        matches = _match_targets(mz_arr, targets, PPM_TOL, ABS_DA_TOL, available_mask=available)\n",
    "\n",
    "        matched_indices = []\n",
    "        matched_z_list  = []\n",
    "        matched_mz_list = []\n",
    "\n",
    "        for ti, k in matches.items():\n",
    "            if k is not None:\n",
    "                matched_indices.append(k)\n",
    "                matched_z_list.append(int(series.iloc[ti][\"z\"]))\n",
    "                matched_mz_list.append(mz_arr[k])\n",
    "\n",
    "        if len(matched_indices) >= MIN_MATCHED_CHARGE_STATES:\n",
    "            for idx, z_val in zip(matched_indices, matched_z_list):\n",
    "                if available[idx]:\n",
    "                    available[idx] = False\n",
    "                    assigned_mass[idx] = mass\n",
    "                    assigned_z[idx] = z_val\n",
    "\n",
    "            frac_intensity_removed = (\n",
    "                float(np.sum(inten_arr[matched_indices])) / float(np.sum(inten_arr))\n",
    "                if inten_arr.sum() > 0 else 0.0\n",
    "            )\n",
    "\n",
    "            row = {\n",
    "                \"neutral_mass\": mass,\n",
    "                \"deconv_intensity\": mass_intensity,\n",
    "                \"snr\": mass_snr,\n",
    "                \"n_matches\": len(matched_indices),\n",
    "                \"matched_z_list\": json.dumps(matched_z_list),\n",
    "                \"matched_mz_list\": json.dumps([round(float(x), 1) for x in matched_mz_list]),\n",
    "                \"ppm_tol\": PPM_TOL,\n",
    "                \"abs_da_tol\": ABS_DA_TOL,\n",
    "                \"fraction_total_intensity_captured\": frac_intensity_removed\n",
    "            }\n",
    "            if meta:\n",
    "                row.update({\n",
    "                    \"bin\": meta.get(\"bin\"),\n",
    "                    \"experiments_ids\": meta.get(\"experiments_ids\"),\n",
    "                    \"controls_ids\": meta.get(\"controls_ids\"),\n",
    "                    \"regulation\": meta.get(\"regulation\"),\n",
    "                    \"replicate\": meta.get(\"replicate\"),\n",
    "                    \"source_file\": meta.get(\"source_file\"),\n",
    "                })\n",
    "            summary_rows.append(row)\n",
    "\n",
    "    assigned_raw = raw_df.copy()\n",
    "    assigned_raw[\"assigned_mass\"] = assigned_mass\n",
    "    assigned_raw[\"assigned_charge\"] = assigned_z\n",
    "    assigned_raw[\"is_assigned\"] = ~np.isnan(assigned_mass)\n",
    "\n",
    "    assignments_summary = pd.DataFrame(summary_rows).sort_values(\n",
    "        \"deconv_intensity\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "    if meta and \"bin\" in meta:\n",
    "        assigned_raw[\"bin\"] = meta[\"bin\"]\n",
    "\n",
    "    return assigned_raw, assignments_summary\n",
    "\n",
    "# ============================================================\n",
    "# ----------------------  PLOTTING  --------------------------\n",
    "# ============================================================\n",
    "def plot_neutral_mass_spectrum(deconv_peaks_df: pd.DataFrame, out_dir: str,\n",
    "                               filename: str = \"neutral_mass_spectrum.png\"):\n",
    "    if deconv_peaks_df.empty:\n",
    "        print(\"No neutral masses to plot.\"); return\n",
    "    masses = deconv_peaks_df[\"mass\"].to_numpy()\n",
    "    intens = deconv_peaks_df[\"intensity\"].to_numpy()\n",
    "    plt.figure(figsize=(9, 4.5))\n",
    "    plt.vlines(masses, 0, intens, linewidth=1)\n",
    "    plt.xlabel(\"Neutral mass (Da)\"); plt.ylabel(\"Intensity (arb.)\")\n",
    "    plt.title(\"Neutral Mass Spectrum (detected peaks)\")\n",
    "    plt.tight_layout(); out_path = os.path.join(out_dir, filename)\n",
    "    plt.savefig(out_path, dpi=200); plt.close()\n",
    "    print(f\"Saved plot: {out_path}\")\n",
    "\n",
    "def plot_mirror_assigned_vs_total(assigned_raw: pd.DataFrame, out_dir: str,\n",
    "                                  filename: str = \"mirror_assigned_vs_total.png\"):\n",
    "    if assigned_raw.empty:\n",
    "        print(\"No assigned/raw data to plot.\"); return\n",
    "    mz = assigned_raw[\"mz\"].to_numpy()\n",
    "    total_int = assigned_raw[\"intensity\"].to_numpy()\n",
    "    assigned_mask = assigned_raw[\"is_assigned\"].to_numpy(dtype=bool)\n",
    "    assigned_int = np.where(assigned_mask, total_int, 0.0)\n",
    "    plt.figure(figsize=(10, 5.2))\n",
    "    plt.vlines(mz, 0, total_int, linewidth=0.6)\n",
    "    plt.vlines(mz[assigned_mask], 0, -assigned_int[assigned_mask], linewidth=0.8)\n",
    "    ymax = total_int.max() if len(total_int) else 1.0\n",
    "    ymin = -assigned_int.max() if assigned_int.any() else -0.1 * ymax\n",
    "    plt.ylim(ymin * 1.05, ymax * 1.05)\n",
    "    plt.xlabel(\"m/z\"); plt.ylabel(\"Intensity (arb.)\")\n",
    "    plt.title(\"Mirror Plot: Total (top) vs Assigned (bottom)\")\n",
    "    plt.tight_layout(); out_path = os.path.join(out_dir, filename)\n",
    "    plt.savefig(out_path, dpi=200); plt.close()\n",
    "    print(f\"Saved plot: {out_path}\")\n",
    "\n",
    "def plot_mirror_unassigned_vs_total(assigned_raw: pd.DataFrame, out_dir: str,\n",
    "                                    filename: str = \"mirror_unassigned_vs_total.png\"):\n",
    "    if assigned_raw.empty:\n",
    "        print(\"No assigned/raw data to plot.\"); return\n",
    "    mz = assigned_raw[\"mz\"].to_numpy()\n",
    "    total_int = assigned_raw[\"intensity\"].to_numpy()\n",
    "    unassigned_mask = ~assigned_raw[\"is_assigned\"].to_numpy(dtype=bool)\n",
    "    unassigned_int = np.where(unassigned_mask, total_int, 0.0)\n",
    "    plt.figure(figsize=(10, 5.2))\n",
    "    plt.vlines(mz, 0, total_int, linewidth=0.6)\n",
    "    plt.vlines(mz[unassigned_mask], 0, -unassigned_int[unassigned_mask], linewidth=0.8)\n",
    "    ymax = total_int.max() if len(total_int) else 1.0\n",
    "    ymin = -unassigned_int.max() if unassigned_int.any() else -0.1 * ymax\n",
    "    plt.ylim(ymin * 1.05, ymax * 1.05)\n",
    "    plt.xlabel(\"m/z\"); plt.ylabel(\"Intensity (arb.)\")\n",
    "    plt.title(\"Mirror Plot: Total (top) vs Non-assigned (bottom)\")\n",
    "    plt.tight_layout(); out_path = os.path.join(out_dir, filename)\n",
    "    plt.savefig(out_path, dpi=200); plt.close()\n",
    "    print(f\"Saved plot: {out_path}\")\n",
    "\n",
    "def plot_mirror_assigned_by_protein_vs_total(assigned_raw: pd.DataFrame, out_dir: str,\n",
    "    filename: str = \"mirror_assigned_by_protein_vs_total.png\", max_legend_items: int = 20):\n",
    "    if assigned_raw.empty:\n",
    "        print(\"No assigned/raw data to plot.\"); return\n",
    "    mz = assigned_raw[\"mz\"].to_numpy()\n",
    "    total_int = assigned_raw[\"intensity\"].to_numpy()\n",
    "    plt.figure(figsize=(11, 5.6))\n",
    "    plt.vlines(mz, 0, total_int, linewidth=0.5)\n",
    "    df_assigned_only = assigned_raw[assigned_raw[\"is_assigned\"]].copy()\n",
    "    if df_assigned_only.empty:\n",
    "        plt.xlabel(\"m/z\"); plt.ylabel(\"Intensity (arb.)\")\n",
    "        plt.title(\"Mirror Plot: Total (top) vs Assigned by Protein (bottom)\")\n",
    "        plt.tight_layout(); out_path = os.path.join(out_dir, filename)\n",
    "        plt.savefig(out_path, dpi=200); plt.close(); print(f\"Saved plot: {out_path}\"); return\n",
    "    counts = (df_assigned_only.groupby(\"assigned_mass\", dropna=True)[\"is_assigned\"]\n",
    "              .count().sort_values(ascending=False))\n",
    "    proteins_in_order = counts.index.tolist()\n",
    "    color_cycle = plt.rcParams['axes.prop_cycle'].by_key().get(\n",
    "        'color', ['C0','C1','C2','C3','C4','C5','C6','C7','C8','C9'])\n",
    "    legend_entries = 0\n",
    "    for i, mass in enumerate(proteins_in_order):\n",
    "        mask = (assigned_raw[\"assigned_mass\"] == mass)\n",
    "        mz_i = assigned_raw.loc[mask, \"mz\"].to_numpy()\n",
    "        inten_i = assigned_raw.loc[mask, \"intensity\"].to_numpy()\n",
    "        label = None\n",
    "        if legend_entries < max_legend_items:\n",
    "            label = f\"{mass/1000:.2f} kDa (n={len(mz_i)})\"; legend_entries += 1\n",
    "        plt.vlines(mz_i, 0, -inten_i, linewidth=0.8,\n",
    "                   color=color_cycle[i % len(color_cycle)], label=label)\n",
    "    ymax = total_int.max() if len(total_int) else 1.0\n",
    "    ymin = -df_assigned_only[\"intensity\"].max() if len(df_assigned_only) else -0.1 * ymax\n",
    "    plt.ylim(ymin * 1.05, ymax * 1.05)\n",
    "    if legend_entries:\n",
    "        plt.legend(title=\"Assigned proteins\", loc=\"upper right\", fontsize=8, ncol=1)\n",
    "    plt.xlabel(\"m/z\"); plt.ylabel(\"Intensity (arb.)\")\n",
    "    plt.title(\"Mirror Plot: Total (top) vs Assigned by Protein (bottom)\")\n",
    "    plt.tight_layout(); out_path = os.path.join(out_dir, filename)\n",
    "    plt.savefig(out_path, dpi=200); plt.close(); print(f\"Saved plot: {out_path}\")\n",
    "\n",
    "# ============================================================\n",
    "# ----------------------  BATCH MAIN  ------------------------\n",
    "# ============================================================\n",
    "\n",
    "# ---------------------- User-configurable ----------------------\n",
    "RAW_DIR    = r\"F:\\binary\\final\\raw\"\n",
    "DECONV_DIR = r\"F:\\binary\\final\\decon\"\n",
    "OUT_DIR    = r\"F:\\binary\\final\\firstpass\"\n",
    "\n",
    "RAW_GLOB    = \"*.csv\"\n",
    "DECONV_GLOB = \"*_mass.txt\"\n",
    "\n",
    "DECONV_DETECT_PARAMS = PeakFindingParams(\n",
    "    min_distance_pts=20,\n",
    "    min_snr=10,\n",
    "    smooth_window=0,\n",
    ")\n",
    "\n",
    "# >>> Set max neutral mass here (kDa). Use None to disable the filter.\n",
    "MAX_PROTEIN_MASS_KDA: float | None = 25.0  # e.g., 80 kDa; set to None for no limit\n",
    "\n",
    "def _base_key_from_deconv(path: Path) -> str:\n",
    "    stem = path.stem\n",
    "    return stem[:-5] if stem.endswith(\"_mass\") else stem\n",
    "\n",
    "def _base_key_from_raw(path: Path) -> str:\n",
    "    return path.stem\n",
    "\n",
    "def _ensure_dir(p: str | Path) -> None:\n",
    "    Path(p).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def process_one_pair(deconv_path: Path, raw_path: Path | None) -> None:\n",
    "    base = _base_key_from_deconv(deconv_path)\n",
    "    out_root = Path(OUT_DIR) / base\n",
    "    _ensure_dir(out_root)\n",
    "\n",
    "    meta = parse_metadata_from_filename(deconv_path)\n",
    "    deconv_raw = load_space_separated(deconv_path)\n",
    "\n",
    "    # --- apply max mass in kDa (converted to Da) BEFORE detection ---\n",
    "    if MAX_PROTEIN_MASS_KDA is not None and np.isfinite(MAX_PROTEIN_MASS_KDA):\n",
    "        cutoff_da = float(MAX_PROTEIN_MASS_KDA) * 1000.0\n",
    "        before = len(deconv_raw)\n",
    "        deconv_raw = deconv_raw[deconv_raw[\"mass\"] <= cutoff_da].reset_index(drop=True)\n",
    "        after = len(deconv_raw)\n",
    "        print(f\"[{base}] Applied max mass filter ‚â§ {MAX_PROTEIN_MASS_KDA:.3f} kDa ({cutoff_da:.1f} Da): kept {after}/{before} points.\")\n",
    "    else:\n",
    "        cutoff_da = None\n",
    "\n",
    "    # detect peaks\n",
    "    deconv_peaks = detect_signals(deconv_raw, params=DECONV_DETECT_PARAMS)\n",
    "\n",
    "    # attach metadata + filter provenance\n",
    "    add = {}\n",
    "    if cutoff_da is not None:\n",
    "        add = {\"max_mass_filter_kDa\": float(MAX_PROTEIN_MASS_KDA),\n",
    "               \"max_mass_filter_Da\": float(cutoff_da)}\n",
    "    deconv_peaks = deconv_peaks.assign(\n",
    "        bin=meta.get(\"bin\"),\n",
    "        experiments_ids=meta.get(\"experiments_ids\"),\n",
    "        controls_ids=meta.get(\"controls_ids\"),\n",
    "        regulation=meta.get(\"regulation\"),\n",
    "        replicate=meta.get(\"replicate\"),\n",
    "        source_file=meta.get(\"source_file\"),\n",
    "        **add\n",
    "    )\n",
    "\n",
    "    # save + plot detections\n",
    "    out_detect_csv = out_root / f\"{base}_detected_signals.csv\"\n",
    "    out_detect_png = out_root / f\"{base}_detected_signals.png\"\n",
    "    deconv_peaks.to_csv(out_detect_csv, index=False)\n",
    "    plot_spectrum_with_peaks(deconv_raw, deconv_peaks, out_png=str(out_detect_png))\n",
    "    print(f\"[{base}] Neutral-mass detection: {len(deconv_peaks)} peaks ‚Üí {out_detect_csv}\")\n",
    "    print(f\"[{base}] Detection plot saved ‚Üí {out_detect_png}\")\n",
    "    print(f\"[{base}] Parsed filename metadata: {meta}\")\n",
    "\n",
    "    # assignment (if raw exists)\n",
    "    if raw_path is None or not raw_path.exists():\n",
    "        print(f\"[{base}] ‚ö† No matching RAW CSV found. Skipping assignment.\")\n",
    "        return\n",
    "\n",
    "    raw_df = _read_raw_ms1(raw_path)\n",
    "    assigned_raw, summary = assign_ms1_peaks(raw_df, deconv_peaks, meta=meta)\n",
    "    if cutoff_da is not None and not summary.empty:\n",
    "        summary[\"max_mass_filter_kDa\"] = float(MAX_PROTEIN_MASS_KDA)\n",
    "        summary[\"max_mass_filter_Da\"]  = float(cutoff_da)\n",
    "\n",
    "    out_assigned = out_root / f\"{base}_assigned_ms1_with_peaks.csv\"\n",
    "    out_summary  = out_root / f\"{base}_assignments_summary.csv\"\n",
    "    assigned_raw.to_csv(out_assigned, index=False)\n",
    "    summary.to_csv(out_summary, index=False)\n",
    "\n",
    "    # plots\n",
    "    plot_neutral_mass_spectrum(deconv_peaks, str(out_root), filename=f\"{base}_neutral_mass_spectrum.png\")\n",
    "    plot_mirror_assigned_vs_total(assigned_raw, str(out_root), filename=f\"{base}_mirror_assigned_vs_total.png\")\n",
    "    plot_mirror_unassigned_vs_total(assigned_raw, str(out_root), filename=f\"{base}_mirror_unassigned_vs_total.png\")\n",
    "    plot_mirror_assigned_by_protein_vs_total(\n",
    "        assigned_raw, str(out_root), filename=f\"{base}_mirror_assigned_by_protein_vs_total.png\", max_legend_items=20\n",
    "    )\n",
    "\n",
    "    print(f\"\\n=== [{base}] Summary ===\")\n",
    "    print(f\"Raw MS1 peaks (rows): {len(raw_df):,}\")\n",
    "    print(f\"Detected neutral-mass peaks: {len(deconv_peaks):,}\")\n",
    "    print(f\"Assigned raw peaks: {int(assigned_raw['is_assigned'].sum()):,}\")\n",
    "    print(f\"Non-assigned raw peaks: {int((~assigned_raw['is_assigned']).sum()):,}\")\n",
    "    if cutoff_da is not None:\n",
    "        print(f\"Max mass filter: ‚â§ {MAX_PROTEIN_MASS_KDA:.3f} kDa ({cutoff_da:.1f} Da)\")\n",
    "    print(f\"Saved: {out_assigned}\")\n",
    "    print(f\"Saved: {out_summary}\")\n",
    "    print(\"------------------------------------------------------------\")\n",
    "\n",
    "def main():\n",
    "    _ensure_dir(OUT_DIR)\n",
    "\n",
    "    raw_files = [Path(p) for p in glob.glob(str(Path(RAW_DIR) / RAW_GLOB))]\n",
    "    raw_index = {_base_key_from_raw(p): p for p in raw_files}\n",
    "\n",
    "    deconv_files = [Path(p) for p in glob.glob(str(Path(DECONV_DIR) / DECONV_GLOB))]\n",
    "    if not deconv_files:\n",
    "        print(f\"‚ö† No deconvoluted files found in: {DECONV_DIR} (pattern: {DECONV_GLOB})\")\n",
    "        return\n",
    "\n",
    "    print(f\"Found {len(deconv_files)} decon file(s) in {DECONV_DIR}\")\n",
    "    print(f\"Found {len(raw_files)} raw CSV file(s) in {RAW_DIR}\")\n",
    "    if MAX_PROTEIN_MASS_KDA is not None:\n",
    "        print(f\"Applying maximum protein mass filter: ‚â§ {MAX_PROTEIN_MASS_KDA:.3f} kDa\")\n",
    "\n",
    "    for deconv_path in sorted(deconv_files):\n",
    "        base = _base_key_from_deconv(deconv_path)\n",
    "        raw_path = raw_index.get(base, None)\n",
    "        try:\n",
    "            process_one_pair(deconv_path, raw_path)\n",
    "        except Exception as e:\n",
    "            print(f\"[{base}] ‚ùå Error: {e}\")\n",
    "\n",
    "    print(\"‚úÖ Batch processing complete.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3f0ca97",
   "metadata": {},
   "source": [
    "Combining the reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "01fb70d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8 summary file(s). Copying to F:\\binary\\final\\report...\n",
      "Copied: F:\\binary\\final\\firstpass\\frac_pellet_grads_AB__pos_1__neg_0_negabs_runA\\frac_pellet_grads_AB__pos_1__neg_0_negabs_runA_assignments_summary.csv ‚Üí F:\\binary\\final\\report\\frac_pellet_grads_AB__pos_1__neg_0_negabs_runA_assignments_summary.csv\n",
      "Copied: F:\\binary\\final\\firstpass\\frac_pellet_grads_AB__pos_1__neg_0_negabs_runB\\frac_pellet_grads_AB__pos_1__neg_0_negabs_runB_assignments_summary.csv ‚Üí F:\\binary\\final\\report\\frac_pellet_grads_AB__pos_1__neg_0_negabs_runB_assignments_summary.csv\n",
      "Copied: F:\\binary\\final\\firstpass\\frac_pellet_grads_AB__pos_1__neg_0_pos_runA\\frac_pellet_grads_AB__pos_1__neg_0_pos_runA_assignments_summary.csv ‚Üí F:\\binary\\final\\report\\frac_pellet_grads_AB__pos_1__neg_0_pos_runA_assignments_summary.csv\n",
      "Copied: F:\\binary\\final\\firstpass\\frac_pellet_grads_AB__pos_1__neg_0_pos_runB\\frac_pellet_grads_AB__pos_1__neg_0_pos_runB_assignments_summary.csv ‚Üí F:\\binary\\final\\report\\frac_pellet_grads_AB__pos_1__neg_0_pos_runB_assignments_summary.csv\n",
      "Copied: F:\\binary\\final\\firstpass\\frac_soluble_grads_AB__pos_1__neg_0_negabs_runA\\frac_soluble_grads_AB__pos_1__neg_0_negabs_runA_assignments_summary.csv ‚Üí F:\\binary\\final\\report\\frac_soluble_grads_AB__pos_1__neg_0_negabs_runA_assignments_summary.csv\n",
      "Copied: F:\\binary\\final\\firstpass\\frac_soluble_grads_AB__pos_1__neg_0_negabs_runB\\frac_soluble_grads_AB__pos_1__neg_0_negabs_runB_assignments_summary.csv ‚Üí F:\\binary\\final\\report\\frac_soluble_grads_AB__pos_1__neg_0_negabs_runB_assignments_summary.csv\n",
      "Copied: F:\\binary\\final\\firstpass\\frac_soluble_grads_AB__pos_1__neg_0_pos_runA\\frac_soluble_grads_AB__pos_1__neg_0_pos_runA_assignments_summary.csv ‚Üí F:\\binary\\final\\report\\frac_soluble_grads_AB__pos_1__neg_0_pos_runA_assignments_summary.csv\n",
      "Copied: F:\\binary\\final\\firstpass\\frac_soluble_grads_AB__pos_1__neg_0_pos_runB\\frac_soluble_grads_AB__pos_1__neg_0_pos_runB_assignments_summary.csv ‚Üí F:\\binary\\final\\report\\frac_soluble_grads_AB__pos_1__neg_0_pos_runB_assignments_summary.csv\n",
      "‚úÖ Copy complete.\n",
      "Reading F:\\binary\\final\\report\\frac_pellet_grads_AB__pos_1__neg_0_negabs_runA_assignments_summary.csv\n",
      "Reading F:\\binary\\final\\report\\frac_pellet_grads_AB__pos_1__neg_0_negabs_runB_assignments_summary.csv\n",
      "Reading F:\\binary\\final\\report\\frac_pellet_grads_AB__pos_1__neg_0_pos_runA_assignments_summary.csv\n",
      "Reading F:\\binary\\final\\report\\frac_pellet_grads_AB__pos_1__neg_0_pos_runB_assignments_summary.csv\n",
      "Reading F:\\binary\\final\\report\\frac_soluble_grads_AB__pos_1__neg_0_negabs_runA_assignments_summary.csv\n",
      "Reading F:\\binary\\final\\report\\frac_soluble_grads_AB__pos_1__neg_0_negabs_runB_assignments_summary.csv\n",
      "Reading F:\\binary\\final\\report\\frac_soluble_grads_AB__pos_1__neg_0_pos_runA_assignments_summary.csv\n",
      "Reading F:\\binary\\final\\report\\frac_soluble_grads_AB__pos_1__neg_0_pos_runB_assignments_summary.csv\n",
      "‚úÖ Combined 8 files ‚Üí F:\\binary\\final\\report\\report.csv\n",
      "üß™ Added 'fractions' column with values 'Pellet' or 'Soluble'.\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Collect all *_assignments_summary.csv files from subfolders into one folder,\n",
    "then concatenate them into a single report.csv.\n",
    "\n",
    "Steps:\n",
    "1. Search recursively for *_assignments_summary.csv in BATCH_OUT_DIR.\n",
    "2. Copy all to SUMMARY_OUT (renaming duplicates).\n",
    "3. Concatenate all collected CSVs ‚Üí report.csv.\n",
    "4. Add 'fractions' column based on 'source_file' content:\n",
    "      - \"Pellet\"  if '_pellet_' in source_file\n",
    "      - \"Soluble\" if '_soluble_' in source_file\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "# ---------------------- USER SETTINGS ----------------------\n",
    "BATCH_OUT_DIR = r\"F:\\binary\\final\\firstpass\"     # where all subfolders were created\n",
    "SUMMARY_OUT   = r\"F:\\binary\\final\\report\"        # folder to collect summaries\n",
    "PATTERN       = \"*_assignments_summary.csv\"      # filename pattern\n",
    "OUTPUT_FILE   = Path(SUMMARY_OUT) / \"report.csv\"\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "def collect_summary_files():\n",
    "    \"\"\"Collect all *_assignments_summary.csv into SUMMARY_OUT.\"\"\"\n",
    "    os.makedirs(SUMMARY_OUT, exist_ok=True)\n",
    "    summary_files = glob.glob(str(Path(BATCH_OUT_DIR) / \"**\" / PATTERN), recursive=True)\n",
    "\n",
    "    if not summary_files:\n",
    "        print(\"‚ö† No summary files found.\")\n",
    "        return []\n",
    "\n",
    "    print(f\"Found {len(summary_files)} summary file(s). Copying to {SUMMARY_OUT}...\")\n",
    "    copied = []\n",
    "    for f in summary_files:\n",
    "        src = Path(f)\n",
    "        dst = Path(SUMMARY_OUT) / src.name\n",
    "        if dst.exists():\n",
    "            dst = Path(SUMMARY_OUT) / f\"{src.parent.name}_{src.name}\"\n",
    "        shutil.copy2(src, dst)\n",
    "        copied.append(dst)\n",
    "        print(f\"Copied: {src} ‚Üí {dst}\")\n",
    "\n",
    "    print(\"‚úÖ Copy complete.\")\n",
    "    return copied\n",
    "\n",
    "\n",
    "def concat_csvs(folder_path: str, output_file: str):\n",
    "    \"\"\"Concatenate all CSV files in folder_path ‚Üí output_file with 'fractions' column.\"\"\"\n",
    "    csv_files = [f for f in os.listdir(folder_path) if f.endswith(\".csv\")]\n",
    "    if not csv_files:\n",
    "        raise FileNotFoundError(\"No CSV files found in the folder!\")\n",
    "\n",
    "    df_list = []\n",
    "    for file in csv_files:\n",
    "        file_path = os.path.join(folder_path, file)\n",
    "        print(f\"Reading {file_path}\")\n",
    "        df = pd.read_csv(file_path)\n",
    "\n",
    "        # Determine fraction based on file name or source_file column\n",
    "        fraction = None\n",
    "        if '_pellet_' in file.lower():\n",
    "            fraction = \"Pellet\"\n",
    "        elif '_soluble_' in file.lower():\n",
    "            fraction = \"Soluble\"\n",
    "        else:\n",
    "            # Try checking inside the dataframe if it has 'source_file' column\n",
    "            if 'source_file' in df.columns:\n",
    "                if df['source_file'].str.contains('_pellet_', case=False, na=False).any():\n",
    "                    fraction = \"Pellet\"\n",
    "                elif df['source_file'].str.contains('_soluble_', case=False, na=False).any():\n",
    "                    fraction = \"Soluble\"\n",
    "\n",
    "        # Default to \"Unknown\" if not detected\n",
    "        df['fractions'] = fraction if fraction else \"Unknown\"\n",
    "        df_list.append(df)\n",
    "\n",
    "    combined_df = pd.concat(df_list, ignore_index=True)\n",
    "    combined_df.to_csv(output_file, index=False)\n",
    "    print(f\"‚úÖ Combined {len(csv_files)} files ‚Üí {output_file}\")\n",
    "    print(\"üß™ Added 'fractions' column with values 'Pellet' or 'Soluble'.\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    copied_files = collect_summary_files()\n",
    "    if copied_files:\n",
    "        concat_csvs(SUMMARY_OUT, OUTPUT_FILE)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76e45e0d",
   "metadata": {},
   "source": [
    "Quantification of all proteoforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "15558a3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: F:\\binary\\final\\report\\assignments_with_quant_sums_aaa.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import ast\n",
    "import pandas as pd\n",
    "\n",
    "# --------------------\n",
    "# Config (edit paths)\n",
    "# --------------------\n",
    "DATASET_RT_PATH = r\"F:\\binary\\final\\ms1_per_sample.csv\"          # wide matrix with cast_* columns\n",
    "ASSIGNMENTS_PATH = r\"F:\\binary\\final\\report\\report.csv\"          # has 'fractions' and 'matched_mz_list'\n",
    "OUT_PATH = os.path.join(\n",
    "    os.path.dirname(ASSIGNMENTS_PATH) or \".\",\n",
    "    \"assignments_with_quant_sums_aaa.csv\"\n",
    ")\n",
    "\n",
    "# --------------------\n",
    "# Helpers\n",
    "# --------------------\n",
    "def to_cast_col(n: float) -> str:\n",
    "    \"\"\"Map an m/z to its cast_* column name: int((mz-600)*10), zero-padded.\"\"\"\n",
    "    col_num = int((float(n) - 600.0) * 10.0)\n",
    "    return \"cast_\" + str(col_num).zfill(5)\n",
    "\n",
    "def parse_mz_list(val):\n",
    "    \"\"\"Safely parse matched_mz_list cells that look like '[864.9, 865.2, ...]'.\"\"\"\n",
    "    try:\n",
    "        out = ast.literal_eval(str(val))\n",
    "        if isinstance(out, (list, tuple)):\n",
    "            return [float(x) for x in out]\n",
    "    except Exception:\n",
    "        pass\n",
    "    return []\n",
    "\n",
    "# --------------------\n",
    "# Load data\n",
    "# --------------------\n",
    "df_rt = pd.read_csv(DATASET_RT_PATH)\n",
    "df_asn = pd.read_csv(ASSIGNMENTS_PATH)\n",
    "\n",
    "# Basic checks\n",
    "for col in [\"fractions\", \"target\"]:\n",
    "    if col not in df_rt.columns:\n",
    "        raise KeyError(f\"'{col}' column is required in dataset_rt.csv\")\n",
    "\n",
    "if \"fractions\" not in df_asn.columns or \"matched_mz_list\" not in df_asn.columns:\n",
    "    raise KeyError(\"assignments CSV must contain 'fractions' and 'matched_mz_list' columns\")\n",
    "\n",
    "# NEW columns to be added to assignments\n",
    "new_cols = [\"group_0_sum\", \"group_1_sum\", \"group_2_sum\", \"group_3_sum\",\n",
    "            \"n_mz_used\", \"n_mz_found\", \"missing_cast_columns\"]\n",
    "for c in new_cols:\n",
    "    if c in df_asn.columns:\n",
    "        # avoid accidental overwrite\n",
    "        df_asn.drop(columns=[c], inplace=True)\n",
    "\n",
    "# --------------------\n",
    "# Row-wise quantification\n",
    "# --------------------\n",
    "results = []\n",
    "for idx, row in df_asn.iterrows():\n",
    "    frac_value = row[\"fractions\"]  # keep as-is (can be string like 'soluble_fraction')\n",
    "    mz_list = parse_mz_list(row[\"matched_mz_list\"])\n",
    "    cast_cols = [to_cast_col(mz) for mz in mz_list]\n",
    "\n",
    "    # Filter dataset_rt to this fraction\n",
    "    df_frac = df_rt[df_rt[\"fractions\"] == frac_value]\n",
    "    if df_frac.empty:\n",
    "        res = dict(\n",
    "            group_0_sum=float(\"nan\"),\n",
    "            group_1_sum=float(\"nan\"),\n",
    "            group_2_sum=float(\"nan\"),\n",
    "            group_3_sum=float(\"nan\"),\n",
    "            n_mz_used=len(cast_cols),\n",
    "            n_mz_found=0,\n",
    "            missing_cast_columns=\", \".join(cast_cols) if cast_cols else \"\"\n",
    "        )\n",
    "        results.append(res)\n",
    "        continue\n",
    "\n",
    "    # Ensure target present\n",
    "    if \"target\" not in df_frac.columns:\n",
    "        raise KeyError(\"Column 'target' not found in dataset_rt.csv\")\n",
    "\n",
    "    existing = [c for c in cast_cols if c in df_frac.columns]\n",
    "    missing = [c for c in cast_cols if c not in df_frac.columns]\n",
    "\n",
    "    if not existing:\n",
    "        sums = {0: float(\"nan\"), 1: float(\"nan\"), 2: float(\"nan\"), 3: float(\"nan\")}\n",
    "    else:\n",
    "        # Sum intensities across all selected cast_* columns per target\n",
    "        grouped = df_frac.groupby(\"target\")[existing].sum()\n",
    "        total_per_target = grouped.sum(axis=1)  # sum across those cast_* columns\n",
    "        sums = {t: float(total_per_target.get(t, float(\"nan\"))) for t in [0, 1, 2, 3]}\n",
    "\n",
    "    res = dict(\n",
    "        group_0_sum=sums[0],\n",
    "        group_1_sum=sums[1],\n",
    "        group_2_sum=sums[2],\n",
    "        group_3_sum=sums[3],\n",
    "        n_mz_used=len(cast_cols),\n",
    "        n_mz_found=len(existing),\n",
    "        missing_cast_columns=\", \".join(missing) if missing else \"\"\n",
    "    )\n",
    "    results.append(res)\n",
    "\n",
    "# Attach results\n",
    "df_quant = pd.DataFrame(results, index=df_asn.index)\n",
    "df_asn_out = pd.concat([df_asn, df_quant], axis=1)\n",
    "\n",
    "# --------------------\n",
    "# Save updated CSV\n",
    "# --------------------\n",
    "df_asn_out.to_csv(OUT_PATH, index=False)\n",
    "print(f\"Saved: {OUT_PATH}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8dc503a",
   "metadata": {},
   "source": [
    "identification of PFR by matching with tdportal report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf3fd5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Done. Saved ‚Üí F:/binary/final\\ids2.csv\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Two-pass matching with precursor-based support:\n",
    "\n",
    "PRIMARY (Accession):\n",
    "  - For each precursor m/z in matched_mz_list:\n",
    "      collect Accessions where |precursor_mz - m/z| <= MZ_TOL AND |MASS - neutral_mass| <= MASS_TOL\n",
    "  - Choose the mode Accession by MAX number of precursors that contain it (support).\n",
    "    Ties -> break by total match frequency, then lexicographic.\n",
    "  - primary_count = number of precursors supporting that mode Accession.\n",
    "  - Accept primary if primary_count >= 2.\n",
    "\n",
    "SECONDARY (Prediction, only if primary fails):\n",
    "  - For each precursor m/z:\n",
    "      collect predictions where |precursor_mz - m/z| <= PRED_MZ_TOL (ignore MASS)\n",
    "      (normalize to \"ASYN\" or \"NASYN\"; output uses \"ASYN\" / \"nASYN\")\n",
    "  - Choose the mode prediction by MAX precursor support (same tie-break rules).\n",
    "  - secondary_count = number of precursors supporting that mode prediction.\n",
    "  - Accept secondary if secondary_count >= 4.\n",
    "\n",
    "Else => final_call = \"unidentified\".\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import ast\n",
    "from collections import Counter, defaultdict\n",
    "from typing import List, Optional, Tuple, Dict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ----------------------------\n",
    "# CONFIG\n",
    "# ----------------------------\n",
    "REPORT_PATH = r\"F:/binary/final/report/report.csv\"\n",
    "MS2_PATH    = r\"F:/binary/final/ms2_per_scan_with_ids_hash.csv\"\n",
    "OUTPUT_PATH = r\"F:/binary/final/ids2.csv\"\n",
    "\n",
    "# Tolerances\n",
    "MZ_TOL      = 2.0     # primary (m/z + mass)\n",
    "MASS_TOL    = 50.0    # primary (Da)\n",
    "PRED_MZ_TOL = 0.5     # secondary (m/z only)\n",
    "\n",
    "# Thresholds applied to PRECURSOR SUPPORT (not raw matches)\n",
    "PRIMARY_MIN_PRECURSORS   = 2\n",
    "SECONDARY_MIN_PRECURSORS = 4\n",
    "\n",
    "# ----------------------------\n",
    "# Helpers\n",
    "# ----------------------------\n",
    "def _num(s: pd.Series) -> pd.Series:\n",
    "    return pd.to_numeric(s, errors=\"coerce\")\n",
    "\n",
    "def _safe_parse_list(val) -> List[float]:\n",
    "    if isinstance(val, str):\n",
    "        try:\n",
    "            parsed = ast.literal_eval(val)\n",
    "            if isinstance(parsed, (list, tuple, np.ndarray)):\n",
    "                return [float(x) for x in parsed]\n",
    "        except Exception:\n",
    "            pass\n",
    "    elif isinstance(val, (list, tuple, np.ndarray)):\n",
    "        try:\n",
    "            return [float(x) for x in val]\n",
    "        except Exception:\n",
    "            pass\n",
    "    return []\n",
    "\n",
    "def _choose_mode_by_precursor_support(\n",
    "    per_precursor_sets: List[set],\n",
    "    total_match_counter: Counter\n",
    ") -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Given:\n",
    "      - per_precursor_sets: list of sets (one per precursor) containing labels present for that precursor\n",
    "      - total_match_counter: counts across ALL matches (for tie-break)\n",
    "    Return the label with:\n",
    "      1) max precursor-support count (how many sets contain it),\n",
    "      2) then max total match frequency,\n",
    "      3) then lexicographic min.\n",
    "    \"\"\"\n",
    "    if not per_precursor_sets:\n",
    "        return None\n",
    "\n",
    "    # precursor support counts\n",
    "    support: Dict[str, int] = defaultdict(int)\n",
    "    for s in per_precursor_sets:\n",
    "        for lab in s:\n",
    "            support[lab] += 1\n",
    "    if not support:\n",
    "        return None\n",
    "\n",
    "    # Prepare sorted candidates by the criteria\n",
    "    # (-support, -total_matches, label)\n",
    "    candidates = sorted(\n",
    "        support.keys(),\n",
    "        key=lambda lab: (-support[lab], -total_match_counter.get(lab, 0), str(lab))\n",
    "    )\n",
    "    return candidates[0] if candidates else None\n",
    "\n",
    "# ----------------------------\n",
    "# Matching\n",
    "# ----------------------------\n",
    "def primary_accession_pass(\n",
    "    df2: pd.DataFrame,\n",
    "    mz_list: List[float],\n",
    "    neutral_mass: float,\n",
    "    mz_tol: float,\n",
    "    mass_tol: float\n",
    ") -> Tuple[Optional[str], int]:\n",
    "    \"\"\"\n",
    "    Build, for each precursor m/z, the SET of matched Accessions (primary criteria).\n",
    "    Choose the Accession with the highest number of precursor sets containing it.\n",
    "    Return (mode_accession, precursor_support_count_for_mode).\n",
    "    \"\"\"\n",
    "    if not mz_list or pd.isna(neutral_mass):\n",
    "        return None, 0\n",
    "\n",
    "    per_precursor_sets: List[set] = []\n",
    "    total_matches = Counter()\n",
    "\n",
    "    for mz in mz_list:\n",
    "        d_mz   = (df2[\"precursor_mz\"] - mz).abs()\n",
    "        d_mass = (df2[\"MASS\"] - neutral_mass).abs()\n",
    "        mask = (d_mz <= mz_tol) & (d_mass <= mass_tol)\n",
    "        if not mask.any():\n",
    "            continue\n",
    "\n",
    "        accs = df2.loc[mask, \"Accession\"].astype(str).tolist()\n",
    "        if accs:\n",
    "            per_precursor_sets.append(set(accs))\n",
    "            total_matches.update(accs)\n",
    "\n",
    "    if not per_precursor_sets:\n",
    "        return None, 0\n",
    "\n",
    "    mode_acc = _choose_mode_by_precursor_support(per_precursor_sets, total_matches)\n",
    "    if mode_acc is None:\n",
    "        return None, 0\n",
    "\n",
    "    # precursor-based count for the chosen mode\n",
    "    support_count = sum(1 for s in per_precursor_sets if mode_acc in s)\n",
    "    return mode_acc, support_count\n",
    "\n",
    "def secondary_prediction_pass(\n",
    "    df2: pd.DataFrame,\n",
    "    mz_list: List[float],\n",
    "    mz_tol: float\n",
    ") -> Tuple[Optional[str], int]:\n",
    "    \"\"\"\n",
    "    Per precursor m/z, collect the SET of predictions matched within mz_tol (ignore MASS).\n",
    "    Normalize predictions to \"ASYN\"/\"NASYN\". Output uses \"ASYN\" / \"nASYN\".\n",
    "    Choose the prediction with the highest precursor support.\n",
    "    Return (mode_prediction_token, precursor_support_count_for_mode).\n",
    "    \"\"\"\n",
    "    if not mz_list:\n",
    "        return None, 0\n",
    "\n",
    "    per_precursor_sets: List[set] = []\n",
    "    total_matches = Counter()\n",
    "\n",
    "    for mz in mz_list:\n",
    "        d_mz = (df2[\"precursor_mz\"] - mz).abs()\n",
    "        mask = (d_mz <= mz_tol)\n",
    "        if not mask.any():\n",
    "            continue\n",
    "\n",
    "        preds = (\n",
    "            df2.loc[mask, \"prediction\"]\n",
    "            .astype(str).str.upper().str.strip()\n",
    "            .tolist()\n",
    "        )\n",
    "        # keep only ASYN/NASYN; form a set for this precursor\n",
    "        lab_set = set(p for p in preds if p in (\"ASYN\", \"NASYN\"))\n",
    "        if lab_set:\n",
    "            per_precursor_sets.append(lab_set)\n",
    "            total_matches.update(lab_set)  # update by labels present for this precursor\n",
    "\n",
    "    if not per_precursor_sets:\n",
    "        return None, 0\n",
    "\n",
    "    mode_pred = _choose_mode_by_precursor_support(per_precursor_sets, total_matches)\n",
    "    if mode_pred is None:\n",
    "        return None, 0\n",
    "\n",
    "    support_count = sum(1 for s in per_precursor_sets if mode_pred in s)\n",
    "    # canonicalize to requested output token\n",
    "    mode_token = \"ASYN\" if mode_pred == \"ASYN\" else \"nASYN\"\n",
    "    return mode_token, support_count\n",
    "\n",
    "# ----------------------------\n",
    "# Main\n",
    "# ----------------------------\n",
    "def main():\n",
    "    # Load\n",
    "    if not os.path.exists(REPORT_PATH):\n",
    "        raise FileNotFoundError(f\"Not found: {REPORT_PATH}\")\n",
    "    if not os.path.exists(MS2_PATH):\n",
    "        raise FileNotFoundError(f\"Not found: {MS2_PATH}\")\n",
    "\n",
    "    df1 = pd.read_csv(REPORT_PATH)\n",
    "    df2 = pd.read_csv(MS2_PATH)\n",
    "\n",
    "    # Required columns\n",
    "    for col in [\"precursor_mz\", \"MASS\", \"Accession\", \"prediction\"]:\n",
    "        if col not in df2.columns:\n",
    "            raise KeyError(f\"MS2 file must contain '{col}' column.\")\n",
    "\n",
    "    # Types\n",
    "    df2[\"precursor_mz\"] = _num(df2[\"precursor_mz\"])\n",
    "    df2[\"MASS\"]         = _num(df2[\"MASS\"])\n",
    "    df2[\"Accession\"]    = df2[\"Accession\"].astype(str)\n",
    "    df2[\"prediction\"]   = df2[\"prediction\"].astype(str)\n",
    "\n",
    "    # Outputs\n",
    "    primary_modes, primary_counts = [], []\n",
    "    secondary_modes, secondary_counts = [], []\n",
    "    finals = []\n",
    "\n",
    "    # Process each row\n",
    "    for _, row in df1.iterrows():\n",
    "        mz_list = _safe_parse_list(row.get(\"matched_mz_list\", []))\n",
    "        neutral_mass = row.get(\"neutral_mass\", np.nan)\n",
    "\n",
    "        # PRIMARY (Accession, m/z+mass)\n",
    "        p_mode, p_support = primary_accession_pass(df2, mz_list, neutral_mass, MZ_TOL, MASS_TOL)\n",
    "        if p_mode is not None and p_support >= PRIMARY_MIN_PRECURSORS:\n",
    "            primary_modes.append(p_mode)\n",
    "            primary_counts.append(p_support)\n",
    "            secondary_modes.append(None)\n",
    "            secondary_counts.append(0)\n",
    "            finals.append(p_mode)\n",
    "            continue\n",
    "\n",
    "        # SECONDARY (Prediction, m/z-only)\n",
    "        s_mode, s_support = secondary_prediction_pass(df2, mz_list, PRED_MZ_TOL)\n",
    "        primary_modes.append(None)\n",
    "        primary_counts.append(0)\n",
    "\n",
    "        if s_mode is not None and s_support >= SECONDARY_MIN_PRECURSORS:\n",
    "            secondary_modes.append(s_mode)\n",
    "            secondary_counts.append(s_support)\n",
    "            finals.append(s_mode)\n",
    "        else:\n",
    "            secondary_modes.append(None)\n",
    "            secondary_counts.append(0)\n",
    "            finals.append(\"unidentified\")\n",
    "\n",
    "    # Attach outputs\n",
    "    df1[\"primary_mode_accession\"]    = primary_modes\n",
    "    df1[\"primary_count\"]             = primary_counts          # number of PRECURSORS supporting mode Accession\n",
    "    df1[\"secondary_mode_prediction\"] = secondary_modes\n",
    "    df1[\"secondary_count\"]           = secondary_counts        # number of PRECURSORS supporting mode prediction\n",
    "    df1[\"final_call\"]                = finals\n",
    "\n",
    "    # Save\n",
    "    os.makedirs(os.path.dirname(OUTPUT_PATH) or \".\", exist_ok=True)\n",
    "    df1.to_csv(OUTPUT_PATH, index=False)\n",
    "    print(f\"‚úÖ Done. Saved ‚Üí {OUTPUT_PATH}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vae",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
