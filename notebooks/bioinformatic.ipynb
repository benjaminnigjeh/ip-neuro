{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8104e0c9",
   "metadata": {},
   "source": [
    "Generating deconvoluted spectra from the informative features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "975db4cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: frac_pellet_grads_AB__pos_1__neg_0_negabs_runA.csv ‚Üí F:\\binary\\raw\\result\\frac_pellet_grads_AB__pos_1__neg_0_negabs_runA\n",
      "Processing: frac_pellet_grads_AB__pos_1__neg_0_negabs_runB.csv ‚Üí F:\\binary\\raw\\result\\frac_pellet_grads_AB__pos_1__neg_0_negabs_runB\n",
      "Processing: frac_pellet_grads_AB__pos_1__neg_0_pos_runA.csv ‚Üí F:\\binary\\raw\\result\\frac_pellet_grads_AB__pos_1__neg_0_pos_runA\n",
      "Processing: frac_pellet_grads_AB__pos_1__neg_0_pos_runB.csv ‚Üí F:\\binary\\raw\\result\\frac_pellet_grads_AB__pos_1__neg_0_pos_runB\n",
      "Processing: frac_soluble_grads_AB__pos_1__neg_0_negabs_runA.csv ‚Üí F:\\binary\\raw\\result\\frac_soluble_grads_AB__pos_1__neg_0_negabs_runA\n",
      "Processing: frac_soluble_grads_AB__pos_1__neg_0_negabs_runB.csv ‚Üí F:\\binary\\raw\\result\\frac_soluble_grads_AB__pos_1__neg_0_negabs_runB\n",
      "Processing: frac_soluble_grads_AB__pos_1__neg_0_pos_runA.csv ‚Üí F:\\binary\\raw\\result\\frac_soluble_grads_AB__pos_1__neg_0_pos_runA\n",
      "Processing: frac_soluble_grads_AB__pos_1__neg_0_pos_runB.csv ‚Üí F:\\binary\\raw\\result\\frac_soluble_grads_AB__pos_1__neg_0_pos_runB\n",
      "‚úÖ All files processed. Results saved in: F:\\binary\\raw\\result\n",
      "Staging: F:\\binary\\raw\\result\\frac_pellet_grads_AB__pos_1__neg_0_negabs_runA_mass.txt ‚Üí C:\\Users\\benja\\AppData\\Local\\Temp\\mass_collect_um_pnrxi\\frac_pellet_grads_AB__pos_1__neg_0_negabs_runA_mass.txt\n",
      "Staging: F:\\binary\\raw\\result\\frac_pellet_grads_AB__pos_1__neg_0_negabs_runB_mass.txt ‚Üí C:\\Users\\benja\\AppData\\Local\\Temp\\mass_collect_um_pnrxi\\frac_pellet_grads_AB__pos_1__neg_0_negabs_runB_mass.txt\n",
      "Staging: F:\\binary\\raw\\result\\frac_pellet_grads_AB__pos_1__neg_0_pos_runA_mass.txt ‚Üí C:\\Users\\benja\\AppData\\Local\\Temp\\mass_collect_um_pnrxi\\frac_pellet_grads_AB__pos_1__neg_0_pos_runA_mass.txt\n",
      "Staging: F:\\binary\\raw\\result\\frac_pellet_grads_AB__pos_1__neg_0_pos_runB_mass.txt ‚Üí C:\\Users\\benja\\AppData\\Local\\Temp\\mass_collect_um_pnrxi\\frac_pellet_grads_AB__pos_1__neg_0_pos_runB_mass.txt\n",
      "Staging: F:\\binary\\raw\\result\\frac_soluble_grads_AB__pos_1__neg_0_negabs_runA_mass.txt ‚Üí C:\\Users\\benja\\AppData\\Local\\Temp\\mass_collect_um_pnrxi\\frac_soluble_grads_AB__pos_1__neg_0_negabs_runA_mass.txt\n",
      "Staging: F:\\binary\\raw\\result\\frac_soluble_grads_AB__pos_1__neg_0_negabs_runB_mass.txt ‚Üí C:\\Users\\benja\\AppData\\Local\\Temp\\mass_collect_um_pnrxi\\frac_soluble_grads_AB__pos_1__neg_0_negabs_runB_mass.txt\n",
      "Staging: F:\\binary\\raw\\result\\frac_soluble_grads_AB__pos_1__neg_0_pos_runA_mass.txt ‚Üí C:\\Users\\benja\\AppData\\Local\\Temp\\mass_collect_um_pnrxi\\frac_soluble_grads_AB__pos_1__neg_0_pos_runA_mass.txt\n",
      "Staging: F:\\binary\\raw\\result\\frac_soluble_grads_AB__pos_1__neg_0_pos_runB_mass.txt ‚Üí C:\\Users\\benja\\AppData\\Local\\Temp\\mass_collect_um_pnrxi\\frac_soluble_grads_AB__pos_1__neg_0_pos_runB_mass.txt\n",
      "Finalizing: C:\\Users\\benja\\AppData\\Local\\Temp\\mass_collect_um_pnrxi\\frac_pellet_grads_AB__pos_1__neg_0_negabs_runA_mass.txt ‚Üí F:\\binary\\raw\\result\\frac_pellet_grads_AB__pos_1__neg_0_negabs_runA_mass.txt\n",
      "Finalizing: C:\\Users\\benja\\AppData\\Local\\Temp\\mass_collect_um_pnrxi\\frac_pellet_grads_AB__pos_1__neg_0_negabs_runB_mass.txt ‚Üí F:\\binary\\raw\\result\\frac_pellet_grads_AB__pos_1__neg_0_negabs_runB_mass.txt\n",
      "Finalizing: C:\\Users\\benja\\AppData\\Local\\Temp\\mass_collect_um_pnrxi\\frac_pellet_grads_AB__pos_1__neg_0_pos_runA_mass.txt ‚Üí F:\\binary\\raw\\result\\frac_pellet_grads_AB__pos_1__neg_0_pos_runA_mass.txt\n",
      "Finalizing: C:\\Users\\benja\\AppData\\Local\\Temp\\mass_collect_um_pnrxi\\frac_pellet_grads_AB__pos_1__neg_0_pos_runB_mass.txt ‚Üí F:\\binary\\raw\\result\\frac_pellet_grads_AB__pos_1__neg_0_pos_runB_mass.txt\n",
      "Finalizing: C:\\Users\\benja\\AppData\\Local\\Temp\\mass_collect_um_pnrxi\\frac_soluble_grads_AB__pos_1__neg_0_negabs_runA_mass.txt ‚Üí F:\\binary\\raw\\result\\frac_soluble_grads_AB__pos_1__neg_0_negabs_runA_mass.txt\n",
      "Finalizing: C:\\Users\\benja\\AppData\\Local\\Temp\\mass_collect_um_pnrxi\\frac_soluble_grads_AB__pos_1__neg_0_negabs_runB_mass.txt ‚Üí F:\\binary\\raw\\result\\frac_soluble_grads_AB__pos_1__neg_0_negabs_runB_mass.txt\n",
      "Finalizing: C:\\Users\\benja\\AppData\\Local\\Temp\\mass_collect_um_pnrxi\\frac_soluble_grads_AB__pos_1__neg_0_pos_runA_mass.txt ‚Üí F:\\binary\\raw\\result\\frac_soluble_grads_AB__pos_1__neg_0_pos_runA_mass.txt\n",
      "Finalizing: C:\\Users\\benja\\AppData\\Local\\Temp\\mass_collect_um_pnrxi\\frac_soluble_grads_AB__pos_1__neg_0_pos_runB_mass.txt ‚Üí F:\\binary\\raw\\result\\frac_soluble_grads_AB__pos_1__neg_0_pos_runB_mass.txt\n",
      "üìÇ Clean result folder ready with only *_mass.txt files: F:\\binary\\raw\\result\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import os\n",
    "import subprocess\n",
    "import shutil\n",
    "import tempfile\n",
    "\n",
    "def _unique_dst_path(dst_dir, fname):\n",
    "    \"\"\"Return a unique path in dst_dir for fname, adding a numeric suffix if needed.\"\"\"\n",
    "    base, ext = os.path.splitext(fname)\n",
    "    candidate = os.path.join(dst_dir, fname)\n",
    "    i = 1\n",
    "    while os.path.exists(candidate):\n",
    "        candidate = os.path.join(dst_dir, f\"{base}__{i}{ext}\")\n",
    "        i += 1\n",
    "    return candidate\n",
    "\n",
    "def _prefixed_name(src_path, result_root):\n",
    "    \"\"\"\n",
    "    Build a safer filename using the immediate parent folder under result/ as a prefix\n",
    "    to reduce collisions: e.g., result/sampleA/sampleA_mass.txt -> sampleA__sampleA_mass.txt\n",
    "    \"\"\"\n",
    "    # src_path like .../result/<parent>/<file>\n",
    "    parent = os.path.basename(os.path.dirname(src_path))\n",
    "    fname = os.path.basename(src_path)\n",
    "    return f\"{parent}__{fname}\" if parent and parent != \"result\" else fname\n",
    "\n",
    "def run_unidec_on_folder(folder_path):\n",
    "    # Ensure result root folder exists\n",
    "    result_root = os.path.join(folder_path, \"result\")\n",
    "    os.makedirs(result_root, exist_ok=True)\n",
    "\n",
    "    # Loop through files in the folder (top-level only)\n",
    "    for file_name in os.listdir(folder_path):\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "\n",
    "        # Skip directories\n",
    "        if not os.path.isfile(file_path):\n",
    "            continue\n",
    "\n",
    "        # Create a unique subfolder named after the file (without extension)\n",
    "        base_name = os.path.splitext(file_name)[0]\n",
    "        file_result_folder = os.path.join(result_root, base_name)\n",
    "        os.makedirs(file_result_folder, exist_ok=True)\n",
    "\n",
    "        # Run UniDec for this file, send outputs to its subfolder\n",
    "        print(f\"Processing: {file_name} ‚Üí {file_result_folder}\")\n",
    "        subprocess.run([\"python\", \"-m\", \"unidec\", \"-f\", file_path, \"-o\", file_result_folder])\n",
    "\n",
    "    print(\"‚úÖ All files processed. Results saved in:\", result_root)\n",
    "\n",
    "    # 1) Collect *_mass.txt paths from result_root (including subfolders)\n",
    "    collected = []\n",
    "    for root, _, files in os.walk(result_root):\n",
    "        for f in files:\n",
    "            if f.endswith(\"_mass.txt\"):\n",
    "                collected.append(os.path.join(root, f))\n",
    "\n",
    "    if not collected:\n",
    "        print(\"‚ö†Ô∏è No *_mass.txt files found under:\", result_root)\n",
    "        return\n",
    "\n",
    "    # 2) Copy them to a temp folder FIRST (so deleting result/ content won't break src paths)\n",
    "    temp_dir = tempfile.mkdtemp(prefix=\"mass_collect_\")\n",
    "    copied = []\n",
    "    for src in collected:\n",
    "        try:\n",
    "            # Prefix with subfolder name to avoid collisions\n",
    "            safe_name = _prefixed_name(src, result_root)\n",
    "            dst = os.path.join(temp_dir, safe_name)\n",
    "            dst = _unique_dst_path(temp_dir, os.path.basename(dst))  # ensure uniqueness\n",
    "            print(f\"Staging: {src} ‚Üí {dst}\")\n",
    "            shutil.copy2(src, dst)\n",
    "            copied.append(dst)\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Skip (copy error): {src} ‚Äî {e}\")\n",
    "\n",
    "    # 3) Clean the result_root completely\n",
    "    for item in os.listdir(result_root):\n",
    "        item_path = os.path.join(result_root, item)\n",
    "        try:\n",
    "            if os.path.isfile(item_path) or os.path.islink(item_path):\n",
    "                os.remove(item_path)\n",
    "            else:\n",
    "                shutil.rmtree(item_path)\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Could not remove {item_path}: {e}\")\n",
    "\n",
    "    # 4) Move staged files back into a clean result_root\n",
    "    for staged in copied:\n",
    "        try:\n",
    "            final_dst = os.path.join(result_root, os.path.basename(staged))\n",
    "            final_dst = _unique_dst_path(result_root, os.path.basename(final_dst))\n",
    "            print(f\"Finalizing: {staged} ‚Üí {final_dst}\")\n",
    "            shutil.move(staged, final_dst)\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Move error for {staged}: {e}\")\n",
    "\n",
    "    # 5) Remove temp dir (ignore errors)\n",
    "    try:\n",
    "        shutil.rmtree(temp_dir)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    print(\"üìÇ Clean result folder ready with only *_mass.txt files:\", result_root)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Example usage\n",
    "    folder_path = r\"F:\\binary\\raw\"  # <-- replace with your folder\n",
    "    run_unidec_on_folder(folder_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "88417a17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8 decon file(s) in F:\\binary\\decon\n",
      "Found 8 raw CSV file(s) in F:\\binary\\raw\n",
      "[frac_pellet_grads_AB__pos_1__neg_0_negabs_runA] Neutral-mass detection: 55 peaks ‚Üí F:\\binary\\firstpass\\frac_pellet_grads_AB__pos_1__neg_0_negabs_runA\\frac_pellet_grads_AB__pos_1__neg_0_negabs_runA_detected_signals.csv\n",
      "[frac_pellet_grads_AB__pos_1__neg_0_negabs_runA] Detection plot saved ‚Üí F:\\binary\\firstpass\\frac_pellet_grads_AB__pos_1__neg_0_negabs_runA\\frac_pellet_grads_AB__pos_1__neg_0_negabs_runA_detected_signals.png\n",
      "[frac_pellet_grads_AB__pos_1__neg_0_negabs_runA] Parsed filename metadata: {'bin': None, 'experiments': 1, 'controls': 1, 'experiments_ids': '1', 'controls_ids': '0', 'experiments_n': 1, 'controls_n': 1, 'regulation': 'downregulated', 'replicate': 'A', 'source_file': 'frac_pellet_grads_AB__pos_1__neg_0_negabs_runA_mass.txt'}\n",
      "Saved plot: F:\\binary\\firstpass\\frac_pellet_grads_AB__pos_1__neg_0_negabs_runA\\frac_pellet_grads_AB__pos_1__neg_0_negabs_runA_neutral_mass_spectrum.png\n",
      "Saved plot: F:\\binary\\firstpass\\frac_pellet_grads_AB__pos_1__neg_0_negabs_runA\\frac_pellet_grads_AB__pos_1__neg_0_negabs_runA_mirror_assigned_vs_total.png\n",
      "Saved plot: F:\\binary\\firstpass\\frac_pellet_grads_AB__pos_1__neg_0_negabs_runA\\frac_pellet_grads_AB__pos_1__neg_0_negabs_runA_mirror_unassigned_vs_total.png\n",
      "Saved plot: F:\\binary\\firstpass\\frac_pellet_grads_AB__pos_1__neg_0_negabs_runA\\frac_pellet_grads_AB__pos_1__neg_0_negabs_runA_mirror_assigned_by_protein_vs_total.png\n",
      "\n",
      "=== [frac_pellet_grads_AB__pos_1__neg_0_negabs_runA] Summary ===\n",
      "Raw MS1 peaks (rows): 4,301\n",
      "Detected neutral-mass peaks: 55\n",
      "Assigned raw peaks: 1,146\n",
      "Non-assigned raw peaks: 3,155\n",
      "Saved: F:\\binary\\firstpass\\frac_pellet_grads_AB__pos_1__neg_0_negabs_runA\\frac_pellet_grads_AB__pos_1__neg_0_negabs_runA_assigned_ms1_with_peaks.csv\n",
      "Saved: F:\\binary\\firstpass\\frac_pellet_grads_AB__pos_1__neg_0_negabs_runA\\frac_pellet_grads_AB__pos_1__neg_0_negabs_runA_assignments_summary.csv\n",
      "------------------------------------------------------------\n",
      "[frac_pellet_grads_AB__pos_1__neg_0_negabs_runB] Neutral-mass detection: 111 peaks ‚Üí F:\\binary\\firstpass\\frac_pellet_grads_AB__pos_1__neg_0_negabs_runB\\frac_pellet_grads_AB__pos_1__neg_0_negabs_runB_detected_signals.csv\n",
      "[frac_pellet_grads_AB__pos_1__neg_0_negabs_runB] Detection plot saved ‚Üí F:\\binary\\firstpass\\frac_pellet_grads_AB__pos_1__neg_0_negabs_runB\\frac_pellet_grads_AB__pos_1__neg_0_negabs_runB_detected_signals.png\n",
      "[frac_pellet_grads_AB__pos_1__neg_0_negabs_runB] Parsed filename metadata: {'bin': None, 'experiments': 1, 'controls': 1, 'experiments_ids': '1', 'controls_ids': '0', 'experiments_n': 1, 'controls_n': 1, 'regulation': 'downregulated', 'replicate': 'B', 'source_file': 'frac_pellet_grads_AB__pos_1__neg_0_negabs_runB_mass.txt'}\n",
      "Saved plot: F:\\binary\\firstpass\\frac_pellet_grads_AB__pos_1__neg_0_negabs_runB\\frac_pellet_grads_AB__pos_1__neg_0_negabs_runB_neutral_mass_spectrum.png\n",
      "Saved plot: F:\\binary\\firstpass\\frac_pellet_grads_AB__pos_1__neg_0_negabs_runB\\frac_pellet_grads_AB__pos_1__neg_0_negabs_runB_mirror_assigned_vs_total.png\n",
      "Saved plot: F:\\binary\\firstpass\\frac_pellet_grads_AB__pos_1__neg_0_negabs_runB\\frac_pellet_grads_AB__pos_1__neg_0_negabs_runB_mirror_unassigned_vs_total.png\n",
      "Saved plot: F:\\binary\\firstpass\\frac_pellet_grads_AB__pos_1__neg_0_negabs_runB\\frac_pellet_grads_AB__pos_1__neg_0_negabs_runB_mirror_assigned_by_protein_vs_total.png\n",
      "\n",
      "=== [frac_pellet_grads_AB__pos_1__neg_0_negabs_runB] Summary ===\n",
      "Raw MS1 peaks (rows): 4,058\n",
      "Detected neutral-mass peaks: 111\n",
      "Assigned raw peaks: 1,905\n",
      "Non-assigned raw peaks: 2,153\n",
      "Saved: F:\\binary\\firstpass\\frac_pellet_grads_AB__pos_1__neg_0_negabs_runB\\frac_pellet_grads_AB__pos_1__neg_0_negabs_runB_assigned_ms1_with_peaks.csv\n",
      "Saved: F:\\binary\\firstpass\\frac_pellet_grads_AB__pos_1__neg_0_negabs_runB\\frac_pellet_grads_AB__pos_1__neg_0_negabs_runB_assignments_summary.csv\n",
      "------------------------------------------------------------\n",
      "[frac_pellet_grads_AB__pos_1__neg_0_pos_runA] Neutral-mass detection: 144 peaks ‚Üí F:\\binary\\firstpass\\frac_pellet_grads_AB__pos_1__neg_0_pos_runA\\frac_pellet_grads_AB__pos_1__neg_0_pos_runA_detected_signals.csv\n",
      "[frac_pellet_grads_AB__pos_1__neg_0_pos_runA] Detection plot saved ‚Üí F:\\binary\\firstpass\\frac_pellet_grads_AB__pos_1__neg_0_pos_runA\\frac_pellet_grads_AB__pos_1__neg_0_pos_runA_detected_signals.png\n",
      "[frac_pellet_grads_AB__pos_1__neg_0_pos_runA] Parsed filename metadata: {'bin': None, 'experiments': 1, 'controls': 1, 'experiments_ids': '1', 'controls_ids': '0', 'experiments_n': 1, 'controls_n': 1, 'regulation': 'upregulated', 'replicate': 'A', 'source_file': 'frac_pellet_grads_AB__pos_1__neg_0_pos_runA_mass.txt'}\n",
      "Saved plot: F:\\binary\\firstpass\\frac_pellet_grads_AB__pos_1__neg_0_pos_runA\\frac_pellet_grads_AB__pos_1__neg_0_pos_runA_neutral_mass_spectrum.png\n",
      "Saved plot: F:\\binary\\firstpass\\frac_pellet_grads_AB__pos_1__neg_0_pos_runA\\frac_pellet_grads_AB__pos_1__neg_0_pos_runA_mirror_assigned_vs_total.png\n",
      "Saved plot: F:\\binary\\firstpass\\frac_pellet_grads_AB__pos_1__neg_0_pos_runA\\frac_pellet_grads_AB__pos_1__neg_0_pos_runA_mirror_unassigned_vs_total.png\n",
      "Saved plot: F:\\binary\\firstpass\\frac_pellet_grads_AB__pos_1__neg_0_pos_runA\\frac_pellet_grads_AB__pos_1__neg_0_pos_runA_mirror_assigned_by_protein_vs_total.png\n",
      "\n",
      "=== [frac_pellet_grads_AB__pos_1__neg_0_pos_runA] Summary ===\n",
      "Raw MS1 peaks (rows): 5,699\n",
      "Detected neutral-mass peaks: 144\n",
      "Assigned raw peaks: 2,349\n",
      "Non-assigned raw peaks: 3,350\n",
      "Saved: F:\\binary\\firstpass\\frac_pellet_grads_AB__pos_1__neg_0_pos_runA\\frac_pellet_grads_AB__pos_1__neg_0_pos_runA_assigned_ms1_with_peaks.csv\n",
      "Saved: F:\\binary\\firstpass\\frac_pellet_grads_AB__pos_1__neg_0_pos_runA\\frac_pellet_grads_AB__pos_1__neg_0_pos_runA_assignments_summary.csv\n",
      "------------------------------------------------------------\n",
      "[frac_pellet_grads_AB__pos_1__neg_0_pos_runB] Neutral-mass detection: 53 peaks ‚Üí F:\\binary\\firstpass\\frac_pellet_grads_AB__pos_1__neg_0_pos_runB\\frac_pellet_grads_AB__pos_1__neg_0_pos_runB_detected_signals.csv\n",
      "[frac_pellet_grads_AB__pos_1__neg_0_pos_runB] Detection plot saved ‚Üí F:\\binary\\firstpass\\frac_pellet_grads_AB__pos_1__neg_0_pos_runB\\frac_pellet_grads_AB__pos_1__neg_0_pos_runB_detected_signals.png\n",
      "[frac_pellet_grads_AB__pos_1__neg_0_pos_runB] Parsed filename metadata: {'bin': None, 'experiments': 1, 'controls': 1, 'experiments_ids': '1', 'controls_ids': '0', 'experiments_n': 1, 'controls_n': 1, 'regulation': 'upregulated', 'replicate': 'B', 'source_file': 'frac_pellet_grads_AB__pos_1__neg_0_pos_runB_mass.txt'}\n",
      "Saved plot: F:\\binary\\firstpass\\frac_pellet_grads_AB__pos_1__neg_0_pos_runB\\frac_pellet_grads_AB__pos_1__neg_0_pos_runB_neutral_mass_spectrum.png\n",
      "Saved plot: F:\\binary\\firstpass\\frac_pellet_grads_AB__pos_1__neg_0_pos_runB\\frac_pellet_grads_AB__pos_1__neg_0_pos_runB_mirror_assigned_vs_total.png\n",
      "Saved plot: F:\\binary\\firstpass\\frac_pellet_grads_AB__pos_1__neg_0_pos_runB\\frac_pellet_grads_AB__pos_1__neg_0_pos_runB_mirror_unassigned_vs_total.png\n",
      "Saved plot: F:\\binary\\firstpass\\frac_pellet_grads_AB__pos_1__neg_0_pos_runB\\frac_pellet_grads_AB__pos_1__neg_0_pos_runB_mirror_assigned_by_protein_vs_total.png\n",
      "\n",
      "=== [frac_pellet_grads_AB__pos_1__neg_0_pos_runB] Summary ===\n",
      "Raw MS1 peaks (rows): 5,942\n",
      "Detected neutral-mass peaks: 53\n",
      "Assigned raw peaks: 916\n",
      "Non-assigned raw peaks: 5,026\n",
      "Saved: F:\\binary\\firstpass\\frac_pellet_grads_AB__pos_1__neg_0_pos_runB\\frac_pellet_grads_AB__pos_1__neg_0_pos_runB_assigned_ms1_with_peaks.csv\n",
      "Saved: F:\\binary\\firstpass\\frac_pellet_grads_AB__pos_1__neg_0_pos_runB\\frac_pellet_grads_AB__pos_1__neg_0_pos_runB_assignments_summary.csv\n",
      "------------------------------------------------------------\n",
      "[frac_soluble_grads_AB__pos_1__neg_0_negabs_runA] Neutral-mass detection: 44 peaks ‚Üí F:\\binary\\firstpass\\frac_soluble_grads_AB__pos_1__neg_0_negabs_runA\\frac_soluble_grads_AB__pos_1__neg_0_negabs_runA_detected_signals.csv\n",
      "[frac_soluble_grads_AB__pos_1__neg_0_negabs_runA] Detection plot saved ‚Üí F:\\binary\\firstpass\\frac_soluble_grads_AB__pos_1__neg_0_negabs_runA\\frac_soluble_grads_AB__pos_1__neg_0_negabs_runA_detected_signals.png\n",
      "[frac_soluble_grads_AB__pos_1__neg_0_negabs_runA] Parsed filename metadata: {'bin': None, 'experiments': 1, 'controls': 1, 'experiments_ids': '1', 'controls_ids': '0', 'experiments_n': 1, 'controls_n': 1, 'regulation': 'downregulated', 'replicate': 'A', 'source_file': 'frac_soluble_grads_AB__pos_1__neg_0_negabs_runA_mass.txt'}\n",
      "Saved plot: F:\\binary\\firstpass\\frac_soluble_grads_AB__pos_1__neg_0_negabs_runA\\frac_soluble_grads_AB__pos_1__neg_0_negabs_runA_neutral_mass_spectrum.png\n",
      "Saved plot: F:\\binary\\firstpass\\frac_soluble_grads_AB__pos_1__neg_0_negabs_runA\\frac_soluble_grads_AB__pos_1__neg_0_negabs_runA_mirror_assigned_vs_total.png\n",
      "Saved plot: F:\\binary\\firstpass\\frac_soluble_grads_AB__pos_1__neg_0_negabs_runA\\frac_soluble_grads_AB__pos_1__neg_0_negabs_runA_mirror_unassigned_vs_total.png\n",
      "Saved plot: F:\\binary\\firstpass\\frac_soluble_grads_AB__pos_1__neg_0_negabs_runA\\frac_soluble_grads_AB__pos_1__neg_0_negabs_runA_mirror_assigned_by_protein_vs_total.png\n",
      "\n",
      "=== [frac_soluble_grads_AB__pos_1__neg_0_negabs_runA] Summary ===\n",
      "Raw MS1 peaks (rows): 4,018\n",
      "Detected neutral-mass peaks: 44\n",
      "Assigned raw peaks: 893\n",
      "Non-assigned raw peaks: 3,125\n",
      "Saved: F:\\binary\\firstpass\\frac_soluble_grads_AB__pos_1__neg_0_negabs_runA\\frac_soluble_grads_AB__pos_1__neg_0_negabs_runA_assigned_ms1_with_peaks.csv\n",
      "Saved: F:\\binary\\firstpass\\frac_soluble_grads_AB__pos_1__neg_0_negabs_runA\\frac_soluble_grads_AB__pos_1__neg_0_negabs_runA_assignments_summary.csv\n",
      "------------------------------------------------------------\n",
      "[frac_soluble_grads_AB__pos_1__neg_0_negabs_runB] Neutral-mass detection: 100 peaks ‚Üí F:\\binary\\firstpass\\frac_soluble_grads_AB__pos_1__neg_0_negabs_runB\\frac_soluble_grads_AB__pos_1__neg_0_negabs_runB_detected_signals.csv\n",
      "[frac_soluble_grads_AB__pos_1__neg_0_negabs_runB] Detection plot saved ‚Üí F:\\binary\\firstpass\\frac_soluble_grads_AB__pos_1__neg_0_negabs_runB\\frac_soluble_grads_AB__pos_1__neg_0_negabs_runB_detected_signals.png\n",
      "[frac_soluble_grads_AB__pos_1__neg_0_negabs_runB] Parsed filename metadata: {'bin': None, 'experiments': 1, 'controls': 1, 'experiments_ids': '1', 'controls_ids': '0', 'experiments_n': 1, 'controls_n': 1, 'regulation': 'downregulated', 'replicate': 'B', 'source_file': 'frac_soluble_grads_AB__pos_1__neg_0_negabs_runB_mass.txt'}\n",
      "Saved plot: F:\\binary\\firstpass\\frac_soluble_grads_AB__pos_1__neg_0_negabs_runB\\frac_soluble_grads_AB__pos_1__neg_0_negabs_runB_neutral_mass_spectrum.png\n",
      "Saved plot: F:\\binary\\firstpass\\frac_soluble_grads_AB__pos_1__neg_0_negabs_runB\\frac_soluble_grads_AB__pos_1__neg_0_negabs_runB_mirror_assigned_vs_total.png\n",
      "Saved plot: F:\\binary\\firstpass\\frac_soluble_grads_AB__pos_1__neg_0_negabs_runB\\frac_soluble_grads_AB__pos_1__neg_0_negabs_runB_mirror_unassigned_vs_total.png\n",
      "Saved plot: F:\\binary\\firstpass\\frac_soluble_grads_AB__pos_1__neg_0_negabs_runB\\frac_soluble_grads_AB__pos_1__neg_0_negabs_runB_mirror_assigned_by_protein_vs_total.png\n",
      "\n",
      "=== [frac_soluble_grads_AB__pos_1__neg_0_negabs_runB] Summary ===\n",
      "Raw MS1 peaks (rows): 3,081\n",
      "Detected neutral-mass peaks: 100\n",
      "Assigned raw peaks: 1,663\n",
      "Non-assigned raw peaks: 1,418\n",
      "Saved: F:\\binary\\firstpass\\frac_soluble_grads_AB__pos_1__neg_0_negabs_runB\\frac_soluble_grads_AB__pos_1__neg_0_negabs_runB_assigned_ms1_with_peaks.csv\n",
      "Saved: F:\\binary\\firstpass\\frac_soluble_grads_AB__pos_1__neg_0_negabs_runB\\frac_soluble_grads_AB__pos_1__neg_0_negabs_runB_assignments_summary.csv\n",
      "------------------------------------------------------------\n",
      "[frac_soluble_grads_AB__pos_1__neg_0_pos_runA] Neutral-mass detection: 39 peaks ‚Üí F:\\binary\\firstpass\\frac_soluble_grads_AB__pos_1__neg_0_pos_runA\\frac_soluble_grads_AB__pos_1__neg_0_pos_runA_detected_signals.csv\n",
      "[frac_soluble_grads_AB__pos_1__neg_0_pos_runA] Detection plot saved ‚Üí F:\\binary\\firstpass\\frac_soluble_grads_AB__pos_1__neg_0_pos_runA\\frac_soluble_grads_AB__pos_1__neg_0_pos_runA_detected_signals.png\n",
      "[frac_soluble_grads_AB__pos_1__neg_0_pos_runA] Parsed filename metadata: {'bin': None, 'experiments': 1, 'controls': 1, 'experiments_ids': '1', 'controls_ids': '0', 'experiments_n': 1, 'controls_n': 1, 'regulation': 'upregulated', 'replicate': 'A', 'source_file': 'frac_soluble_grads_AB__pos_1__neg_0_pos_runA_mass.txt'}\n",
      "Saved plot: F:\\binary\\firstpass\\frac_soluble_grads_AB__pos_1__neg_0_pos_runA\\frac_soluble_grads_AB__pos_1__neg_0_pos_runA_neutral_mass_spectrum.png\n",
      "Saved plot: F:\\binary\\firstpass\\frac_soluble_grads_AB__pos_1__neg_0_pos_runA\\frac_soluble_grads_AB__pos_1__neg_0_pos_runA_mirror_assigned_vs_total.png\n",
      "Saved plot: F:\\binary\\firstpass\\frac_soluble_grads_AB__pos_1__neg_0_pos_runA\\frac_soluble_grads_AB__pos_1__neg_0_pos_runA_mirror_unassigned_vs_total.png\n",
      "Saved plot: F:\\binary\\firstpass\\frac_soluble_grads_AB__pos_1__neg_0_pos_runA\\frac_soluble_grads_AB__pos_1__neg_0_pos_runA_mirror_assigned_by_protein_vs_total.png\n",
      "\n",
      "=== [frac_soluble_grads_AB__pos_1__neg_0_pos_runA] Summary ===\n",
      "Raw MS1 peaks (rows): 5,982\n",
      "Detected neutral-mass peaks: 39\n",
      "Assigned raw peaks: 899\n",
      "Non-assigned raw peaks: 5,083\n",
      "Saved: F:\\binary\\firstpass\\frac_soluble_grads_AB__pos_1__neg_0_pos_runA\\frac_soluble_grads_AB__pos_1__neg_0_pos_runA_assigned_ms1_with_peaks.csv\n",
      "Saved: F:\\binary\\firstpass\\frac_soluble_grads_AB__pos_1__neg_0_pos_runA\\frac_soluble_grads_AB__pos_1__neg_0_pos_runA_assignments_summary.csv\n",
      "------------------------------------------------------------\n",
      "[frac_soluble_grads_AB__pos_1__neg_0_pos_runB] Neutral-mass detection: 35 peaks ‚Üí F:\\binary\\firstpass\\frac_soluble_grads_AB__pos_1__neg_0_pos_runB\\frac_soluble_grads_AB__pos_1__neg_0_pos_runB_detected_signals.csv\n",
      "[frac_soluble_grads_AB__pos_1__neg_0_pos_runB] Detection plot saved ‚Üí F:\\binary\\firstpass\\frac_soluble_grads_AB__pos_1__neg_0_pos_runB\\frac_soluble_grads_AB__pos_1__neg_0_pos_runB_detected_signals.png\n",
      "[frac_soluble_grads_AB__pos_1__neg_0_pos_runB] Parsed filename metadata: {'bin': None, 'experiments': 1, 'controls': 1, 'experiments_ids': '1', 'controls_ids': '0', 'experiments_n': 1, 'controls_n': 1, 'regulation': 'upregulated', 'replicate': 'B', 'source_file': 'frac_soluble_grads_AB__pos_1__neg_0_pos_runB_mass.txt'}\n",
      "Saved plot: F:\\binary\\firstpass\\frac_soluble_grads_AB__pos_1__neg_0_pos_runB\\frac_soluble_grads_AB__pos_1__neg_0_pos_runB_neutral_mass_spectrum.png\n",
      "Saved plot: F:\\binary\\firstpass\\frac_soluble_grads_AB__pos_1__neg_0_pos_runB\\frac_soluble_grads_AB__pos_1__neg_0_pos_runB_mirror_assigned_vs_total.png\n",
      "Saved plot: F:\\binary\\firstpass\\frac_soluble_grads_AB__pos_1__neg_0_pos_runB\\frac_soluble_grads_AB__pos_1__neg_0_pos_runB_mirror_unassigned_vs_total.png\n",
      "Saved plot: F:\\binary\\firstpass\\frac_soluble_grads_AB__pos_1__neg_0_pos_runB\\frac_soluble_grads_AB__pos_1__neg_0_pos_runB_mirror_assigned_by_protein_vs_total.png\n",
      "\n",
      "=== [frac_soluble_grads_AB__pos_1__neg_0_pos_runB] Summary ===\n",
      "Raw MS1 peaks (rows): 6,919\n",
      "Detected neutral-mass peaks: 35\n",
      "Assigned raw peaks: 859\n",
      "Non-assigned raw peaks: 6,060\n",
      "Saved: F:\\binary\\firstpass\\frac_soluble_grads_AB__pos_1__neg_0_pos_runB\\frac_soluble_grads_AB__pos_1__neg_0_pos_runB_assigned_ms1_with_peaks.csv\n",
      "Saved: F:\\binary\\firstpass\\frac_soluble_grads_AB__pos_1__neg_0_pos_runB\\frac_soluble_grads_AB__pos_1__neg_0_pos_runB_assignments_summary.csv\n",
      "------------------------------------------------------------\n",
      "‚úÖ Batch processing complete.\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Pipeline:\n",
    "1) Detect neutral-mass peaks from a deconvoluted spectrum (mass intensity; whitespace- or csv-delimited).\n",
    "2) Use detected peaks as candidate proteins and assign raw MS1 peaks by charge-series matching.\n",
    "\n",
    "Outputs:\n",
    "- <deconv_stem>_detected_signals.csv / .png   (neutral-mass peak picks + metadata, includes SNR)\n",
    "- OUT_DIR/assigned_ms1_with_peaks.csv         (annotated raw MS1 with assigned_mass/charge)\n",
    "- OUT_DIR/assignments_summary.csv             (one row per accepted neutral mass; now includes SNR)\n",
    "- OUT_DIR/<several_plots>.png                 (mirror plots + neutral-mass spectrum)\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import glob  # <-- moved up to avoid duplicate import\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from bisect import bisect_left\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.signal import find_peaks\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# --------------------  PEAK DETECTION  ----------------------\n",
    "# ============================================================\n",
    "\n",
    "@dataclass\n",
    "class PeakFindingParams:\n",
    "    min_prominence: float | None = None\n",
    "    min_height: float | None = None\n",
    "    min_distance_pts: int = 10\n",
    "    smooth_window: int = 0\n",
    "    min_snr: float = 0.0\n",
    "\n",
    "\n",
    "def _mad_sigma(y: np.ndarray) -> float:\n",
    "    if y.size == 0:\n",
    "        return 0.0\n",
    "    med = np.median(y)\n",
    "    mad = np.median(np.abs(y - med))\n",
    "    return 1.4826 * mad\n",
    "\n",
    "\n",
    "def _smooth(y: np.ndarray, window: int) -> np.ndarray:\n",
    "    if window < 3 or window % 2 == 0:\n",
    "        return y\n",
    "    kernel = np.ones(window, dtype=float) / window\n",
    "    return np.convolve(y, kernel, mode=\"same\")\n",
    "\n",
    "\n",
    "def _extract_id_list(name: str, key: str) -> list[int] | None:\n",
    "    \"\"\"\n",
    "    Extract an underscore- or hyphen-separated list of integers after a key.\n",
    "    Examples:\n",
    "      \"__pos_3__\"          -> [3]\n",
    "      \"__neg_0_1_2_\"       -> [0,1,2]\n",
    "      \"-pos-10-11\"         -> [10,11]\n",
    "    \"\"\"\n",
    "    m = re.search(rf\"(?:^|[_-]){key}((?:[_-]\\d+)+)(?=[_-]|$)\", name, flags=re.I)\n",
    "    if not m:\n",
    "        m1 = re.search(rf\"(?:^|[_-]){key}[_-]?(\\d+)(?=[_-]|$)\", name, flags=re.I)\n",
    "        if m1:\n",
    "            return [int(m1.group(1))]\n",
    "        return None\n",
    "    parts = re.findall(r\"\\d+\", m.group(1))\n",
    "    return [int(x) for x in parts] if parts else None\n",
    "\n",
    "\n",
    "def parse_metadata_from_filename(path: str | Path) -> dict:\n",
    "    \"\"\"\n",
    "    Extract bin, experiments/controls (IDs + counts), regulation, replicate, source_file.\n",
    "    Regulation token is taken as the LAST occurrence among (negabs|posabs|neg|pos).\n",
    "    \"\"\"\n",
    "    p = Path(path)\n",
    "    name = p.stem\n",
    "\n",
    "    meta = {\n",
    "        \"bin\": None,\n",
    "        \"experiments\": None,\n",
    "        \"controls\": None,\n",
    "        \"experiments_ids\": None,\n",
    "        \"controls_ids\": None,\n",
    "        \"experiments_n\": None,\n",
    "        \"controls_n\": None,\n",
    "        \"regulation\": None,\n",
    "        \"replicate\": None,\n",
    "        \"source_file\": p.name,\n",
    "    }\n",
    "\n",
    "    # bin: \"bin5\"/\"bin_5\" or a leading number \"75__pos_...\"\n",
    "    m = re.search(r\"(?:^|[_-])bin[_-]?(\\d+)(?=[_-]|$)\", name, flags=re.I)\n",
    "    if m:\n",
    "        meta[\"bin\"] = int(m.group(1))\n",
    "    else:\n",
    "        m2 = re.match(r\"^(\\d+)(?=[_-])\", name)\n",
    "        if m2:\n",
    "            meta[\"bin\"] = int(m2.group(1))\n",
    "\n",
    "    exp_ids = _extract_id_list(name, \"pos\")\n",
    "    ctl_ids = _extract_id_list(name, \"neg\")\n",
    "    if exp_ids is not None:\n",
    "        meta[\"experiments_ids\"] = \",\".join(str(x) for x in exp_ids)\n",
    "        meta[\"experiments_n\"] = len(exp_ids)\n",
    "        meta[\"experiments\"] = len(exp_ids)\n",
    "    if ctl_ids is not None:\n",
    "        meta[\"controls_ids\"] = \",\".join(str(x) for x in ctl_ids)\n",
    "        meta[\"controls_n\"] = len(ctl_ids)\n",
    "        meta[\"controls\"] = len(ctl_ids)\n",
    "\n",
    "    reg_tokens = [m.group(1).lower() for m in re.finditer(\n",
    "        r\"(?:^|[_-])(negabs|posabs|neg|pos)(?=[_-]|$)\", name, flags=re.I\n",
    "    )]\n",
    "    if reg_tokens:\n",
    "        token = reg_tokens[-1]\n",
    "        reg_map = {\"negabs\": \"downregulated\", \"neg\": \"downregulated\",\n",
    "                   \"posabs\": \"upregulated\", \"pos\": \"upregulated\"}\n",
    "        meta[\"regulation\"] = reg_map.get(token)\n",
    "\n",
    "    m = re.search(r\"(?:^|[_-])run([A-Za-z])(?=[_-]|$)\", name)\n",
    "    if m:\n",
    "        meta[\"replicate\"] = m.group(1).upper()\n",
    "\n",
    "    return meta\n",
    "\n",
    "\n",
    "def load_space_separated(path: str | Path) -> pd.DataFrame:\n",
    "    path = Path(path)\n",
    "    # whitespace-delimited by default; fallback to CSV\n",
    "    try:\n",
    "        df = pd.read_csv(path, sep=r\"\\s+\", engine=\"python\", header=None,\n",
    "                         names=[\"mass\", \"intensity\"], comment=\"#\")\n",
    "    except Exception:\n",
    "        df = pd.read_csv(path, header=None)\n",
    "        if df.shape[1] >= 2:\n",
    "            df = df.iloc[:, :2]\n",
    "            df.columns = [\"mass\", \"intensity\"]\n",
    "        else:\n",
    "            raise ValueError(\"Deconvoluted file must have at least two columns: mass intensity\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def detect_signals(\n",
    "    df: pd.DataFrame,\n",
    "    params: PeakFindingParams = PeakFindingParams()\n",
    ") -> pd.DataFrame:\n",
    "    # normalize columns\n",
    "    if not {\"mass\", \"intensity\"}.issubset(df.columns):\n",
    "        if df.shape[1] >= 2:\n",
    "            df = df.copy()\n",
    "            df.columns = [\"mass\", \"intensity\"] + [f\"col{i}\" for i in range(2, df.shape[1])]\n",
    "        else:\n",
    "            raise ValueError(\"Input DataFrame must have columns ['mass','intensity'].\")\n",
    "\n",
    "    df = df.replace([np.inf, -np.inf], np.nan).dropna(subset=[\"mass\", \"intensity\"])\n",
    "    df = df.sort_values(\"mass\").reset_index(drop=True)\n",
    "\n",
    "    x = df[\"mass\"].to_numpy(float)\n",
    "    y = df[\"intensity\"].to_numpy(float)\n",
    "\n",
    "    y_proc = _smooth(y, params.smooth_window)\n",
    "\n",
    "    sigma = _mad_sigma(y_proc)\n",
    "    ymax = float(np.max(y_proc)) if y_proc.size else 0.0\n",
    "\n",
    "    min_prom = params.min_prominence or max(6.0 * sigma, 0.001 * ymax)\n",
    "    min_h    = params.min_height     or max(4.0 * sigma, 0.0005 * ymax)\n",
    "\n",
    "    peaks, props = find_peaks(\n",
    "        y_proc,\n",
    "        prominence=min_prom,\n",
    "        height=min_h,\n",
    "        distance=max(1, int(params.min_distance_pts))\n",
    "    )\n",
    "\n",
    "    out = pd.DataFrame({\n",
    "        \"mass\": x[peaks],\n",
    "        \"intensity\": y[peaks],\n",
    "        \"prominence\": props.get(\"prominences\", np.full(peaks.shape, np.nan)),\n",
    "        \"left_base_idx\": props.get(\"left_bases\", np.full(peaks.shape, -1)),\n",
    "        \"right_base_idx\": props.get(\"right_bases\", np.full(peaks.shape, -1)),\n",
    "    })\n",
    "\n",
    "    # SNR estimate and filter\n",
    "    snr_den = sigma if sigma > 0 else (np.std(y_proc) if y_proc.size else 1.0)\n",
    "    snr_den = snr_den if snr_den > 0 else 1.0\n",
    "    out[\"snr\"] = out[\"intensity\"] / snr_den\n",
    "\n",
    "    if params.min_snr > 0:\n",
    "        out = out[out[\"snr\"] >= params.min_snr].reset_index(drop=True)\n",
    "\n",
    "    return out.sort_values(\"intensity\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "\n",
    "def plot_spectrum_with_peaks(\n",
    "    df: pd.DataFrame,\n",
    "    peaks_df: pd.DataFrame,\n",
    "    out_png: str | Path | None = None,\n",
    "    title: str = \"Detected Neutral-Mass Signals\"\n",
    ") -> None:\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.plot(df[\"mass\"].to_numpy(), df[\"intensity\"].to_numpy(), linewidth=1)\n",
    "    if peaks_df is not None and not peaks_df.empty:\n",
    "        plt.scatter(\n",
    "            peaks_df[\"mass\"].to_numpy(),\n",
    "            peaks_df[\"intensity\"].to_numpy(),  # use peaks‚Äô intensities\n",
    "            s=18\n",
    "        )\n",
    "    plt.xlabel(\"Neutral mass (Da)\")\n",
    "    plt.ylabel(\"Intensity (arb.)\")\n",
    "    plt.title(title)\n",
    "    plt.tight_layout()\n",
    "    if out_png:\n",
    "        plt.savefig(out_png, dpi=150)\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# -----------------  CHARGE-SERIES MATCHING  -----------------\n",
    "# ============================================================\n",
    "\n",
    "# Matching parameters (tweak as needed)\n",
    "PROTON_MASS = 1.007276466812  # Da\n",
    "\n",
    "Z_MIN, Z_MAX = 5, 50\n",
    "PPM_TOL = 1000.0              # ppm window for m/z match\n",
    "ABS_DA_TOL = 1.0              # absolute Da floor (used with ppm)\n",
    "MIN_MATCHED_CHARGE_STATES = 4 # require ‚â• N charge-state hits to accept a protein\n",
    "\n",
    "\n",
    "def _read_raw_ms1(path: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Robustly read raw MS1 CSV. Expected two columns (m/z, intensity), with or without headers.\n",
    "    If more columns exist, pick the best 'mz' and 'intensity' columns.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(path)\n",
    "    except Exception:\n",
    "        df = pd.read_csv(path, header=None)\n",
    "\n",
    "    if df.shape[1] == 2:\n",
    "        df.columns = [\"mz\", \"intensity\"]\n",
    "    else:\n",
    "        cols_lower = [str(c).lower() for c in df.columns]\n",
    "        mz_candidates = [i for i, c in enumerate(cols_lower)\n",
    "                         if (\"mz\" in c) or (\"m/z\" in c) or (\"mass/charge\" in c) or (c.strip() == \"m z\")]\n",
    "        int_candidates = [i for i, c in enumerate(cols_lower)\n",
    "                          if (\"int\" in c) or (\"abund\" in c) or (\"height\" in c) or (\"signal\" in c)]\n",
    "        if not mz_candidates:\n",
    "            mz_candidates = [0]\n",
    "        if not int_candidates:\n",
    "            int_candidates = [1 if df.shape[1] > 1 else 0]\n",
    "        df = df.iloc[:, [mz_candidates[0], int_candidates[0]]].copy()\n",
    "        df.columns = [\"mz\", \"intensity\"]\n",
    "\n",
    "    df = df.replace([np.inf, -np.inf], np.nan).dropna()\n",
    "    df = df[df[\"intensity\"] > 0].copy()\n",
    "    df[\"mz\"] = pd.to_numeric(df[\"mz\"], errors=\"coerce\")\n",
    "    df[\"intensity\"] = pd.to_numeric(df[\"intensity\"], errors=\"coerce\")\n",
    "    df = df.dropna().sort_values(\"mz\").reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "def _ppm_window(target_mz: float, ppm: float, abs_da: float) -> tuple[float, float]:\n",
    "    da = target_mz * ppm * 1e-6\n",
    "    tol = max(da, abs_da)\n",
    "    return target_mz - tol, target_mz + tol\n",
    "\n",
    "\n",
    "def _match_targets(sorted_mz: np.ndarray, targets: np.ndarray,\n",
    "                   ppm: float, abs_da: float, available_mask: np.ndarray) -> dict[int, int | None]:\n",
    "    results: dict[int, int | None] = {}\n",
    "    for ti, t in enumerate(targets):\n",
    "        lo, hi = _ppm_window(t, ppm, abs_da)\n",
    "        j = bisect_left(sorted_mz, t)\n",
    "        best_idx = None\n",
    "        best_delta = float(\"inf\")\n",
    "        for k in (j, j-1, j+1, j-2, j+2, j-3, j+3):\n",
    "            if 0 <= k < len(sorted_mz):\n",
    "                mz_k = sorted_mz[k]\n",
    "                if available_mask[k] and (lo <= mz_k <= hi):\n",
    "                    delta = abs(mz_k - t)\n",
    "                    if delta < best_delta:\n",
    "                        best_delta = delta\n",
    "                        best_idx = k\n",
    "        results[ti] = best_idx\n",
    "    return results\n",
    "\n",
    "\n",
    "def _generate_charge_series(neutral_mass: float, z_min: int, z_max: int) -> pd.DataFrame:\n",
    "    z = np.arange(z_min, z_max + 1, dtype=int)\n",
    "    mz = (neutral_mass + z * PROTON_MASS) / z\n",
    "    return pd.DataFrame({\"z\": z, \"target_mz\": mz})\n",
    "\n",
    "\n",
    "def assign_ms1_peaks(raw_df: pd.DataFrame, deconv_peaks_df: pd.DataFrame,\n",
    "                     meta: dict | None = None) -> tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Assign raw MS1 peaks to detected neutral masses (from deconvoluted spectrum) by charge-series matching.\n",
    "\n",
    "    Returns:\n",
    "      assigned_raw: raw_df with columns [assigned_mass, assigned_charge, is_assigned]\n",
    "      assignments_summary: one row per accepted neutral mass with metadata, includes SNR\n",
    "    \"\"\"\n",
    "    raw_df = raw_df.sort_values(\"mz\").reset_index(drop=True)\n",
    "    mz_arr = raw_df[\"mz\"].to_numpy()\n",
    "    inten_arr = raw_df[\"intensity\"].to_numpy()\n",
    "    available = np.ones(len(raw_df), dtype=bool)\n",
    "\n",
    "    assigned_mass = np.full(len(raw_df), np.nan)\n",
    "    assigned_z    = np.full(len(raw_df), np.nan)\n",
    "\n",
    "    summary_rows = []\n",
    "\n",
    "    for r in deconv_peaks_df.itertuples(index=False):\n",
    "        mass = float(r.mass)\n",
    "        mass_intensity = float(r.intensity)\n",
    "        mass_snr = float(getattr(r, \"snr\", np.nan))  # <-- carry SNR into the summary\n",
    "\n",
    "        series = _generate_charge_series(mass, Z_MIN, Z_MAX)\n",
    "        targets = series[\"target_mz\"].to_numpy()\n",
    "\n",
    "        matches = _match_targets(mz_arr, targets, PPM_TOL, ABS_DA_TOL, available_mask=available)\n",
    "\n",
    "        matched_indices = []\n",
    "        matched_z_list  = []\n",
    "        matched_mz_list = []\n",
    "\n",
    "        for ti, k in matches.items():\n",
    "            if k is not None:\n",
    "                matched_indices.append(k)\n",
    "                matched_z_list.append(int(series.iloc[ti][\"z\"]))\n",
    "                matched_mz_list.append(mz_arr[k])\n",
    "\n",
    "        if len(matched_indices) >= MIN_MATCHED_CHARGE_STATES:\n",
    "            # accept and mark assigned\n",
    "            for idx, z_val in zip(matched_indices, matched_z_list):\n",
    "                if available[idx]:\n",
    "                    available[idx] = False\n",
    "                    assigned_mass[idx] = mass\n",
    "                    assigned_z[idx] = z_val\n",
    "\n",
    "            frac_intensity_removed = (\n",
    "                float(np.sum(inten_arr[matched_indices])) / float(np.sum(inten_arr))\n",
    "                if inten_arr.sum() > 0 else 0.0\n",
    "            )\n",
    "\n",
    "            row = {\n",
    "                \"neutral_mass\": mass,\n",
    "                \"deconv_intensity\": mass_intensity,\n",
    "                \"snr\": mass_snr,  # <-- new column in assignments_summary\n",
    "                \"n_matches\": len(matched_indices),\n",
    "                \"matched_z_list\": json.dumps(matched_z_list),\n",
    "                \"matched_mz_list\": json.dumps([round(float(x), 1) for x in matched_mz_list]),\n",
    "                \"ppm_tol\": PPM_TOL,\n",
    "                \"abs_da_tol\": ABS_DA_TOL,\n",
    "                \"fraction_total_intensity_captured\": frac_intensity_removed\n",
    "            }\n",
    "            # attach filename metadata if available\n",
    "            if meta:\n",
    "                row.update({\n",
    "                    \"bin\": meta.get(\"bin\"),\n",
    "                    \"experiments_ids\": meta.get(\"experiments_ids\"),\n",
    "                    \"controls_ids\": meta.get(\"controls_ids\"),\n",
    "                    \"regulation\": meta.get(\"regulation\"),\n",
    "                    \"replicate\": meta.get(\"replicate\"),\n",
    "                    \"source_file\": meta.get(\"source_file\"),\n",
    "                })\n",
    "            summary_rows.append(row)\n",
    "\n",
    "    assigned_raw = raw_df.copy()\n",
    "    assigned_raw[\"assigned_mass\"] = assigned_mass\n",
    "    assigned_raw[\"assigned_charge\"] = assigned_z\n",
    "    assigned_raw[\"is_assigned\"] = ~np.isnan(assigned_mass)\n",
    "\n",
    "    assignments_summary = pd.DataFrame(summary_rows).sort_values(\n",
    "        \"deconv_intensity\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "    # if meta present, also add bin to assigned_raw (useful downstream)\n",
    "    if meta and \"bin\" in meta:\n",
    "        assigned_raw[\"bin\"] = meta[\"bin\"]\n",
    "\n",
    "    return assigned_raw, assignments_summary\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# ----------------------  PLOTTING  --------------------------\n",
    "# ============================================================\n",
    "\n",
    "def plot_neutral_mass_spectrum(deconv_peaks_df: pd.DataFrame, out_dir: str,\n",
    "                               filename: str = \"neutral_mass_spectrum.png\"):\n",
    "    if deconv_peaks_df.empty:\n",
    "        print(\"No neutral masses to plot.\")\n",
    "        return\n",
    "    masses = deconv_peaks_df[\"mass\"].to_numpy()\n",
    "    intens = deconv_peaks_df[\"intensity\"].to_numpy()\n",
    "\n",
    "    plt.figure(figsize=(9, 4.5))\n",
    "    plt.vlines(masses, 0, intens, linewidth=1)\n",
    "    plt.xlabel(\"Neutral mass (Da)\")\n",
    "    plt.ylabel(\"Intensity (arb.)\")\n",
    "    plt.title(\"Neutral Mass Spectrum (detected peaks)\")\n",
    "    plt.tight_layout()\n",
    "    out_path = os.path.join(out_dir, filename)\n",
    "    plt.savefig(out_path, dpi=200)\n",
    "    plt.close()\n",
    "    print(f\"Saved plot: {out_path}\")\n",
    "\n",
    "\n",
    "def plot_mirror_assigned_vs_total(assigned_raw: pd.DataFrame, out_dir: str,\n",
    "                                  filename: str = \"mirror_assigned_vs_total.png\"):\n",
    "    if assigned_raw.empty:\n",
    "        print(\"No assigned/raw data to plot.\")\n",
    "        return\n",
    "\n",
    "    mz = assigned_raw[\"mz\"].to_numpy()\n",
    "    total_int = assigned_raw[\"intensity\"].to_numpy()\n",
    "    assigned_mask = assigned_raw[\"is_assigned\"].to_numpy(dtype=bool)\n",
    "    assigned_int = np.where(assigned_mask, total_int, 0.0)\n",
    "\n",
    "    plt.figure(figsize=(10, 5.2))\n",
    "    plt.vlines(mz, 0, total_int, linewidth=0.6)                 # Top: total\n",
    "    plt.vlines(mz[assigned_mask], 0, -assigned_int[assigned_mask], linewidth=0.8)  # Bottom: assigned (neg)\n",
    "\n",
    "    ymax = total_int.max() if len(total_int) else 1.0\n",
    "    ymin = -assigned_int.max() if assigned_int.any() else -0.1 * ymax\n",
    "    plt.ylim(ymin * 1.05, ymax * 1.05)\n",
    "\n",
    "    plt.xlabel(\"m/z\")\n",
    "    plt.ylabel(\"Intensity (arb.)\")\n",
    "    plt.title(\"Mirror Plot: Total (top) vs Assigned (bottom)\")\n",
    "    plt.tight_layout()\n",
    "    out_path = os.path.join(out_dir, filename)\n",
    "    plt.savefig(out_path, dpi=200)\n",
    "    plt.close()\n",
    "    print(f\"Saved plot: {out_path}\")\n",
    "\n",
    "\n",
    "def plot_mirror_unassigned_vs_total(assigned_raw: pd.DataFrame, out_dir: str,\n",
    "                                    filename: str = \"mirror_unassigned_vs_total.png\"):\n",
    "    if assigned_raw.empty:\n",
    "        print(\"No assigned/raw data to plot.\")\n",
    "        return\n",
    "\n",
    "    mz = assigned_raw[\"mz\"].to_numpy()\n",
    "    total_int = assigned_raw[\"intensity\"].to_numpy()\n",
    "    unassigned_mask = ~assigned_raw[\"is_assigned\"].to_numpy(dtype=bool)\n",
    "    unassigned_int = np.where(unassigned_mask, total_int, 0.0)\n",
    "\n",
    "    plt.figure(figsize=(10, 5.2))\n",
    "    plt.vlines(mz, 0, total_int, linewidth=0.6)                     # Top: total\n",
    "    plt.vlines(mz[unassigned_mask], 0, -unassigned_int[unassigned_mask], linewidth=0.8)  # Bottom: unassigned\n",
    "\n",
    "    ymax = total_int.max() if len(total_int) else 1.0\n",
    "    ymin = -unassigned_int.max() if unassigned_int.any() else -0.1 * ymax\n",
    "    plt.ylim(ymin * 1.05, ymax * 1.05)\n",
    "\n",
    "    plt.xlabel(\"m/z\")\n",
    "    plt.ylabel(\"Intensity (arb.)\")\n",
    "    plt.title(\"Mirror Plot: Total (top) vs Non-assigned (bottom)\")\n",
    "    plt.tight_layout()\n",
    "    out_path = os.path.join(out_dir, filename)\n",
    "    plt.savefig(out_path, dpi=200)\n",
    "    plt.close()\n",
    "    print(f\"Saved plot: {out_path}\")\n",
    "\n",
    "\n",
    "def plot_mirror_assigned_by_protein_vs_total(\n",
    "    assigned_raw: pd.DataFrame,\n",
    "    out_dir: str,\n",
    "    filename: str = \"mirror_assigned_by_protein_vs_total.png\",\n",
    "    max_legend_items: int = 20\n",
    "):\n",
    "    if assigned_raw.empty:\n",
    "        print(\"No assigned/raw data to plot.\")\n",
    "        return\n",
    "\n",
    "    mz = assigned_raw[\"mz\"].to_numpy()\n",
    "    total_int = assigned_raw[\"intensity\"].to_numpy()\n",
    "\n",
    "    plt.figure(figsize=(11, 5.6))\n",
    "    plt.vlines(mz, 0, total_int, linewidth=0.5)  # top: total\n",
    "\n",
    "    df_assigned_only = assigned_raw[assigned_raw[\"is_assigned\"]].copy()\n",
    "    if df_assigned_only.empty:\n",
    "        plt.xlabel(\"m/z\")\n",
    "        plt.ylabel(\"Intensity (arb.)\")\n",
    "        plt.title(\"Mirror Plot: Total (top) vs Assigned by Protein (bottom)\")\n",
    "        plt.tight_layout()\n",
    "        out_path = os.path.join(out_dir, filename)\n",
    "        plt.savefig(out_path, dpi=200)\n",
    "        plt.close()\n",
    "        print(f\"Saved plot: {out_path}\")\n",
    "        return\n",
    "\n",
    "    counts = (\n",
    "        df_assigned_only.groupby(\"assigned_mass\", dropna=True)[\"is_assigned\"]\n",
    "        .count()\n",
    "        .sort_values(ascending=False)\n",
    "    )\n",
    "    proteins_in_order = counts.index.tolist()\n",
    "    color_cycle = plt.rcParams['axes.prop_cycle'].by_key().get(\n",
    "        'color', ['C0','C1','C2','C3','C4','C5','C6','C7','C8','C9']\n",
    "    )\n",
    "\n",
    "    legend_entries = 0\n",
    "    for i, mass in enumerate(proteins_in_order):\n",
    "        mask = (assigned_raw[\"assigned_mass\"] == mass)\n",
    "        mz_i = assigned_raw.loc[mask, \"mz\"].to_numpy()\n",
    "        inten_i = assigned_raw.loc[mask, \"intensity\"].to_numpy()\n",
    "\n",
    "        label = None\n",
    "        if legend_entries < max_legend_items:\n",
    "            label = f\"{mass/1000:.2f} kDa (n={len(mz_i)})\"\n",
    "            legend_entries += 1\n",
    "\n",
    "        plt.vlines(\n",
    "            mz_i, 0, -inten_i,\n",
    "            linewidth=0.8,\n",
    "            color=color_cycle[i % len(color_cycle)],\n",
    "            label=label\n",
    "        )\n",
    "\n",
    "    ymax = total_int.max() if len(total_int) else 1.0\n",
    "    ymin = -df_assigned_only[\"intensity\"].max() if len(df_assigned_only) else -0.1 * ymax\n",
    "    plt.ylim(ymin * 1.05, ymax * 1.05)\n",
    "\n",
    "    if legend_entries:\n",
    "        plt.legend(title=\"Assigned proteins\", loc=\"upper right\", fontsize=8, ncol=1)\n",
    "\n",
    "    plt.xlabel(\"m/z\")\n",
    "    plt.ylabel(\"Intensity (arb.)\")\n",
    "    plt.title(\"Mirror Plot: Total (top) vs Assigned by Protein (bottom)\")\n",
    "    plt.tight_layout()\n",
    "    out_path = os.path.join(out_dir, filename)\n",
    "    plt.savefig(out_path, dpi=200)\n",
    "    plt.close()\n",
    "    print(f\"Saved plot: {out_path}\")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# ----------------------  BATCH MAIN  ------------------------\n",
    "# ============================================================\n",
    "# Process many files: deconvoluted spectra in one folder,\n",
    "# raw MS1 spectra in another folder. Outputs go to a separate folder.\n",
    "#\n",
    "# Pairing rule: files are matched by a shared \"base key\"\n",
    "#   - deconv: <base>_mass.txt\n",
    "#   - raw:    <base>.csv\n",
    "# Example:\n",
    "#   RAW_DIR:    F:/raw_folder\n",
    "#       5__pos_1__neg_0_pos_runA.csv\n",
    "#   DECONV_DIR: F:/deconv_folder\n",
    "#       5__pos_1__neg_0_pos_runA_mass.txt\n",
    "#   -> base key = \"5__pos_1__neg_0_pos_runA\"\n",
    "#\n",
    "# Notes:\n",
    "# - Neutral-mass peak detection is always run (on decon files).\n",
    "# - Assignment & mirror plots are run only if the matching raw CSV exists.\n",
    "# - All outputs (CSVs/PNGs) go under OUT_DIR/<base>/...\n",
    "# ============================================================\n",
    "\n",
    "# ---------------------- User-configurable ----------------------\n",
    "RAW_DIR    = r\"F:\\binary\\raw\"      # folder with raw MS1 CSVs (m/z, intensity)\n",
    "DECONV_DIR = r\"F:\\binary\\decon\"    # folder with *_mass.txt (mass intensity)\n",
    "OUT_DIR    = r\"F:\\binary\\firstpass\"   # folder to hold all outputs\n",
    "\n",
    "# Glob patterns (adjust if your extensions differ)\n",
    "RAW_GLOB    = \"*.csv\"\n",
    "DECONV_GLOB = \"*_mass.txt\"\n",
    "\n",
    "# ---- Neutral-mass peak detection parameters ----\n",
    "DECONV_DETECT_PARAMS = PeakFindingParams(\n",
    "    min_distance_pts=20,  # decon masses can be coarse; 20 is a good start\n",
    "    min_snr=10,           # enforce minimum SNR\n",
    "    smooth_window=0,      # set to 5/7 if your decon spectrum is very noisy\n",
    "    # min_prominence=None, min_height=None  # auto from MAD if None\n",
    ")\n",
    "\n",
    "def _base_key_from_deconv(path: Path) -> str:\n",
    "    \"\"\"\n",
    "    For '.../<base>_mass.txt' ‚Üí return '<base>'.\n",
    "    \"\"\"\n",
    "    stem = path.stem\n",
    "    if stem.endswith(\"_mass\"):\n",
    "        return stem[:-5]  # drop \"_mass\"\n",
    "    return stem  # fallback\n",
    "\n",
    "\n",
    "def _base_key_from_raw(path: Path) -> str:\n",
    "    \"\"\"\n",
    "    For '.../<base>.csv' ‚Üí return '<base>'.\n",
    "    \"\"\"\n",
    "    return path.stem\n",
    "\n",
    "\n",
    "def _ensure_dir(p: str | Path) -> None:\n",
    "    Path(p).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "def process_one_pair(deconv_path: Path, raw_path: Path | None) -> None:\n",
    "    \"\"\"\n",
    "    - Detect neutral-mass peaks from deconv_path (always).\n",
    "    - If raw_path exists, assign charge-series and make plots.\n",
    "    - Write all outputs under OUT_DIR/<base>/...\n",
    "    \"\"\"\n",
    "    base = _base_key_from_deconv(deconv_path)\n",
    "    out_root = Path(OUT_DIR) / base\n",
    "    _ensure_dir(out_root)\n",
    "\n",
    "    # --- 1) Load deconvoluted spectrum & detect neutral-mass peaks ---\n",
    "    meta = parse_metadata_from_filename(deconv_path)\n",
    "    deconv_raw = load_space_separated(deconv_path)\n",
    "    deconv_peaks = detect_signals(deconv_raw, params=DECONV_DETECT_PARAMS)\n",
    "\n",
    "    # Attach filename metadata to neutral-mass peaks table\n",
    "    deconv_peaks = deconv_peaks.assign(\n",
    "        bin=meta.get(\"bin\"),\n",
    "        experiments_ids=meta.get(\"experiments_ids\"),\n",
    "        controls_ids=meta.get(\"controls_ids\"),\n",
    "        regulation=meta.get(\"regulation\"),\n",
    "        replicate=meta.get(\"replicate\"),\n",
    "        source_file=meta.get(\"source_file\"),\n",
    "    )\n",
    "\n",
    "    # Save + plot neutral-mass detections (to OUT_DIR/<base>/...)\n",
    "    out_detect_csv = out_root / f\"{base}_detected_signals.csv\"\n",
    "    out_detect_png = out_root / f\"{base}_detected_signals.png\"\n",
    "    deconv_peaks.to_csv(out_detect_csv, index=False)\n",
    "    plot_spectrum_with_peaks(deconv_raw, deconv_peaks, out_png=str(out_detect_png))\n",
    "    print(f\"[{base}] Neutral-mass detection: {len(deconv_peaks)} peaks ‚Üí {out_detect_csv}\")\n",
    "    print(f\"[{base}] Detection plot saved ‚Üí {out_detect_png}\")\n",
    "    print(f\"[{base}] Parsed filename metadata: {meta}\")\n",
    "\n",
    "    # --- 2) If we have the matching raw MS1 CSV, assign charge-series ---\n",
    "    if raw_path is None or not raw_path.exists():\n",
    "        print(f\"[{base}] ‚ö† No matching RAW CSV found. Skipping assignment.\")\n",
    "        return\n",
    "\n",
    "    raw_df = _read_raw_ms1(raw_path)\n",
    "    assigned_raw, summary = assign_ms1_peaks(raw_df, deconv_peaks, meta=meta)\n",
    "\n",
    "    out_assigned = out_root / f\"{base}_assigned_ms1_with_peaks.csv\"\n",
    "    out_summary  = out_root / f\"{base}_assignments_summary.csv\"\n",
    "    assigned_raw.to_csv(out_assigned, index=False)\n",
    "    summary.to_csv(out_summary, index=False)\n",
    "\n",
    "    # --- 3) Plots on assignments ---\n",
    "    plot_neutral_mass_spectrum(deconv_peaks, str(out_root), filename=f\"{base}_neutral_mass_spectrum.png\")\n",
    "    plot_mirror_assigned_vs_total(assigned_raw, str(out_root), filename=f\"{base}_mirror_assigned_vs_total.png\")\n",
    "    plot_mirror_unassigned_vs_total(assigned_raw, str(out_root), filename=f\"{base}_mirror_unassigned_vs_total.png\")\n",
    "    plot_mirror_assigned_by_protein_vs_total(\n",
    "        assigned_raw, str(out_root), filename=f\"{base}_mirror_assigned_by_protein_vs_total.png\", max_legend_items=20\n",
    "    )\n",
    "\n",
    "    # --- 4) Console report ---\n",
    "    print(f\"\\n=== [{base}] Summary ===\")\n",
    "    print(f\"Raw MS1 peaks (rows): {len(raw_df):,}\")\n",
    "    print(f\"Detected neutral-mass peaks: {len(deconv_peaks):,}\")\n",
    "    print(f\"Assigned raw peaks: {int(assigned_raw['is_assigned'].sum()):,}\")\n",
    "    print(f\"Non-assigned raw peaks: {int((~assigned_raw['is_assigned']).sum()):,}\")\n",
    "    print(f\"Saved: {out_assigned}\")\n",
    "    print(f\"Saved: {out_summary}\")\n",
    "    print(\"------------------------------------------------------------\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    _ensure_dir(OUT_DIR)\n",
    "\n",
    "    # Index raw files by base key\n",
    "    raw_files = [Path(p) for p in glob.glob(str(Path(RAW_DIR) / RAW_GLOB))]\n",
    "    raw_index = {_base_key_from_raw(p): p for p in raw_files}\n",
    "\n",
    "    # Walk all deconvoluted files and process\n",
    "    deconv_files = [Path(p) for p in glob.glob(str(Path(DECONV_DIR) / DECONV_GLOB))]\n",
    "    if not deconv_files:\n",
    "        print(f\"‚ö† No deconvoluted files found in: {DECONV_DIR} (pattern: {DECONV_GLOB})\")\n",
    "        return\n",
    "\n",
    "    print(f\"Found {len(deconv_files)} decon file(s) in {DECONV_DIR}\")\n",
    "    print(f\"Found {len(raw_files)} raw CSV file(s) in {RAW_DIR}\")\n",
    "\n",
    "    for deconv_path in sorted(deconv_files):\n",
    "        base = _base_key_from_deconv(deconv_path)\n",
    "        raw_path = raw_index.get(base, None)\n",
    "        try:\n",
    "            process_one_pair(deconv_path, raw_path)\n",
    "        except Exception as e:\n",
    "            print(f\"[{base}] ‚ùå Error: {e}\")\n",
    "\n",
    "    print(\"‚úÖ Batch processing complete.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "01fb70d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8 summary file(s). Copying to F:\\binary\\report1...\n",
      "Copied: F:\\binary\\firstpass\\frac_pellet_grads_AB__pos_1__neg_0_negabs_runA\\frac_pellet_grads_AB__pos_1__neg_0_negabs_runA_assignments_summary.csv ‚Üí F:\\binary\\report1\\frac_pellet_grads_AB__pos_1__neg_0_negabs_runA_assignments_summary.csv\n",
      "Copied: F:\\binary\\firstpass\\frac_pellet_grads_AB__pos_1__neg_0_negabs_runB\\frac_pellet_grads_AB__pos_1__neg_0_negabs_runB_assignments_summary.csv ‚Üí F:\\binary\\report1\\frac_pellet_grads_AB__pos_1__neg_0_negabs_runB_assignments_summary.csv\n",
      "Copied: F:\\binary\\firstpass\\frac_pellet_grads_AB__pos_1__neg_0_pos_runA\\frac_pellet_grads_AB__pos_1__neg_0_pos_runA_assignments_summary.csv ‚Üí F:\\binary\\report1\\frac_pellet_grads_AB__pos_1__neg_0_pos_runA_assignments_summary.csv\n",
      "Copied: F:\\binary\\firstpass\\frac_pellet_grads_AB__pos_1__neg_0_pos_runB\\frac_pellet_grads_AB__pos_1__neg_0_pos_runB_assignments_summary.csv ‚Üí F:\\binary\\report1\\frac_pellet_grads_AB__pos_1__neg_0_pos_runB_assignments_summary.csv\n",
      "Copied: F:\\binary\\firstpass\\frac_soluble_grads_AB__pos_1__neg_0_negabs_runA\\frac_soluble_grads_AB__pos_1__neg_0_negabs_runA_assignments_summary.csv ‚Üí F:\\binary\\report1\\frac_soluble_grads_AB__pos_1__neg_0_negabs_runA_assignments_summary.csv\n",
      "Copied: F:\\binary\\firstpass\\frac_soluble_grads_AB__pos_1__neg_0_negabs_runB\\frac_soluble_grads_AB__pos_1__neg_0_negabs_runB_assignments_summary.csv ‚Üí F:\\binary\\report1\\frac_soluble_grads_AB__pos_1__neg_0_negabs_runB_assignments_summary.csv\n",
      "Copied: F:\\binary\\firstpass\\frac_soluble_grads_AB__pos_1__neg_0_pos_runA\\frac_soluble_grads_AB__pos_1__neg_0_pos_runA_assignments_summary.csv ‚Üí F:\\binary\\report1\\frac_soluble_grads_AB__pos_1__neg_0_pos_runA_assignments_summary.csv\n",
      "Copied: F:\\binary\\firstpass\\frac_soluble_grads_AB__pos_1__neg_0_pos_runB\\frac_soluble_grads_AB__pos_1__neg_0_pos_runB_assignments_summary.csv ‚Üí F:\\binary\\report1\\frac_soluble_grads_AB__pos_1__neg_0_pos_runB_assignments_summary.csv\n",
      "‚úÖ Copy complete.\n",
      "Reading F:\\binary\\report1\\frac_pellet_grads_AB__pos_1__neg_0_negabs_runA_assignments_summary.csv\n",
      "Reading F:\\binary\\report1\\frac_pellet_grads_AB__pos_1__neg_0_negabs_runB_assignments_summary.csv\n",
      "Reading F:\\binary\\report1\\frac_pellet_grads_AB__pos_1__neg_0_pos_runA_assignments_summary.csv\n",
      "Reading F:\\binary\\report1\\frac_pellet_grads_AB__pos_1__neg_0_pos_runB_assignments_summary.csv\n",
      "Reading F:\\binary\\report1\\frac_soluble_grads_AB__pos_1__neg_0_negabs_runA_assignments_summary.csv\n",
      "Reading F:\\binary\\report1\\frac_soluble_grads_AB__pos_1__neg_0_negabs_runB_assignments_summary.csv\n",
      "Reading F:\\binary\\report1\\frac_soluble_grads_AB__pos_1__neg_0_pos_runA_assignments_summary.csv\n",
      "Reading F:\\binary\\report1\\frac_soluble_grads_AB__pos_1__neg_0_pos_runB_assignments_summary.csv\n",
      "‚úÖ Combined 8 files ‚Üí F:\\binary\\report1\\report.csv\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Collect all *_assignments_summary.csv files from subfolders into one folder,\n",
    "then concatenate them into a single report.csv.\n",
    "\n",
    "Steps:\n",
    "1. Search recursively for *_assignments_summary.csv in BATCH_OUT_DIR.\n",
    "2. Copy all to SUMMARY_OUT (renaming duplicates).\n",
    "3. Concatenate all collected CSVs ‚Üí report.csv.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "# ---------------------- USER SETTINGS ----------------------\n",
    "BATCH_OUT_DIR = r\"F:\\binary\\firstpass\"          # where all subfolders were created\n",
    "SUMMARY_OUT   = r\"F:\\binary\\report1\"             # folder to collect summaries\n",
    "PATTERN       = \"*_assignments_summary.csv\"     # filename pattern\n",
    "OUTPUT_FILE   = Path(SUMMARY_OUT) / \"report.csv\"\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "def collect_summary_files():\n",
    "    \"\"\"Collect all *_assignments_summary.csv into SUMMARY_OUT.\"\"\"\n",
    "    os.makedirs(SUMMARY_OUT, exist_ok=True)\n",
    "    summary_files = glob.glob(str(Path(BATCH_OUT_DIR) / \"**\" / PATTERN), recursive=True)\n",
    "\n",
    "    if not summary_files:\n",
    "        print(\"‚ö† No summary files found.\")\n",
    "        return []\n",
    "\n",
    "    print(f\"Found {len(summary_files)} summary file(s). Copying to {SUMMARY_OUT}...\")\n",
    "    copied = []\n",
    "    for f in summary_files:\n",
    "        src = Path(f)\n",
    "        dst = Path(SUMMARY_OUT) / src.name\n",
    "        if dst.exists():\n",
    "            dst = Path(SUMMARY_OUT) / f\"{src.parent.name}_{src.name}\"\n",
    "        shutil.copy2(src, dst)\n",
    "        copied.append(dst)\n",
    "        print(f\"Copied: {src} ‚Üí {dst}\")\n",
    "\n",
    "    print(\"‚úÖ Copy complete.\")\n",
    "    return copied\n",
    "\n",
    "\n",
    "def concat_csvs(folder_path: str, output_file: str):\n",
    "    \"\"\"Concatenate all CSV files in folder_path ‚Üí output_file.\"\"\"\n",
    "    csv_files = [f for f in os.listdir(folder_path) if f.endswith(\".csv\")]\n",
    "    if not csv_files:\n",
    "        raise FileNotFoundError(\"No CSV files found in the folder!\")\n",
    "\n",
    "    df_list = []\n",
    "    for file in csv_files:\n",
    "        file_path = os.path.join(folder_path, file)\n",
    "        print(f\"Reading {file_path}\")\n",
    "        df = pd.read_csv(file_path)\n",
    "        df_list.append(df)\n",
    "\n",
    "    combined_df = pd.concat(df_list, ignore_index=True)\n",
    "    combined_df.to_csv(output_file, index=False)\n",
    "    print(f\"‚úÖ Combined {len(csv_files)} files ‚Üí {output_file}\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    copied_files = collect_summary_files()\n",
    "    if copied_files:\n",
    "        concat_csvs(SUMMARY_OUT, OUTPUT_FILE)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76e45e0d",
   "metadata": {},
   "source": [
    "Quantification of all proteoforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "15558a3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: F:\\binary\\report1\\assignments_with_quant_sums_aaa.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import ast\n",
    "import pandas as pd\n",
    "\n",
    "# --------------------\n",
    "# Config (edit paths)\n",
    "# --------------------\n",
    "DATASET_RT_PATH = r\"F:\\binary\\ms1_aggregate_per_sample_training.csv\"          # wide matrix with cast_* columns\n",
    "ASSIGNMENTS_PATH = r\"F:\\binary\\report1\\report.csv\"          # has 'fractions' and 'matched_mz_list'\n",
    "OUT_PATH = os.path.join(\n",
    "    os.path.dirname(ASSIGNMENTS_PATH) or \".\",\n",
    "    \"assignments_with_quant_sums_aaa.csv\"\n",
    ")\n",
    "\n",
    "# --------------------\n",
    "# Helpers\n",
    "# --------------------\n",
    "def to_cast_col(n: float) -> str:\n",
    "    \"\"\"Map an m/z to its cast_* column name: int((mz-600)*10), zero-padded.\"\"\"\n",
    "    col_num = int((float(n) - 600.0) * 10.0)\n",
    "    return \"cast_\" + str(col_num).zfill(5)\n",
    "\n",
    "def parse_mz_list(val):\n",
    "    \"\"\"Safely parse matched_mz_list cells that look like '[864.9, 865.2, ...]'.\"\"\"\n",
    "    try:\n",
    "        out = ast.literal_eval(str(val))\n",
    "        if isinstance(out, (list, tuple)):\n",
    "            return [float(x) for x in out]\n",
    "    except Exception:\n",
    "        pass\n",
    "    return []\n",
    "\n",
    "# --------------------\n",
    "# Load data\n",
    "# --------------------\n",
    "df_rt = pd.read_csv(DATASET_RT_PATH)\n",
    "df_asn = pd.read_csv(ASSIGNMENTS_PATH)\n",
    "\n",
    "# Basic checks\n",
    "for col in [\"fractions\", \"target\"]:\n",
    "    if col not in df_rt.columns:\n",
    "        raise KeyError(f\"'{col}' column is required in dataset_rt.csv\")\n",
    "\n",
    "if \"fractions\" not in df_asn.columns or \"matched_mz_list\" not in df_asn.columns:\n",
    "    raise KeyError(\"assignments CSV must contain 'fractions' and 'matched_mz_list' columns\")\n",
    "\n",
    "# NEW columns to be added to assignments\n",
    "new_cols = [\"group_0_sum\", \"group_1_sum\", \"group_2_sum\", \"group_3_sum\",\n",
    "            \"n_mz_used\", \"n_mz_found\", \"missing_cast_columns\"]\n",
    "for c in new_cols:\n",
    "    if c in df_asn.columns:\n",
    "        # avoid accidental overwrite\n",
    "        df_asn.drop(columns=[c], inplace=True)\n",
    "\n",
    "# --------------------\n",
    "# Row-wise quantification\n",
    "# --------------------\n",
    "results = []\n",
    "for idx, row in df_asn.iterrows():\n",
    "    frac_value = row[\"fractions\"]  # keep as-is (can be string like 'soluble_fraction')\n",
    "    mz_list = parse_mz_list(row[\"matched_mz_list\"])\n",
    "    cast_cols = [to_cast_col(mz) for mz in mz_list]\n",
    "\n",
    "    # Filter dataset_rt to this fraction\n",
    "    df_frac = df_rt[df_rt[\"fractions\"] == frac_value]\n",
    "    if df_frac.empty:\n",
    "        res = dict(\n",
    "            group_0_sum=float(\"nan\"),\n",
    "            group_1_sum=float(\"nan\"),\n",
    "            group_2_sum=float(\"nan\"),\n",
    "            group_3_sum=float(\"nan\"),\n",
    "            n_mz_used=len(cast_cols),\n",
    "            n_mz_found=0,\n",
    "            missing_cast_columns=\", \".join(cast_cols) if cast_cols else \"\"\n",
    "        )\n",
    "        results.append(res)\n",
    "        continue\n",
    "\n",
    "    # Ensure target present\n",
    "    if \"target\" not in df_frac.columns:\n",
    "        raise KeyError(\"Column 'target' not found in dataset_rt.csv\")\n",
    "\n",
    "    existing = [c for c in cast_cols if c in df_frac.columns]\n",
    "    missing = [c for c in cast_cols if c not in df_frac.columns]\n",
    "\n",
    "    if not existing:\n",
    "        sums = {0: float(\"nan\"), 1: float(\"nan\"), 2: float(\"nan\"), 3: float(\"nan\")}\n",
    "    else:\n",
    "        # Sum intensities across all selected cast_* columns per target\n",
    "        grouped = df_frac.groupby(\"target\")[existing].sum()\n",
    "        total_per_target = grouped.sum(axis=1)  # sum across those cast_* columns\n",
    "        sums = {t: float(total_per_target.get(t, float(\"nan\"))) for t in [0, 1, 2, 3]}\n",
    "\n",
    "    res = dict(\n",
    "        group_0_sum=sums[0],\n",
    "        group_1_sum=sums[1],\n",
    "        group_2_sum=sums[2],\n",
    "        group_3_sum=sums[3],\n",
    "        n_mz_used=len(cast_cols),\n",
    "        n_mz_found=len(existing),\n",
    "        missing_cast_columns=\", \".join(missing) if missing else \"\"\n",
    "    )\n",
    "    results.append(res)\n",
    "\n",
    "# Attach results\n",
    "df_quant = pd.DataFrame(results, index=df_asn.index)\n",
    "df_asn_out = pd.concat([df_asn, df_quant], axis=1)\n",
    "\n",
    "# --------------------\n",
    "# Save updated CSV\n",
    "# --------------------\n",
    "df_asn_out.to_csv(OUT_PATH, index=False)\n",
    "print(f\"Saved: {OUT_PATH}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8dc503a",
   "metadata": {},
   "source": [
    "identification of PFR by matching with tdportal report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc6cb961",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\benja\\AppData\\Local\\Temp\\ipykernel_16692\\3179398044.py:318: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_numeric without passing `errors` and catch exceptions explicitly instead\n",
      "  df2[\"PFR\"] = pd.to_numeric(df2[\"PFR\"], errors=\"ignore\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved with 1 sets of columns (best_match / matched_pfr / mode_pfr / mode_pfr_count / mode_accession) ‚Üí F:/binary/ids.csv\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Combine charge-assignment summary with best matches from a databank,\n",
    "supporting MULTIPLE (rt_window, mz_tol, mass_tol) triplets in one run.\n",
    "\n",
    "- Reads:\n",
    "    df1: assignments_summary (must have: neutral_mass, bin (or 'bin '), matched_mz_list)\n",
    "    df2: databank_with_ids (must have: rt_aligned, precursor_mz, MASS, Accession, PFR)\n",
    "\n",
    "- For each row in df1, for each m/z in matched_mz_list:\n",
    "    find the single best df2 row where ALL hold:\n",
    "        |rt_aligned - bin|    <= rt_window\n",
    "        |precursor_mz - m/z|  <= mz_tol\n",
    "        |MASS - neutral_mass| <= mass_tol\n",
    "  Then format:\n",
    "      best_match_* : \"[<mz>: <Accession>, <MASS_from_df2>, <PFR>] ...\"  (PFR optional)\n",
    "      matched_pfr_*: \"[<PFR_or_null_per_mz> ...]\"  (aligned with matched_mz_list)\n",
    "      mode_pfr_*   : most common non-null PFR across matches (per row)\n",
    "      mode_pfr_count_* : frequency (count) of that PFR (non-null only)\n",
    "      mode_accession_* : most common Accession across matches (per row), shown only if mode_pfr_count_* >= MIN_MODE_PFR_COUNT\n",
    "\n",
    "- Outputs:\n",
    "    One CSV with multiple columns per tolerance triplet:\n",
    "      best_match_rt<RT>_mz<MZ>_mass<MASS>\n",
    "      matched_pfr_rt<RT>_mz<MZ>_mass<MASS>\n",
    "      mode_pfr_rt<RT>_mz<MZ>_mass<MASS>\n",
    "      mode_pfr_count_rt<RT>_mz<MZ>_mass<MASS>\n",
    "      mode_accession_rt<RT>_mz<MZ>_mass<MASS>\n",
    "\n",
    "Edit the 3 PATHS and the PARAM_SETS below before running.\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "import os\n",
    "import ast\n",
    "from typing import Optional, Dict, List, Tuple, Any\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# CONFIG: edit these paths\n",
    "# ----------------------------\n",
    "CHARGE_FILE_PATH = r\"F:/binary/report1/report1.csv\"\n",
    "DATABANK_PATH    = r\"F:/binary/ms2_with_ids_hash_nocast.csv\"\n",
    "OUTPUT_PATH      = r\"F:/binary/ids1.csv\"\n",
    "\n",
    "# Provide one or more (rt_window, mz_tol, mass_tol) triplets here.\n",
    "PARAM_SETS: List[Tuple[float, float, float]] = [\n",
    "    (55.0, 2.0, 90.0),\n",
    "    # (30.0, 1.0, 50.0),\n",
    "]\n",
    "\n",
    "# Keep \"null\" placeholders in matched_pfr_* so positions align with mz_list.\n",
    "PFR_KEEP_PLACEHOLDERS: bool = True\n",
    "\n",
    "# Minimum frequency required to report mode PFR and mode Accession.\n",
    "MIN_MODE_PFR_COUNT: int = 1\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Helpers\n",
    "# ----------------------------\n",
    "def _num(s: pd.Series) -> pd.Series:\n",
    "    \"\"\"Coerce to numeric, invalid ‚Üí NaN.\"\"\"\n",
    "    return pd.to_numeric(s, errors=\"coerce\")\n",
    "\n",
    "\n",
    "def _to_scalar(x: Any) -> Any:\n",
    "    \"\"\"Flatten 0-d arrays and coerce numeric strings to float when possible.\"\"\"\n",
    "    if isinstance(x, np.ndarray) and x.ndim == 0:\n",
    "        x = x.item()\n",
    "    if isinstance(x, str):\n",
    "        try:\n",
    "            return float(x)\n",
    "        except Exception:\n",
    "            return x\n",
    "    return x\n",
    "\n",
    "\n",
    "def _safe_parse_list(val) -> List[float]:\n",
    "    \"\"\"Convert a string-repr list into a Python list of floats safely.\"\"\"\n",
    "    if isinstance(val, str):\n",
    "        try:\n",
    "            parsed = ast.literal_eval(val)\n",
    "            if isinstance(parsed, (list, tuple, np.ndarray)):\n",
    "                return [float(x) for x in parsed]\n",
    "            return []\n",
    "        except Exception:\n",
    "            return []\n",
    "    if isinstance(val, (list, tuple, np.ndarray)):\n",
    "        try:\n",
    "            return [float(x) for x in val]\n",
    "        except Exception:\n",
    "            return []\n",
    "    return []\n",
    "\n",
    "\n",
    "def _ensure_columns(df: pd.DataFrame, required: List[str]) -> None:\n",
    "    missing = [c for c in required if c not in df.columns]\n",
    "    if missing:\n",
    "        raise KeyError(f\"Missing required column(s): {missing}\")\n",
    "\n",
    "\n",
    "def _fmt_suffix(v: float) -> str:\n",
    "    \"\"\"\n",
    "    Make a tidy string for column suffixes (avoid many decimals).\n",
    "    e.g., 10 -> '10', 2.0 -> '2', 1.5 -> '1p5'\n",
    "    \"\"\"\n",
    "    if float(v).is_integer():\n",
    "        return f\"{int(v)}\"\n",
    "    # Replace '.' with 'p' to keep it column-name friendly\n",
    "    return str(v).replace('.', 'p')\n",
    "\n",
    "\n",
    "def _mode_or_none(items: List[Any]) -> Optional[Any]:\n",
    "    \"\"\"\n",
    "    Return the most common value in `items` excluding None/NaN.\n",
    "    If tie, Counter.most_common returns first encountered top count.\n",
    "    \"\"\"\n",
    "    clean = []\n",
    "    for v in items:\n",
    "        if v is None:\n",
    "            continue\n",
    "        if isinstance(v, float) and np.isnan(v):\n",
    "            continue\n",
    "        clean.append(v)\n",
    "    if not clean:\n",
    "        return None\n",
    "    return Counter(clean).most_common(1)[0][0]\n",
    "\n",
    "\n",
    "def _mode_and_count(items: List[Any]) -> Tuple[Optional[Any], int]:\n",
    "    \"\"\"\n",
    "    Return (mode_value, count) over non-null items.\n",
    "    If no non-null items, returns (None, 0).\n",
    "    \"\"\"\n",
    "    clean = []\n",
    "    for v in items:\n",
    "        if v is None:\n",
    "            continue\n",
    "        if isinstance(v, float) and np.isnan(v):\n",
    "            continue\n",
    "        clean.append(v)\n",
    "    if not clean:\n",
    "        return None, 0\n",
    "    val, cnt = Counter(clean).most_common(1)[0]\n",
    "    return val, int(cnt)\n",
    "\n",
    "\n",
    "def _mode_and_count_with_cutoff(\n",
    "    pfrs: List[Any], accs: List[Any], min_count: int\n",
    ") -> Tuple[Optional[Any], int, Optional[Any]]:\n",
    "    \"\"\"\n",
    "    Return (mode_pfr, count, mode_accession) applying a frequency cutoff on PFR.\n",
    "    If the mode PFR count < min_count, return (None, 0, None).\n",
    "    Otherwise return (mode_pfr, count, mode_accession).\n",
    "    \"\"\"\n",
    "    pfr_val, pfr_cnt = _mode_and_count(pfrs)\n",
    "    acc_val = _mode_or_none(accs)\n",
    "\n",
    "    if pfr_cnt < min_count:\n",
    "        return None, 0, None\n",
    "    return pfr_val, pfr_cnt, acc_val\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Core search\n",
    "# ----------------------------\n",
    "def search_best(\n",
    "    df2: pd.DataFrame,\n",
    "    rt_query: float,\n",
    "    mz_query: float,\n",
    "    mass_query: float,\n",
    "    rt_window: float,\n",
    "    mz_tol: float,\n",
    "    mass_tol: float\n",
    ") -> Optional[Dict]:\n",
    "    \"\"\"\n",
    "    Return the single best match (row as dict) if ALL three criteria match:\n",
    "      |rt - rt_query| <= rt_window\n",
    "      |mz - mz_query| <= mz_tol\n",
    "      |mass - mass_query| <= mass_tol\n",
    "    Otherwise returns None.\n",
    "    \"\"\"\n",
    "    d_rt   = (df2[\"rt_aligned\"] - float(rt_query)).abs()\n",
    "    d_mz   = (df2[\"precursor_mz\"] - float(mz_query)).abs()\n",
    "    d_mass = (df2[\"MASS\"] - float(mass_query)).abs()\n",
    "\n",
    "    mask = (d_rt <= rt_window) & (d_mz <= mz_tol) & (d_mass <= mass_tol)\n",
    "    if not mask.any():\n",
    "        return None\n",
    "\n",
    "    cand = df2.loc[mask].copy()\n",
    "    cand[\"score\"] = (\n",
    "        d_rt.loc[cand.index] / max(rt_window, 1e-12) +\n",
    "        d_mz.loc[cand.index] / max(mz_tol, 1e-12) +\n",
    "        d_mass.loc[cand.index] / max(mass_tol, 1e-12)\n",
    "    )\n",
    "    best_row = cand.sort_values(\"score\", kind=\"mergesort\").iloc[0]\n",
    "    return best_row.to_dict()\n",
    "\n",
    "\n",
    "# ---------- per-row collectors (single pass across m/z list) ----------\n",
    "def _collect_matches_for_row(\n",
    "    row: pd.Series,\n",
    "    df2: pd.DataFrame,\n",
    "    rt_window: float,\n",
    "    mz_tol: float,\n",
    "    mass_tol: float\n",
    ") -> Tuple[List[str], List[Optional[Any]], List[Optional[str]], List[Optional[float]]]:\n",
    "    \"\"\"\n",
    "    For a df1 row, iterate over matched_mz_list and collect:\n",
    "      - mz_tokens for best_match string (aligned, with placeholders)\n",
    "      - pfr_list  (aligned, None for missing/NaN)\n",
    "      - acc_list  (aligned, None for no match)\n",
    "      - mass_list (aligned, MASS from df2 if matched, else None)\n",
    "\n",
    "    Returns (best_match_tokens, pfr_list, acc_list, mass_list).\n",
    "    \"\"\"\n",
    "    neutral_mass   = row.get(\"neutral_mass\", np.nan)\n",
    "    retention_time = row.get(\"bin\", row.get(\"bin \", np.nan))\n",
    "    mz_list        = _safe_parse_list(row.get(\"matched_mz_list\", []))\n",
    "\n",
    "    if pd.isna(neutral_mass) or pd.isna(retention_time) or not mz_list:\n",
    "        return [], [], [], []\n",
    "\n",
    "    tokens: List[str] = []\n",
    "    pfrs:   List[Optional[Any]]   = []\n",
    "    accs:   List[Optional[str]]   = []\n",
    "    masses: List[Optional[float]] = []\n",
    "\n",
    "    for mz_value in mz_list:\n",
    "        res = search_best(\n",
    "            df2,\n",
    "            rt_query=float(retention_time),\n",
    "            mz_query=float(mz_value),\n",
    "            mass_query=float(neutral_mass),\n",
    "            rt_window=rt_window,\n",
    "            mz_tol=mz_tol,\n",
    "            mass_tol=mass_tol,\n",
    "        )\n",
    "        if res is not None:\n",
    "            uniprot_id = res.get(\"Accession\", \"NA\")\n",
    "            mass_match = _to_scalar(res.get(\"MASS\", neutral_mass))\n",
    "            pfr_val    = _to_scalar(res.get(\"PFR\", None))\n",
    "            if pfr_val is None or (isinstance(pfr_val, float) and np.isnan(pfr_val)):\n",
    "                tokens.append(f\"{mz_value}: {uniprot_id}, {mass_match}\")\n",
    "                pfrs.append(None)\n",
    "            else:\n",
    "                tokens.append(f\"{mz_value}: {uniprot_id}, {mass_match}, {pfr_val}\")\n",
    "                pfrs.append(pfr_val)\n",
    "            accs.append(uniprot_id)\n",
    "            masses.append(mass_match)\n",
    "        else:\n",
    "            tokens.append(f\"{mz_value}: NA\")\n",
    "            pfrs.append(None)\n",
    "            accs.append(None)\n",
    "            masses.append(None)\n",
    "\n",
    "    return tokens, pfrs, accs, masses\n",
    "\n",
    "\n",
    "def best_match_formatter_from_tokens(tokens: List[str]) -> Optional[str]:\n",
    "    if not tokens:\n",
    "        return None\n",
    "    return \"[\" + \", \".join(tokens) + \"]\"\n",
    "\n",
    "\n",
    "def matched_pfr_from_list(pfrs: List[Optional[Any]], keep_placeholders: bool) -> Optional[str]:\n",
    "    if not pfrs:\n",
    "        return None\n",
    "    if keep_placeholders:\n",
    "        return \"[\" + \", \".join(\"null\" if v is None else str(v) for v in pfrs) + \"]\"\n",
    "    else:\n",
    "        pruned = [str(v) for v in pfrs if v is not None]\n",
    "        return \"[\" + \", \".join(pruned) + \"]\" if pruned else None\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Main\n",
    "# ----------------------------\n",
    "def main():\n",
    "    # Load CSVs\n",
    "    if not os.path.exists(CHARGE_FILE_PATH):\n",
    "        raise FileNotFoundError(f\"Not found: {CHARGE_FILE_PATH}\")\n",
    "    if not os.path.exists(DATABANK_PATH):\n",
    "        raise FileNotFoundError(f\"Not found: {DATABANK_PATH}\")\n",
    "\n",
    "    df1 = pd.read_csv(CHARGE_FILE_PATH)\n",
    "    df2 = pd.read_csv(DATABANK_PATH)\n",
    "\n",
    "    # Normalize df1 column names to handle accidental trailing spaces, capitalization, etc.\n",
    "    df1.columns = [c.strip() for c in df1.columns]\n",
    "\n",
    "    # Ensure required columns in both tables (with tolerant check for 'bin' / 'bin ')\n",
    "    need_df1 = [\"neutral_mass\", \"matched_mz_list\"]\n",
    "    _ensure_columns(df1, need_df1)\n",
    "    if \"bin\" not in df1.columns and \"bin \" not in df1.columns:\n",
    "        raise KeyError(\"df1 must contain 'bin' (or 'bin ').\")\n",
    "\n",
    "    # Ensure essential df2 columns (PFR required for the new output)\n",
    "    _ensure_columns(df2, [\"rt_aligned\", \"precursor_mz\", \"MASS\", \"Accession\", \"PFR\"])\n",
    "\n",
    "    # If df1 had 'bin ' originally, create 'bin' as an alias to simplify downstream code\n",
    "    if \"bin\" not in df1.columns and \"bin \" in df1.columns:\n",
    "        df1[\"bin\"] = df1[\"bin \"]\n",
    "\n",
    "    # Pre-coerce df2 numerics once (for speed)\n",
    "    df2 = df2.copy()\n",
    "    df2[\"rt_aligned\"]   = _num(df2[\"rt_aligned\"])\n",
    "    df2[\"precursor_mz\"] = _num(df2[\"precursor_mz\"])\n",
    "    df2[\"MASS\"]         = _num(df2[\"MASS\"])\n",
    "    # PFR can be numeric or categorical; try to coerce but keep strings if not\n",
    "    try:\n",
    "        df2[\"PFR\"] = pd.to_numeric(df2[\"PFR\"], errors=\"ignore\")\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # Build output columns per tolerance triplet\n",
    "    for (rt_w, mz_t, mass_t) in PARAM_SETS:\n",
    "        suffix = f\"rt{_fmt_suffix(rt_w)}_mz{_fmt_suffix(mz_t)}_mass{_fmt_suffix(mass_t)}\"\n",
    "\n",
    "        match_col   = f\"best_match_{suffix}\"\n",
    "        pfr_col     = f\"matched_pfr_{suffix}\"\n",
    "        mode_pfr    = f\"mode_pfr_{suffix}\"\n",
    "        mode_pfr_n  = f\"mode_pfr_count_{suffix}\"\n",
    "        mode_acc    = f\"mode_accession_{suffix}\"\n",
    "\n",
    "        best_tokens_series: List[List[str]] = []\n",
    "        pfr_list_series:    List[List[Optional[Any]]] = []\n",
    "        acc_list_series:    List[List[Optional[str]]] = []\n",
    "\n",
    "        # Compute per-row tokens and lists in one pass\n",
    "        for _, row in df1.iterrows():\n",
    "            tokens, pfrs, accs, _masses = _collect_matches_for_row(\n",
    "                row, df2, rt_window=rt_w, mz_tol=mz_t, mass_tol=mass_t\n",
    "            )\n",
    "            best_tokens_series.append(tokens)\n",
    "            pfr_list_series.append(pfrs)\n",
    "            acc_list_series.append(accs)\n",
    "\n",
    "        # Populate columns\n",
    "        df1[match_col] = [best_match_formatter_from_tokens(toks) for toks in best_tokens_series]\n",
    "        df1[pfr_col]   = [matched_pfr_from_list(pfrs, keep_placeholders=PFR_KEEP_PLACEHOLDERS)\n",
    "                          for pfrs in pfr_list_series]\n",
    "\n",
    "        # Most common PFR + its frequency (with cutoff), and most common Accession (masked if cutoff not met)\n",
    "        mode_results = [\n",
    "            _mode_and_count_with_cutoff(pfrs, accs, min_count=MIN_MODE_PFR_COUNT)\n",
    "            for pfrs, accs in zip(pfr_list_series, acc_list_series)\n",
    "        ]\n",
    "        df1[mode_pfr]   = [mr[0] for mr in mode_results]\n",
    "        df1[mode_pfr_n] = [mr[1] for mr in mode_results]\n",
    "        df1[mode_acc]   = [mr[2] for mr in mode_results]\n",
    "\n",
    "    # Save single CSV containing all columns\n",
    "    out_dir = os.path.dirname(OUTPUT_PATH) or \".\"\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    df1.to_csv(OUTPUT_PATH, index=False)\n",
    "    print(\n",
    "        f\"Saved with {len(PARAM_SETS)} sets of columns \"\n",
    "        f\"(best_match / matched_pfr / mode_pfr / mode_pfr_count / mode_accession) ‚Üí {OUTPUT_PATH}\"\n",
    "    )\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "caabaa7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Done. Saved ‚Üí F:/binary/ids1.csv\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Match assignments (report.csv) to MS2 identification table (ms2_id.csv)\n",
    "by precursor_mz and neutral_mass ONLY (no retention time).\n",
    "\n",
    "Inputs:\n",
    "  df1 (report): must have 'neutral_mass' and 'matched_mz_list'\n",
    "  df2 (ms2_id): must have 'precursor_mz', 'MASS', 'Accession'\n",
    "\n",
    "Outputs per tolerance pair:\n",
    "  best_match_mz<MZ>_mass<MASS>\n",
    "  matched_acc_mz<MZ>_mass<MASS>\n",
    "  matched_mass_mz<MZ>_mass<MASS>\n",
    "  mode_accession_mz<MZ>_mass<MASS>\n",
    "  mode_mass_mz<MZ>_mass<MASS>\n",
    "  mode_accession_count_mz<MZ>_mass<MASS>\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import ast\n",
    "from typing import Optional, Dict, List, Tuple, Any\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ----------------------------\n",
    "# CONFIG\n",
    "# ----------------------------\n",
    "REPORT_PATH = r\"F:/binary/report1/report.csv\"                 # your report\n",
    "MS2_PATH    = r\"F:/binary/ms2_with_ids_hash_nocast.csv\"       # your ms2_id table\n",
    "OUTPUT_PATH = r\"F:/binary/ids1.csv\"\n",
    "\n",
    "# Provide one or more (mz_tol, mass_tol) pairs here\n",
    "PARAM_SETS: List[Tuple[float, float]] = [\n",
    "    (2.0, 90.0),\n",
    "    # (1.0, 50.0),\n",
    "]\n",
    "\n",
    "# ----------------------------\n",
    "# Helpers\n",
    "# ----------------------------\n",
    "def _num(s: pd.Series) -> pd.Series:\n",
    "    \"\"\"Coerce to numeric, invalid ‚Üí NaN.\"\"\"\n",
    "    return pd.to_numeric(s, errors=\"coerce\")\n",
    "\n",
    "def _safe_parse_list(val) -> List[float]:\n",
    "    \"\"\"Safely parse matched_mz_list into a list of floats.\"\"\"\n",
    "    if isinstance(val, str):\n",
    "        try:\n",
    "            parsed = ast.literal_eval(val)\n",
    "            if isinstance(parsed, (list, tuple, np.ndarray)):\n",
    "                return [float(x) for x in parsed]\n",
    "            return []\n",
    "        except Exception:\n",
    "            return []\n",
    "    if isinstance(val, (list, tuple, np.ndarray)):\n",
    "        try:\n",
    "            return [float(x) for x in val]\n",
    "        except Exception:\n",
    "            return []\n",
    "    return []\n",
    "\n",
    "def _fmt_suffix(v: float) -> str:\n",
    "    \"\"\"Make a tidy string for column suffixes (avoid many decimals).\"\"\"\n",
    "    if float(v).is_integer():\n",
    "        return str(int(v))\n",
    "    return str(v).replace(\".\", \"p\")\n",
    "\n",
    "def _stringify_non_null(seq) -> Optional[str]:\n",
    "    \"\"\"Return JSON-ish list string of non-null items, or None if empty.\"\"\"\n",
    "    out = []\n",
    "    for x in seq:\n",
    "        if x is None:\n",
    "            continue\n",
    "        if isinstance(x, float) and np.isnan(x):\n",
    "            continue\n",
    "        out.append(str(x))\n",
    "    return \"[\" + \", \".join(out) + \"]\" if out else None\n",
    "\n",
    "def _mode_and_count_with_mass(accs: List[Any], masses: List[Any]) -> Tuple[Optional[str], Optional[float], int]:\n",
    "    \"\"\"\n",
    "    Return (mode_accession, mode_mass, count) for the most common accession.\n",
    "    The mass reported is the most common mass among those matching the mode accession.\n",
    "    \"\"\"\n",
    "    clean_accs = []\n",
    "    for a in accs:\n",
    "        if a is None:\n",
    "            continue\n",
    "        if isinstance(a, float) and np.isnan(a):\n",
    "            continue\n",
    "        clean_accs.append(str(a))\n",
    "    if not clean_accs:\n",
    "        return None, None, 0\n",
    "\n",
    "    counter = Counter(clean_accs)\n",
    "    mode_acc, mode_cnt = counter.most_common(1)[0]\n",
    "\n",
    "    # Pick most common mass corresponding to that accession\n",
    "    linked_masses = [m for a, m in zip(accs, masses) if str(a) == mode_acc]\n",
    "    if linked_masses:\n",
    "        mass_mode, _ = Counter(linked_masses).most_common(1)[0]\n",
    "    else:\n",
    "        mass_mode = None\n",
    "\n",
    "    return mode_acc, mass_mode, int(mode_cnt)\n",
    "\n",
    "# ----------------------------\n",
    "# Matching logic (no RT)\n",
    "# ----------------------------\n",
    "def search_best(df2: pd.DataFrame, mz_query: float, mass_query: float,\n",
    "                mz_tol: float, mass_tol: float) -> Optional[Dict]:\n",
    "    d_mz   = (df2[\"precursor_mz\"] - mz_query).abs()\n",
    "    d_mass = (df2[\"MASS\"] - mass_query).abs()\n",
    "    mask = (d_mz <= mz_tol) & (d_mass <= mass_tol)\n",
    "    if not mask.any():\n",
    "        return None\n",
    "\n",
    "    cand = df2.loc[mask].copy()\n",
    "    cand[\"score\"] = d_mz.loc[cand.index] / max(mz_tol, 1e-12) + d_mass.loc[cand.index] / max(mass_tol, 1e-12)\n",
    "    return cand.sort_values(\"score\", kind=\"mergesort\").iloc[0].to_dict()\n",
    "\n",
    "# ----------------------------\n",
    "# Main\n",
    "# ----------------------------\n",
    "def main():\n",
    "    # --- load ---\n",
    "    if not os.path.exists(REPORT_PATH):\n",
    "        raise FileNotFoundError(f\"Not found: {REPORT_PATH}\")\n",
    "    if not os.path.exists(MS2_PATH):\n",
    "        raise FileNotFoundError(f\"Not found: {MS2_PATH}\")\n",
    "\n",
    "    df1 = pd.read_csv(REPORT_PATH)\n",
    "    df2 = pd.read_csv(MS2_PATH)\n",
    "\n",
    "    # --- ensure numeric & safe dtypes ---\n",
    "    df2[\"precursor_mz\"] = _num(df2[\"precursor_mz\"])\n",
    "    df2[\"MASS\"]         = _num(df2[\"MASS\"])\n",
    "    if \"Accession\" not in df2.columns:\n",
    "        raise KeyError(\"MS2 file must contain 'Accession'.\")\n",
    "    df2[\"Accession\"] = df2[\"Accession\"].astype(str)\n",
    "\n",
    "    # --- process ---\n",
    "    for (mz_t, mass_t) in PARAM_SETS:\n",
    "        suffix   = f\"mz{_fmt_suffix(mz_t)}_mass{_fmt_suffix(mass_t)}\"\n",
    "        best_col = f\"best_match_{suffix}\"\n",
    "        acc_col  = f\"matched_acc_{suffix}\"\n",
    "        mass_col = f\"matched_mass_{suffix}\"\n",
    "        mode_acc = f\"mode_accession_{suffix}\"\n",
    "        mode_mass = f\"mode_mass_{suffix}\"\n",
    "        mode_cnt = f\"mode_accession_count_{suffix}\"\n",
    "\n",
    "        best_list, acc_list, mass_list = [], [], []\n",
    "        mode_acc_list, mode_mass_list, mode_cnt_list = [], [], []\n",
    "\n",
    "        for _, row in df1.iterrows():\n",
    "            mz_list = _safe_parse_list(row.get(\"matched_mz_list\", []))\n",
    "            neutral_mass = row.get(\"neutral_mass\", np.nan)\n",
    "\n",
    "            if pd.isna(neutral_mass) or not mz_list:\n",
    "                best_list.append(None)\n",
    "                acc_list.append(None)\n",
    "                mass_list.append(None)\n",
    "                mode_acc_list.append(None)\n",
    "                mode_mass_list.append(None)\n",
    "                mode_cnt_list.append(0)\n",
    "                continue\n",
    "\n",
    "            matches, accs, masses = [], [], []\n",
    "            for mz_val in mz_list:\n",
    "                res = search_best(df2, float(mz_val), float(neutral_mass), mz_t, mass_t)\n",
    "                if res is not None:\n",
    "                    acc = res.get(\"Accession\", \"NA\")\n",
    "                    mass_match = res.get(\"MASS\", neutral_mass)\n",
    "                    matches.append(f\"{mz_val}: {acc}, {mass_match}\")\n",
    "                    accs.append(acc)\n",
    "                    masses.append(mass_match)\n",
    "                else:\n",
    "                    matches.append(f\"{mz_val}: NA\")\n",
    "                    accs.append(None)\n",
    "                    masses.append(None)\n",
    "\n",
    "            best_list.append(\"[\" + \", \".join(matches) + \"]\")\n",
    "            acc_list.append(_stringify_non_null(accs))\n",
    "            mass_list.append(_stringify_non_null(masses))\n",
    "\n",
    "            m_acc, m_mass, m_cnt = _mode_and_count_with_mass(accs, masses)\n",
    "            mode_acc_list.append(m_acc)\n",
    "            mode_mass_list.append(m_mass)\n",
    "            mode_cnt_list.append(m_cnt)\n",
    "\n",
    "        df1[best_col]  = best_list\n",
    "        df1[acc_col]   = acc_list\n",
    "        df1[mass_col]  = mass_list\n",
    "        df1[mode_acc]  = mode_acc_list\n",
    "        df1[mode_mass] = mode_mass_list\n",
    "        df1[mode_cnt]  = mode_cnt_list\n",
    "\n",
    "    # --- save ---\n",
    "    out_dir = os.path.dirname(OUTPUT_PATH) or \".\"\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    df1.to_csv(OUTPUT_PATH, index=False)\n",
    "    print(f\"‚úÖ Done. Saved ‚Üí {OUTPUT_PATH}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ec67574c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Done. Saved ‚Üí F:/binary/ids1.csv\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Match assignments (report.csv) to MS2 identification table (ms2_id.csv)\n",
    "by precursor_mz and neutral_mass ONLY (no retention time).\n",
    "\n",
    "Primary match (per m/z in matched_mz_list):\n",
    "  - |precursor_mz - m/z|  <= mz_tol\n",
    "  - |MASS - neutral_mass| <= mass_tol\n",
    "\n",
    "Fallback if primary fails:\n",
    "  - Restrict to rows where MS2 MASS is missing (NaN)\n",
    "  - Match ONLY by |precursor_mz - m/z| <= mz_tol\n",
    "  - Choose the closest m/z\n",
    "\n",
    "Outputs per tolerance pair:\n",
    "  best_match_mz<MZ>_mass<MASS>        : \"[<mz>: <Accession>, <MASS>] ...\"\n",
    "  matched_acc_mz<MZ>_mass<MASS>       : \"[<Accession> ...]\" (non-null only)\n",
    "  matched_mass_mz<MZ>_mass<MASS>      : \"[<MASS> ...]\" (non-null only)\n",
    "  mode_accession_mz<MZ>_mass<MASS>    : most common accession\n",
    "  mode_mass_mz<MZ>_mass<MASS>         : most common mass among matches of the mode accession\n",
    "  mode_accession_count_mz<MZ>_mass<MASS> : frequency of the mode accession\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import ast\n",
    "from typing import Optional, Dict, List, Tuple, Any\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ----------------------------\n",
    "# CONFIG\n",
    "# ----------------------------\n",
    "REPORT_PATH = r\"F:/binary/report1/report.csv\"                 # your report\n",
    "MS2_PATH    = r\"F:/binary/ms2_with_ids_hash_nocast.csv\"       # your ms2_id table\n",
    "OUTPUT_PATH = r\"F:/binary/ids1.csv\"\n",
    "\n",
    "# Provide one or more (mz_tol, mass_tol) pairs here\n",
    "PARAM_SETS: List[Tuple[float, float]] = [\n",
    "    (2.0, 90.0),\n",
    "    # (1.0, 50.0),\n",
    "]\n",
    "\n",
    "# ----------------------------\n",
    "# Helpers\n",
    "# ----------------------------\n",
    "def _num(s: pd.Series) -> pd.Series:\n",
    "    \"\"\"Coerce to numeric, invalid ‚Üí NaN.\"\"\"\n",
    "    return pd.to_numeric(s, errors=\"coerce\")\n",
    "\n",
    "def _safe_parse_list(val) -> List[float]:\n",
    "    \"\"\"Safely parse matched_mz_list into a list of floats.\"\"\"\n",
    "    if isinstance(val, str):\n",
    "        try:\n",
    "            parsed = ast.literal_eval(val)\n",
    "            if isinstance(parsed, (list, tuple, np.ndarray)):\n",
    "                return [float(x) for x in parsed]\n",
    "            return []\n",
    "        except Exception:\n",
    "            return []\n",
    "    if isinstance(val, (list, tuple, np.ndarray)):\n",
    "        try:\n",
    "            return [float(x) for x in val]\n",
    "        except Exception:\n",
    "            return []\n",
    "    return []\n",
    "\n",
    "def _fmt_suffix(v: float) -> str:\n",
    "    \"\"\"Make a tidy string for column suffixes (avoid many decimals).\"\"\"\n",
    "    if float(v).is_integer():\n",
    "        return str(int(v))\n",
    "    return str(v).replace(\".\", \"p\")\n",
    "\n",
    "def _stringify_non_null(seq) -> Optional[str]:\n",
    "    \"\"\"Return JSON-ish list string of non-null items, or None if empty.\"\"\"\n",
    "    out = []\n",
    "    for x in seq:\n",
    "        if x is None:\n",
    "            continue\n",
    "        if isinstance(x, float) and np.isnan(x):\n",
    "            continue\n",
    "        out.append(str(x))\n",
    "    return \"[\" + \", \".join(out) + \"]\" if out else None\n",
    "\n",
    "def _mode_and_count_with_mass(accs: List[Any], masses: List[Any]) -> Tuple[Optional[str], Optional[float], int]:\n",
    "    \"\"\"\n",
    "    Return (mode_accession, mode_mass, count) for the most common accession.\n",
    "    The mass reported is the most common mass among those matching the mode accession.\n",
    "    \"\"\"\n",
    "    clean_accs = []\n",
    "    for a in accs:\n",
    "        if a is None:\n",
    "            continue\n",
    "        if isinstance(a, float) and np.isnan(a):\n",
    "            continue\n",
    "        clean_accs.append(str(a))\n",
    "    if not clean_accs:\n",
    "        return None, None, 0\n",
    "\n",
    "    counter = Counter(clean_accs)\n",
    "    mode_acc, mode_cnt = counter.most_common(1)[0]\n",
    "\n",
    "    # Pick most common mass corresponding to that accession\n",
    "    linked_masses = [m for a, m in zip(accs, masses) if (a is not None and str(a) == mode_acc)]\n",
    "    if linked_masses:\n",
    "        mass_mode, _ = Counter(linked_masses).most_common(1)[0]\n",
    "    else:\n",
    "        mass_mode = None\n",
    "\n",
    "    return mode_acc, mass_mode, int(mode_cnt)\n",
    "\n",
    "def _fmt_token_val(x: Any) -> str:\n",
    "    \"\"\"Format values for token strings, using 'NA' for None/NaN.\"\"\"\n",
    "    if x is None:\n",
    "        return \"NA\"\n",
    "    if isinstance(x, float) and np.isnan(x):\n",
    "        return \"NA\"\n",
    "    return str(x)\n",
    "\n",
    "# ----------------------------\n",
    "# Matching logic\n",
    "# ----------------------------\n",
    "def search_best(df2: pd.DataFrame, mz_query: float, mass_query: float,\n",
    "                mz_tol: float, mass_tol: float) -> Optional[Dict]:\n",
    "    \"\"\"\n",
    "    Primary matcher: BOTH mz & mass within tolerance; choose smallest normalized deltas.\n",
    "    \"\"\"\n",
    "    d_mz   = (df2[\"precursor_mz\"] - mz_query).abs()\n",
    "    d_mass = (df2[\"MASS\"] - mass_query).abs()\n",
    "    mask = (d_mz <= mz_tol) & (d_mass <= mass_tol)\n",
    "    if not mask.any():\n",
    "        return None\n",
    "\n",
    "    cand = df2.loc[mask].copy()\n",
    "    cand[\"score\"] = d_mz.loc[cand.index] / max(mz_tol, 1e-12) + d_mass.loc[cand.index] / max(mass_tol, 1e-12)\n",
    "    return cand.sort_values(\"score\", kind=\"mergesort\").iloc[0].to_dict()\n",
    "\n",
    "def search_best_mz_only_missing_mass(df2: pd.DataFrame, mz_query: float, mz_tol: float) -> Optional[Dict]:\n",
    "    \"\"\"\n",
    "    Fallback matcher: restrict to rows where MASS is NaN and match ONLY by m/z within mz_tol.\n",
    "    Choose the closest m/z.\n",
    "    \"\"\"\n",
    "    is_missing_mass = df2[\"MASS\"].isna()\n",
    "    d_mz = (df2[\"precursor_mz\"] - mz_query).abs()\n",
    "    mask = is_missing_mass & (d_mz <= mz_tol)\n",
    "    if not mask.any():\n",
    "        return None\n",
    "    cand = df2.loc[mask].copy()\n",
    "    cand[\"score\"] = d_mz.loc[cand.index]  # smaller is better\n",
    "    return cand.sort_values(\"score\", kind=\"mergesort\").iloc[0].to_dict()\n",
    "\n",
    "# ----------------------------\n",
    "# Main\n",
    "# ----------------------------\n",
    "def main():\n",
    "    # --- load ---\n",
    "    if not os.path.exists(REPORT_PATH):\n",
    "        raise FileNotFoundError(f\"Not found: {REPORT_PATH}\")\n",
    "    if not os.path.exists(MS2_PATH):\n",
    "        raise FileNotFoundError(f\"Not found: {MS2_PATH}\")\n",
    "\n",
    "    df1 = pd.read_csv(REPORT_PATH)\n",
    "    df2 = pd.read_csv(MS2_PATH)\n",
    "\n",
    "    # --- ensure numeric & safe dtypes ---\n",
    "    df2[\"precursor_mz\"] = _num(df2[\"precursor_mz\"])\n",
    "    df2[\"MASS\"]         = _num(df2[\"MASS\"])\n",
    "    if \"Accession\" not in df2.columns:\n",
    "        raise KeyError(\"MS2 file must contain 'Accession'.\")\n",
    "    df2[\"Accession\"] = df2[\"Accession\"].astype(str)\n",
    "\n",
    "    # --- process ---\n",
    "    for (mz_t, mass_t) in PARAM_SETS:\n",
    "        suffix     = f\"mz{_fmt_suffix(mz_t)}_mass{_fmt_suffix(mass_t)}\"\n",
    "        best_col   = f\"best_match_{suffix}\"\n",
    "        acc_col    = f\"matched_acc_{suffix}\"\n",
    "        mass_col   = f\"matched_mass_{suffix}\"\n",
    "        mode_acc   = f\"mode_accession_{suffix}\"\n",
    "        mode_mass  = f\"mode_mass_{suffix}\"\n",
    "        mode_cnt   = f\"mode_accession_count_{suffix}\"\n",
    "\n",
    "        best_list, acc_list, mass_list = [], [], []\n",
    "        mode_acc_list, mode_mass_list, mode_cnt_list = [], [], []\n",
    "\n",
    "        for _, row in df1.iterrows():\n",
    "            mz_list = _safe_parse_list(row.get(\"matched_mz_list\", []))\n",
    "            neutral_mass = row.get(\"neutral_mass\", np.nan)\n",
    "\n",
    "            if pd.isna(neutral_mass) or not mz_list:\n",
    "                best_list.append(None)\n",
    "                acc_list.append(None)\n",
    "                mass_list.append(None)\n",
    "                mode_acc_list.append(None)\n",
    "                mode_mass_list.append(None)\n",
    "                mode_cnt_list.append(0)\n",
    "                continue\n",
    "\n",
    "            matches, accs, masses = [], [], []\n",
    "            for mz_val in mz_list:\n",
    "                # Primary: mz + mass\n",
    "                res = search_best(df2, float(mz_val), float(neutral_mass), mz_t, mass_t)\n",
    "\n",
    "                # Fallback: MASS missing rows, mz-only\n",
    "                if res is None:\n",
    "                    res = search_best_mz_only_missing_mass(df2, float(mz_val), mz_t)\n",
    "\n",
    "                if res is not None:\n",
    "                    acc = res.get(\"Accession\", \"NA\")\n",
    "                    mass_match = res.get(\"MASS\", None)  # may be NaN in fallback\n",
    "                    matches.append(f\"{mz_val}: { _fmt_token_val(acc) }, { _fmt_token_val(mass_match) }\")\n",
    "                    accs.append(acc)\n",
    "                    masses.append(mass_match)\n",
    "                else:\n",
    "                    matches.append(f\"{mz_val}: NA\")\n",
    "                    accs.append(None)\n",
    "                    masses.append(None)\n",
    "\n",
    "            best_list.append(\"[\" + \", \".join(matches) + \"]\")\n",
    "            acc_list.append(_stringify_non_null(accs))\n",
    "            mass_list.append(_stringify_non_null(masses))\n",
    "\n",
    "            m_acc, m_mass, m_cnt = _mode_and_count_with_mass(accs, masses)\n",
    "            mode_acc_list.append(m_acc)\n",
    "            mode_mass_list.append(m_mass)\n",
    "            mode_cnt_list.append(m_cnt)\n",
    "\n",
    "        df1[best_col]  = best_list\n",
    "        df1[acc_col]   = acc_list\n",
    "        df1[mass_col]  = mass_list\n",
    "        df1[mode_acc]  = mode_acc_list\n",
    "        df1[mode_mass] = mode_mass_list\n",
    "        df1[mode_cnt]  = mode_cnt_list\n",
    "\n",
    "    # --- save ---\n",
    "    out_dir = os.path.dirname(OUTPUT_PATH) or \".\"\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    df1.to_csv(OUTPUT_PATH, index=False)\n",
    "    print(f\"‚úÖ Done. Saved ‚Üí {OUTPUT_PATH}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vae",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
