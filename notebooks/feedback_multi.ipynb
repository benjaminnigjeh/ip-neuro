{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec172aec",
   "metadata": {},
   "source": [
    "A simple 2D classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d2a5732",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import os, re, json, gc, sys, ast\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Optional (not used in plotting)\n",
    "import seaborn as sns  # noqa: F401\n",
    "from scipy.signal import find_peaks  # noqa: F401\n",
    "from scipy.signal import savgol_filter  # noqa: F401\n",
    "from scipy.ndimage import gaussian_filter1d  # noqa: F401\n",
    "from scipy.linalg import svd  # noqa: F401\n",
    "from scipy import stats  # <-- NEW: t-test\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras import regularizers\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# ----------------------------\n",
    "# Repro & GPU memory growth\n",
    "# ----------------------------\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "try:\n",
    "    gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "    for _gpu in gpus:\n",
    "        tf.config.experimental.set_memory_growth(_gpu, True)\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "def hard_free():\n",
    "    \"\"\"Aggressively release memory after each bin.\"\"\"\n",
    "    try: plt.close('all')\n",
    "    except: pass\n",
    "    try: tf.keras.backend.clear_session()\n",
    "    except: pass\n",
    "    try: gc.collect(); gc.collect()\n",
    "    except: pass\n",
    "    try:\n",
    "        import ctypes, platform\n",
    "        if platform.system().lower() == \"linux\":\n",
    "            ctypes.CDLL(\"libc.so.6\").malloc_trim(0)\n",
    "    except: pass\n",
    "\n",
    "# ----------------------------\n",
    "# Grouped log-odds gradient\n",
    "# ----------------------------\n",
    "@tf.function(reduce_retracing=True)\n",
    "def _group_logodds_grad_for_model(x1, model, pos_ids, neg_ids, eps):\n",
    "    pos_ids = tf.constant(pos_ids, dtype=tf.int32)\n",
    "    neg_ids = tf.constant(neg_ids, dtype=tf.int32)\n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(x1)\n",
    "        p = model(x1, training=False)  # (1, C)\n",
    "        p_pos = tf.reduce_sum(tf.gather(p, pos_ids, axis=1), axis=1)  # (1,)\n",
    "        p_neg = tf.reduce_sum(tf.gather(p, neg_ids, axis=1), axis=1)  # (1,)\n",
    "        log_odds = tf.math.log(p_pos + eps) - tf.math.log(p_neg + eps)\n",
    "    g = tape.gradient(log_odds, x1)  # (1, D)\n",
    "    return tf.squeeze(g, axis=0)     # (D,)\n",
    "\n",
    "def compute_avg_group_logodds_gradient(X: np.ndarray, models: list, pos_ids=(2,3), neg_ids=(0,1), eps: float = 1e-8) -> np.ndarray:\n",
    "    X_t = tf.convert_to_tensor(X, dtype=tf.float32)\n",
    "    N = int(X_t.shape[0])\n",
    "    sample_grads = []\n",
    "    for i in range(N):\n",
    "        x_i = X_t[i:i+1]\n",
    "        grads_over_models = []\n",
    "        for m in models:\n",
    "            g = _group_logodds_grad_for_model(x_i, m, pos_ids, neg_ids, eps)\n",
    "            grads_over_models.append(g)\n",
    "        g_avg_models = tf.reduce_mean(tf.stack(grads_over_models, axis=0), axis=0)\n",
    "        sample_grads.append(g_avg_models)\n",
    "    avg_grad = tf.reduce_mean(tf.stack(sample_grads, axis=0), axis=0)\n",
    "    return avg_grad.numpy()\n",
    "\n",
    "# ----------------------------\n",
    "# Model\n",
    "# ----------------------------\n",
    "def build_model(input_dim: int, num_classes: int):\n",
    "    model = Sequential([\n",
    "        Dense(128, input_dim=input_dim, activation='relu', kernel_regularizer=regularizers.l1(0.01)),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(),\n",
    "                  loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# ----------------------------\n",
    "# Helpers (cosine + plotting)\n",
    "# ----------------------------\n",
    "def cosine_sim(a: np.ndarray, b: np.ndarray, eps: float = 1e-12) -> float:\n",
    "    a = np.asarray(a, dtype=float).ravel()\n",
    "    b = np.asarray(b, dtype=float).ravel()\n",
    "    n = min(a.size, b.size)\n",
    "    a = a[:n]; b = b[:n]\n",
    "    denom = (np.linalg.norm(a) * np.linalg.norm(b)) + eps\n",
    "    return float(np.dot(a, b) / denom)\n",
    "\n",
    "def mirror_plot(x, top_y, bottom_y, title, outfile):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(x, top_y, linewidth=1.0, label=\"Run A\")\n",
    "    plt.plot(x, -bottom_y, linewidth=1.0, label=\"Run B (mirrored)\")\n",
    "    plt.axhline(0.0, linewidth=0.8)\n",
    "    plt.xlabel(\"m/z (approx grid)\")\n",
    "    plt.ylabel(\"Gradient magnitude\")\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(outfile, dpi=200)\n",
    "    plt.close()\n",
    "\n",
    "# ----------------------------\n",
    "# NEW: Stats & plotting helpers for volcano + signed agreement\n",
    "# ----------------------------\n",
    "def ttest_and_log2fc(X: np.ndarray, y: np.ndarray, pos_ids, neg_ids, eps: float = 1e-12):\n",
    "    \"\"\"Welch's t-test (pos vs neg) per feature + log2FC (pos/neg).\"\"\"\n",
    "    pos_mask = np.isin(y, list(pos_ids))\n",
    "    neg_mask = np.isin(y, list(neg_ids))\n",
    "    Xp = X[pos_mask]; Xn = X[neg_mask]\n",
    "    # Handle edge cases gracefully\n",
    "    if Xp.shape[0] < 2 or Xn.shape[0] < 2:\n",
    "        # fall back to NaNs so plots show empty\n",
    "        pvals = np.full(X.shape[1], np.nan)\n",
    "        log2fc = np.full(X.shape[1], np.nan)\n",
    "        return pvals, log2fc\n",
    "\n",
    "    # Welch's t-test, axis=0 over features\n",
    "    tstat, pvals = stats.ttest_ind(Xp, Xn, axis=0, equal_var=False, nan_policy='omit')\n",
    "    mu_p = np.nanmean(Xp, axis=0)\n",
    "    mu_n = np.nanmean(Xn, axis=0)\n",
    "    log2fc = np.log2((mu_p + eps) / (mu_n + eps))\n",
    "    # Replace any nan/inf from degenerate features\n",
    "    pvals = np.where(np.isfinite(pvals), pvals, 1.0)\n",
    "    log2fc = np.where(np.isfinite(log2fc), log2fc, 0.0)\n",
    "    return pvals, log2fc\n",
    "\n",
    "def volcano_plot(log2fc: np.ndarray, pvals: np.ndarray,\n",
    "                 lfc_left: float, lfc_right: float, alpha: float,\n",
    "                 title: str, outfile: str):\n",
    "    xl = log2fc\n",
    "    yl = -np.log10(np.clip(pvals, 1e-300, 1.0))\n",
    "    sig_up   = (xl >= lfc_right) & (pvals <= alpha)\n",
    "    sig_down = (xl <= lfc_left)  & (pvals <= alpha)\n",
    "    other    = ~(sig_up | sig_down)\n",
    "\n",
    "    plt.figure(figsize=(10,6))\n",
    "    plt.scatter(xl[other], yl[other], s=15, alpha=0.6, color='lightgray')\n",
    "    plt.scatter(xl[sig_down], yl[sig_down], s=30, alpha=0.9, color='C0')  # blue\n",
    "    plt.scatter(xl[sig_up], yl[sig_up], s=30, alpha=0.9, color='C3')      # red\n",
    "    # thresholds\n",
    "    plt.axvline(lfc_left,  ls='--', lw=1, color='k')\n",
    "    plt.axvline(lfc_right, ls='--', lw=1, color='k')\n",
    "    plt.axhline(-np.log10(alpha), ls='--', lw=1, color='k')\n",
    "    plt.xlabel(\"log2 fold-change (pos / neg)\")\n",
    "    plt.ylabel(r\"$-\\log_{10}(p)$\")\n",
    "    plt.title(title)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(outfile, dpi=200)\n",
    "    plt.close()\n",
    "\n",
    "def signed_agreement_plot(log2fc: np.ndarray, pvals: np.ndarray, grad_mean: np.ndarray,\n",
    "                          lfc_left: float, lfc_right: float, alpha: float,\n",
    "                          title_prefix: str, outfile: str):\n",
    "    signed_stat = log2fc * (-np.log10(np.clip(pvals, 1e-300, 1.0)))\n",
    "    # Pearson r (ignore NaNs)\n",
    "    mask = np.isfinite(signed_stat) & np.isfinite(grad_mean)\n",
    "    if np.sum(mask) >= 2:\n",
    "        r = np.corrcoef(signed_stat[mask], grad_mean[mask])[0,1]\n",
    "    else:\n",
    "        r = np.nan\n",
    "\n",
    "    sig_up   = (log2fc >= lfc_right) & (pvals <= alpha)\n",
    "    sig_down = (log2fc <= lfc_left)  & (pvals <= alpha)\n",
    "    other    = ~(sig_up | sig_down)\n",
    "\n",
    "    plt.figure(figsize=(10,6))\n",
    "    plt.scatter(signed_stat[other], grad_mean[other], s=15, alpha=0.6, color='lightgray')\n",
    "    plt.scatter(signed_stat[sig_down], grad_mean[sig_down], s=30, alpha=0.9, color='C0')\n",
    "    plt.scatter(signed_stat[sig_up],   grad_mean[sig_up],   s=30, alpha=0.9, color='C3')\n",
    "    plt.axvline(0.0, ls='--', lw=1, color='k'); plt.axhline(0.0, ls='--', lw=1, color='k')\n",
    "    plt.xlabel(r\"log2FC × $-\\log_{10}(p)$  (signed)\")\n",
    "    plt.ylabel(\"Average log-odds gradient (RunA/B mean)\")\n",
    "    plt.title(f\"{title_prefix}  (Pearson r = {r:.3f})\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(outfile, dpi=200)\n",
    "    plt.close()\n",
    "    return float(r)\n",
    "\n",
    "# ----------------------------\n",
    "# Splitter (4 CSVs per combined CSV) with bin prefix\n",
    "# ----------------------------\n",
    "TARGET_COLS = [\"pos_runA\", \"pos_runB\", \"negabs_runA\", \"negabs_runB\"]\n",
    "MZ_COL = \"m/z\"\n",
    "\n",
    "def split_csv(input_path: str, out_dir: str, bin_value: int) -> list[str]:\n",
    "    \"\"\"Split one combined CSV into 4 CSVs, prefixing filenames with bin number.\"\"\"\n",
    "    df = pd.read_csv(input_path)\n",
    "    if MZ_COL not in df.columns:\n",
    "        print(f\"[SKIP] {input_path} (no '{MZ_COL}' column)\")\n",
    "        return []\n",
    "    available_targets = [c for c in TARGET_COLS if c in df.columns]\n",
    "    if not available_targets:\n",
    "        print(f\"[SKIP] {input_path} (none of {TARGET_COLS} found)\")\n",
    "        return []\n",
    "    base = os.path.splitext(os.path.basename(input_path))[0]\n",
    "    written = []\n",
    "    for col in available_targets:\n",
    "        out_df = df[[MZ_COL, col]].copy()\n",
    "        out_path = os.path.join(out_dir, f\"bin{bin_value}_{base}_{col}.csv\")\n",
    "        out_df.to_csv(out_path, index=False)\n",
    "        written.append(out_path)\n",
    "    return written\n",
    "\n",
    "def process_folder(folder_path: str, bin_value: int) -> list[str]:\n",
    "    \"\"\"Split all CSVs in a folder to 'result/' and return list of result paths.\"\"\"\n",
    "    if not os.path.isdir(folder_path):\n",
    "        print(f\"[WARN] Not a folder: {folder_path}\")\n",
    "        return []\n",
    "    out_dir = os.path.join(folder_path, \"result\")\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "    all_outputs = []\n",
    "    for fname in os.listdir(folder_path):\n",
    "        if fname.lower().endswith(\".csv\"):\n",
    "            fpath = os.path.join(folder_path, fname)\n",
    "            print(f\"Splitting {fpath} ...\")\n",
    "            outputs = split_csv(fpath, out_dir, bin_value)\n",
    "            all_outputs.extend(outputs)\n",
    "\n",
    "    print(f\"Split done. Wrote {len(all_outputs)} files to {out_dir}\")\n",
    "    return all_outputs\n",
    "\n",
    "# ----------------------------\n",
    "# Config (edit these)\n",
    "# ----------------------------\n",
    "CSV_PATH    = r\"F:/20251010/dataset_rt_batch.csv\"   # input dataset with 'bin' and 'target'\n",
    "EPOCHS      = 50\n",
    "BATCH_SIZE  = 32\n",
    "K_SPLITS    = 5\n",
    "N_REPEATS   = 2\n",
    "SEED_BASES  = [111, 777]                   # two independent runs\n",
    "OUT_ROOT    = r\"F:/20251018/\"              # everything goes directly under bin_<N>/\n",
    "BIN_WHITELIST = None  # e.g. [35, 75]\n",
    "\n",
    "# Volcano thresholds (edit to taste)\n",
    "ALPHA = 0.05\n",
    "LFC_LEFT, LFC_RIGHT = -0.5, 0.5\n",
    "\n",
    "# Fixed grouping list (no prompt, no AUTO)\n",
    "GROUPINGS = [((2,),(1,)), ((2,),(0,)), ((1,),(0,)), ((3,),(2,)), ((3,),(0,1,2)), ((2,1),(0,)), ((3,),(1,)),  ((3,),(0,))]\n",
    "\n",
    "# ----------------------------\n",
    "# KFold+Repeats trainer\n",
    "# ----------------------------\n",
    "def train_kfold_repeats(X: np.ndarray, Y: np.ndarray, seed_base: int):\n",
    "    kf = KFold(n_splits=K_SPLITS, shuffle=True, random_state=42)\n",
    "    all_models = []\n",
    "    num_classes = int(np.max(Y)) + 1\n",
    "    for fold, (tr, va) in enumerate(kf.split(X, Y), 1):\n",
    "        X_tr, y_tr = X[tr], Y[tr]\n",
    "        X_va, y_va = X[va], Y[va]\n",
    "        for r in range(N_REPEATS):\n",
    "            seed = seed_base * 1000 + fold * 100 + r\n",
    "            tf.keras.utils.set_random_seed(seed)\n",
    "            np.random.seed(seed)\n",
    "            m = build_model(X.shape[1], num_classes)\n",
    "            m.fit(X_tr, y_tr, epochs=EPOCHS, batch_size=BATCH_SIZE,\n",
    "                  validation_data=(X_va, y_va), verbose=0)\n",
    "            all_models.append(m)\n",
    "        print(f\"[Seed base {seed_base}] Fold {fold}/{K_SPLITS} trained {N_REPEATS} models (total: {len(all_models)})\")\n",
    "    return all_models\n",
    "\n",
    "# ----------------------------\n",
    "# MAIN (bin-root outputs + keep only split CSVs)\n",
    "# ----------------------------\n",
    "def main():\n",
    "    df = pd.read_csv(CSV_PATH)\n",
    "    # Discover bins\n",
    "    bins_found = sorted([b for b in df[\"bin\"].dropna().unique().tolist()])\n",
    "    if BIN_WHITELIST is not None:\n",
    "        bins_to_process = [b for b in bins_found if b in set(BIN_WHITELIST)]\n",
    "    else:\n",
    "        bins_to_process = bins_found\n",
    "    if len(bins_to_process) == 0:\n",
    "        raise ValueError(f\"No 'bin' values found in {CSV_PATH}\")\n",
    "\n",
    "    # Keep only classes 0..3 by default\n",
    "    df = df[df[\"target\"].astype(int).isin([0,1,2,3])].copy()\n",
    "    unique_labels = np.sort(df[\"target\"].astype(int).unique())\n",
    "    assert unique_labels[0] == 0 and np.array_equal(unique_labels, np.arange(unique_labels[-1] + 1)), \\\n",
    "        f\"Non-contiguous labels detected: {unique_labels}. Please remap to 0..C-1.\"\n",
    "    groupings = GROUPINGS\n",
    "    print(f\"\\nUsing fixed grouping(s): {groupings}\")\n",
    "\n",
    "    os.makedirs(OUT_ROOT, exist_ok=True)\n",
    "\n",
    "    for BIN_VALUE in bins_to_process:\n",
    "        print(f\"\\n================= BIN {BIN_VALUE} =================\")\n",
    "        models_A = models_B = None\n",
    "        X = Y = fdf = None\n",
    "\n",
    "        try:\n",
    "            fdf = df[df[\"bin\"] == BIN_VALUE].copy()\n",
    "            if fdf.empty:\n",
    "                print(f\"[WARN] No rows for bin {BIN_VALUE}; skipping.\")\n",
    "                continue\n",
    "\n",
    "            # Normalize all features except ['bin','target'] within this bin\n",
    "            cols_to_norm = fdf.columns.difference(['bin', 'target'])\n",
    "            fdf[cols_to_norm] = fdf[cols_to_norm].apply(lambda x: x / (x.max() + 1.0))\n",
    "\n",
    "            Y = fdf[\"target\"].astype(int).to_numpy()\n",
    "            feature_cols = fdf.columns.difference(['bin', 'target'])\n",
    "            X = np.nan_to_num(fdf[feature_cols].to_numpy(), copy=False).astype(np.float32)\n",
    "\n",
    "            if X.shape[0] < 2 or X.shape[1] < 1:\n",
    "                print(f\"[WARN] Insufficient data for bin {BIN_VALUE} (samples={X.shape[0]}, dim={X.shape[1]}). Skipping.\")\n",
    "                continue\n",
    "\n",
    "            print(f\"Bin {BIN_VALUE}: samples={X.shape[0]}, dim={X.shape[1]}  class_counts=\"\n",
    "                  f\"{dict(zip(*np.unique(Y, return_counts=True)))}\")\n",
    "\n",
    "            # Output dirs per bin (everything under bin folder)\n",
    "            OUT_DIR   = os.path.join(OUT_ROOT, f\"bin_{str(BIN_VALUE).replace('.', '_')}\")\n",
    "            CSV_DIR   = os.path.join(OUT_DIR, \"csv\")\n",
    "            PLOTS_DIR = os.path.join(OUT_DIR, \"plots\")\n",
    "            os.makedirs(OUT_DIR, exist_ok=True)\n",
    "            os.makedirs(CSV_DIR, exist_ok=True)\n",
    "            os.makedirs(PLOTS_DIR, exist_ok=True)\n",
    "\n",
    "            # Train ensembles ONCE per run (A, B)\n",
    "            models_A = train_kfold_repeats(X, Y, seed_base=SEED_BASES[0])\n",
    "            models_B = train_kfold_repeats(X, Y, seed_base=SEED_BASES[1])\n",
    "\n",
    "            # grid helper (for your mirror plots)\n",
    "            def _make_grid(n):\n",
    "                n_grid = min(10000, n)\n",
    "                x_grid = np.arange(600, 600 + 0.1 * n_grid, 0.1)[:n_grid]\n",
    "                return n_grid, x_grid\n",
    "\n",
    "            def save_compare_only(pos_ids, neg_ids):\n",
    "                # ---- Gradients (Run A & B)\n",
    "                grad_A = compute_avg_group_logodds_gradient(X, models_A, pos_ids=pos_ids, neg_ids=neg_ids, eps=1e-8)\n",
    "                grad_B = compute_avg_group_logodds_gradient(X, models_B, pos_ids=pos_ids, neg_ids=neg_ids, eps=1e-8)\n",
    "                grad_mean = (grad_A + grad_B) / 2.0\n",
    "\n",
    "                pos_tag = \"_\".join(map(str, pos_ids))\n",
    "                neg_tag = \"_\".join(map(str, neg_ids))\n",
    "                tag = f\"pos_{pos_tag}__neg_{neg_tag}\"\n",
    "\n",
    "                # ---- Mirror plots (unchanged from your flow)\n",
    "                n_grid, x_grid = _make_grid(min(grad_A.size, grad_B.size))\n",
    "                yA = grad_A[:n_grid]; yB = grad_B[:n_grid]\n",
    "                yA_pos = np.where(yA > 0, yA, 0.0)\n",
    "                yB_pos = np.where(yB > 0, yB, 0.0)\n",
    "                yA_neg = np.where(yA < 0, -yA, 0.0)  # abs\n",
    "                yB_neg = np.where(yB < 0, -yB, 0.0)\n",
    "\n",
    "                cos_pos = cosine_sim(yA_pos, yB_pos)\n",
    "                cos_neg = cosine_sim(yA_neg, yB_neg)\n",
    "\n",
    "                comb_csv = os.path.join(CSV_DIR, f\"grads_AB__{tag}.csv\")\n",
    "                pd.DataFrame({\n",
    "                    \"m/z\": x_grid,\n",
    "                    \"grad_runA\": yA,\n",
    "                    \"grad_runB\": yB,\n",
    "                    \"pos_runA\": yA_pos,\n",
    "                    \"pos_runB\": yB_pos,\n",
    "                    \"negabs_runA\": yA_neg,\n",
    "                    \"negabs_runB\": yB_neg,\n",
    "                }).to_csv(comb_csv, index=False)\n",
    "\n",
    "                pos_title = (f\"Bin {BIN_VALUE} — Mirror Positive Gradients \"\n",
    "                             f\"[{tag}] (cos={cos_pos:.4f})\")\n",
    "                neg_title = (f\"Bin {BIN_VALUE} — Mirror Negative Gradients |abs| \"\n",
    "                             f\"[{tag}] (cos={cos_neg:.4f})\")\n",
    "\n",
    "                mirror_plot(\n",
    "                    x_grid, yA_pos, yB_pos,\n",
    "                    title=pos_title,\n",
    "                    outfile=os.path.join(PLOTS_DIR, f\"{tag}__mirror_pos.png\")\n",
    "                )\n",
    "                mirror_plot(\n",
    "                    x_grid, yA_neg, yB_neg,\n",
    "                    title=neg_title,\n",
    "                    outfile=os.path.join(PLOTS_DIR, f\"{tag}__mirror_negabs.png\")\n",
    "                )\n",
    "\n",
    "                # ---- NEW: Stats for volcano + ML agreement\n",
    "                pvals, log2fc = ttest_and_log2fc(X, Y, pos_ids, neg_ids, eps=1e-12)\n",
    "\n",
    "                # Save per-feature stats + raw grads + mean grad\n",
    "                stats_csv = os.path.join(CSV_DIR, f\"stats_and_grads__{tag}.csv\")\n",
    "                pd.DataFrame({\n",
    "                    \"feature\": list(feature_cols),\n",
    "                    \"log2FC\": log2fc,\n",
    "                    \"pval\": pvals,\n",
    "                    \"neglog10p\": -np.log10(np.clip(pvals, 1e-300, 1.0)),\n",
    "                    \"grad_runA_full\": grad_A,\n",
    "                    \"grad_runB_full\": grad_B,\n",
    "                    \"grad_mean\": grad_mean,\n",
    "                    \"signed_stat\": log2fc * (-np.log10(np.clip(pvals, 1e-300, 1.0))),\n",
    "                }).to_csv(stats_csv, index=False)\n",
    "\n",
    "                volcano_plot(\n",
    "                    log2fc, pvals, LFC_LEFT, LFC_RIGHT, ALPHA,\n",
    "                    title=f\"Bin {BIN_VALUE} — Volcano ({tag})\",\n",
    "                    outfile=os.path.join(PLOTS_DIR, f\"{tag}__volcano.png\")\n",
    "                )\n",
    "                r = signed_agreement_plot(\n",
    "                    log2fc, pvals, grad_mean,\n",
    "                    LFC_LEFT, LFC_RIGHT, ALPHA,\n",
    "                    title_prefix=f\"Bin {BIN_VALUE} — Signed agreement ({tag})\",\n",
    "                    outfile=os.path.join(PLOTS_DIR, f\"{tag}__signed_agreement.png\")\n",
    "                )\n",
    "\n",
    "                # summary JSON directly under bin/\n",
    "                with open(os.path.join(OUT_DIR, f\"summary__{tag}.json\"), \"w\") as fC:\n",
    "                    json.dump({\n",
    "                        \"bin\": BIN_VALUE,\n",
    "                        \"grouping\": {\"pos_ids\": list(pos_ids), \"neg_ids\": list(neg_ids)},\n",
    "                        \"comparison\": \"Run A vs Run B\",\n",
    "                        \"cosine_pos\": cos_pos,\n",
    "                        \"cosine_neg_abs\": cos_neg,\n",
    "                        \"pearson_signed_agreement\": r,\n",
    "                        \"thresholds\": {\"alpha\": ALPHA, \"lfc_left\": LFC_LEFT, \"lfc_right\": LFC_RIGHT},\n",
    "                        \"paths\": {\n",
    "                            \"combined_csv_for_mirror\": comb_csv,\n",
    "                            \"stats_and_grads_csv\": stats_csv,\n",
    "                            \"plots_dir\": PLOTS_DIR,\n",
    "                        }\n",
    "                    }, fC, indent=2)\n",
    "\n",
    "                print(f\"  [COMPARE] {tag} | Cos(pos)={cos_pos:.6f} Cos(neg|abs|)={cos_neg:.6f}  r={r:.3f}\")\n",
    "\n",
    "            # run all groupings -> write combined CSVs/plots/JSONs\n",
    "            num_classes = int(np.max(Y)) + 1\n",
    "            for (pos_ids, neg_ids) in groupings:\n",
    "                for idx in (*pos_ids, *neg_ids):\n",
    "                    assert 0 <= idx < num_classes, f\"Class index {idx} out of range 0..{num_classes-1}\"\n",
    "                save_compare_only(pos_ids, neg_ids)\n",
    "\n",
    "            # --- Split step: create split CSVs into csv/result/ ---\n",
    "            split_outputs = process_folder(CSV_DIR, int(BIN_VALUE))\n",
    "\n",
    "            # Move split files from csv/result/ -> csv/ and delete originals\n",
    "            result_dir = os.path.join(CSV_DIR, \"result\")\n",
    "            moved = []\n",
    "            if os.path.isdir(result_dir):\n",
    "                for fname in os.listdir(result_dir):\n",
    "                    src = os.path.join(result_dir, fname)\n",
    "                    dst = os.path.join(CSV_DIR, fname)\n",
    "                    os.replace(src, dst)\n",
    "                    moved.append(dst)\n",
    "                try: os.rmdir(result_dir)\n",
    "                except OSError: pass\n",
    "\n",
    "            # Delete original combined CSVs (keep only split)\n",
    "            for fname in os.listdir(CSV_DIR):\n",
    "                if fname.lower().endswith(\".csv\") and fname.startswith(\"grads_AB__\"):\n",
    "                    try:\n",
    "                        os.remove(os.path.join(CSV_DIR, fname))\n",
    "                    except Exception as e:\n",
    "                        print(f\"[WARN] Could not remove {fname}: {e}\")\n",
    "\n",
    "            # Save manifest of final CSVs\n",
    "            with open(os.path.join(OUT_DIR, \"split_manifest.json\"), \"w\") as fman:\n",
    "                json.dump({\n",
    "                    \"bin\": BIN_VALUE,\n",
    "                    \"final_csvs\": sorted([os.path.basename(p) for p in moved]),\n",
    "                }, fman, indent=2)\n",
    "\n",
    "        finally:\n",
    "            try:\n",
    "                if models_A is not None:\n",
    "                    for _m in models_A: del _m\n",
    "                del models_A\n",
    "            except: pass\n",
    "            try:\n",
    "                if models_B is not None:\n",
    "                    for _m in models_B: del _m\n",
    "                del models_B\n",
    "            except: pass\n",
    "            for v in [\"X\",\"Y\",\"fdf\"]:\n",
    "                try: del globals()[v]\n",
    "                except: pass\n",
    "            hard_free()\n",
    "\n",
    "    print(\"\\nAll bins processed. (bin-root outputs; csv keeps only split files)\\n\")\n",
    "\n",
    "# ---- run ----\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eacf26eb",
   "metadata": {},
   "source": [
    "Actual samples but the volcano graph has TIC normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10054889",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Using fixed grouping(s): [((2,), (1,)), ((2,), (0,)), ((1,), (0,)), ((3,), (2,)), ((3,), (0, 1, 2)), ((2, 1), (0,)), ((3,), (1,)), ((3,), (0,))]\n",
      "\n",
      "================= BIN 5 =================\n",
      "Bin 5: samples=118, dim=13692  class_counts={0: 33, 1: 30, 2: 26, 3: 29}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\benja\\miniconda3\\envs\\vae\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Seed base 111] Fold 1/5 trained 5 models (total: 5)\n",
      "[Seed base 111] Fold 2/5 trained 5 models (total: 10)\n",
      "[Seed base 111] Fold 3/5 trained 5 models (total: 15)\n",
      "[Seed base 111] Fold 4/5 trained 5 models (total: 20)\n",
      "[Seed base 111] Fold 5/5 trained 5 models (total: 25)\n",
      "[Seed base 777] Fold 1/5 trained 5 models (total: 5)\n",
      "[Seed base 777] Fold 2/5 trained 5 models (total: 10)\n",
      "[Seed base 777] Fold 3/5 trained 5 models (total: 15)\n",
      "[Seed base 777] Fold 4/5 trained 5 models (total: 20)\n",
      "[Seed base 777] Fold 5/5 trained 5 models (total: 25)\n",
      "WARNING:tensorflow:5 out of the last 5 calls to <function _group_logodds_grad_for_model at 0x0000029AF775DF80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 6 calls to <function _group_logodds_grad_for_model at 0x0000029AF775DF80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "  [COMPARE] pos_2__neg_1 | Cos(pos)=0.987876 Cos(neg|abs|)=0.986822  r=0.547\n",
      "  [COMPARE] pos_2__neg_0 | Cos(pos)=0.986543 Cos(neg|abs|)=0.985502  r=0.519\n",
      "  [COMPARE] pos_1__neg_0 | Cos(pos)=0.888078 Cos(neg|abs|)=0.928564  r=0.416\n",
      "  [COMPARE] pos_3__neg_2 | Cos(pos)=0.950173 Cos(neg|abs|)=0.972997  r=0.491\n",
      "  [COMPARE] pos_3__neg_0_1_2 | Cos(pos)=0.969280 Cos(neg|abs|)=0.980436  r=0.430\n",
      "  [COMPARE] pos_2_1__neg_0 | Cos(pos)=0.977046 Cos(neg|abs|)=0.979960  r=0.420\n",
      "  [COMPARE] pos_3__neg_1 | Cos(pos)=0.975318 Cos(neg|abs|)=0.989020  r=0.469\n",
      "  [COMPARE] pos_3__neg_0 | Cos(pos)=0.978009 Cos(neg|abs|)=0.991342  r=0.490\n",
      "Splitting F:/20251018d/bin_5\\csv\\grads_AB__pos_2__neg_1.csv ...\n",
      "Splitting F:/20251018d/bin_5\\csv\\stats_and_grads__pos_2__neg_1.csv ...\n",
      "[SKIP] F:/20251018d/bin_5\\csv\\stats_and_grads__pos_2__neg_1.csv (no 'm/z' column)\n",
      "Splitting F:/20251018d/bin_5\\csv\\grads_AB__pos_2__neg_0.csv ...\n",
      "Splitting F:/20251018d/bin_5\\csv\\stats_and_grads__pos_2__neg_0.csv ...\n",
      "[SKIP] F:/20251018d/bin_5\\csv\\stats_and_grads__pos_2__neg_0.csv (no 'm/z' column)\n",
      "Splitting F:/20251018d/bin_5\\csv\\grads_AB__pos_1__neg_0.csv ...\n",
      "Splitting F:/20251018d/bin_5\\csv\\stats_and_grads__pos_1__neg_0.csv ...\n",
      "[SKIP] F:/20251018d/bin_5\\csv\\stats_and_grads__pos_1__neg_0.csv (no 'm/z' column)\n",
      "Splitting F:/20251018d/bin_5\\csv\\grads_AB__pos_3__neg_2.csv ...\n",
      "Splitting F:/20251018d/bin_5\\csv\\stats_and_grads__pos_3__neg_2.csv ...\n",
      "[SKIP] F:/20251018d/bin_5\\csv\\stats_and_grads__pos_3__neg_2.csv (no 'm/z' column)\n",
      "Splitting F:/20251018d/bin_5\\csv\\grads_AB__pos_3__neg_0_1_2.csv ...\n",
      "Splitting F:/20251018d/bin_5\\csv\\stats_and_grads__pos_3__neg_0_1_2.csv ...\n",
      "[SKIP] F:/20251018d/bin_5\\csv\\stats_and_grads__pos_3__neg_0_1_2.csv (no 'm/z' column)\n",
      "Splitting F:/20251018d/bin_5\\csv\\grads_AB__pos_2_1__neg_0.csv ...\n",
      "Splitting F:/20251018d/bin_5\\csv\\stats_and_grads__pos_2_1__neg_0.csv ...\n",
      "[SKIP] F:/20251018d/bin_5\\csv\\stats_and_grads__pos_2_1__neg_0.csv (no 'm/z' column)\n",
      "Splitting F:/20251018d/bin_5\\csv\\grads_AB__pos_3__neg_1.csv ...\n",
      "Splitting F:/20251018d/bin_5\\csv\\stats_and_grads__pos_3__neg_1.csv ...\n",
      "[SKIP] F:/20251018d/bin_5\\csv\\stats_and_grads__pos_3__neg_1.csv (no 'm/z' column)\n",
      "Splitting F:/20251018d/bin_5\\csv\\grads_AB__pos_3__neg_0.csv ...\n",
      "Splitting F:/20251018d/bin_5\\csv\\stats_and_grads__pos_3__neg_0.csv ...\n",
      "[SKIP] F:/20251018d/bin_5\\csv\\stats_and_grads__pos_3__neg_0.csv (no 'm/z' column)\n",
      "Split done. Wrote 32 files to F:/20251018d/bin_5\\csv\\result\n",
      "\n",
      "================= BIN 15 =================\n",
      "Bin 15: samples=118, dim=13692  class_counts={0: 33, 1: 30, 2: 26, 3: 29}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\benja\\miniconda3\\envs\\vae\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Seed base 111] Fold 1/5 trained 5 models (total: 5)\n",
      "[Seed base 111] Fold 2/5 trained 5 models (total: 10)\n",
      "[Seed base 111] Fold 3/5 trained 5 models (total: 15)\n",
      "[Seed base 111] Fold 4/5 trained 5 models (total: 20)\n",
      "[Seed base 111] Fold 5/5 trained 5 models (total: 25)\n",
      "[Seed base 777] Fold 1/5 trained 5 models (total: 5)\n",
      "[Seed base 777] Fold 2/5 trained 5 models (total: 10)\n",
      "[Seed base 777] Fold 3/5 trained 5 models (total: 15)\n",
      "[Seed base 777] Fold 4/5 trained 5 models (total: 20)\n",
      "[Seed base 777] Fold 5/5 trained 5 models (total: 25)\n",
      "  [COMPARE] pos_2__neg_1 | Cos(pos)=0.989463 Cos(neg|abs|)=0.979476  r=0.599\n",
      "  [COMPARE] pos_2__neg_0 | Cos(pos)=0.985870 Cos(neg|abs|)=0.980388  r=0.549\n",
      "  [COMPARE] pos_1__neg_0 | Cos(pos)=0.914871 Cos(neg|abs|)=0.921428  r=0.421\n",
      "  [COMPARE] pos_3__neg_2 | Cos(pos)=0.983107 Cos(neg|abs|)=0.973670  r=0.479\n",
      "  [COMPARE] pos_3__neg_0_1_2 | Cos(pos)=0.992625 Cos(neg|abs|)=0.973871  r=0.401\n",
      "  [COMPARE] pos_2_1__neg_0 | Cos(pos)=0.965530 Cos(neg|abs|)=0.971910  r=0.464\n",
      "  [COMPARE] pos_3__neg_1 | Cos(pos)=0.975030 Cos(neg|abs|)=0.976221  r=0.467\n",
      "  [COMPARE] pos_3__neg_0 | Cos(pos)=0.977431 Cos(neg|abs|)=0.978720  r=0.487\n",
      "Splitting F:/20251018d/bin_15\\csv\\grads_AB__pos_2__neg_1.csv ...\n",
      "Splitting F:/20251018d/bin_15\\csv\\stats_and_grads__pos_2__neg_1.csv ...\n",
      "[SKIP] F:/20251018d/bin_15\\csv\\stats_and_grads__pos_2__neg_1.csv (no 'm/z' column)\n",
      "Splitting F:/20251018d/bin_15\\csv\\grads_AB__pos_2__neg_0.csv ...\n",
      "Splitting F:/20251018d/bin_15\\csv\\stats_and_grads__pos_2__neg_0.csv ...\n",
      "[SKIP] F:/20251018d/bin_15\\csv\\stats_and_grads__pos_2__neg_0.csv (no 'm/z' column)\n",
      "Splitting F:/20251018d/bin_15\\csv\\grads_AB__pos_1__neg_0.csv ...\n",
      "Splitting F:/20251018d/bin_15\\csv\\stats_and_grads__pos_1__neg_0.csv ...\n",
      "[SKIP] F:/20251018d/bin_15\\csv\\stats_and_grads__pos_1__neg_0.csv (no 'm/z' column)\n",
      "Splitting F:/20251018d/bin_15\\csv\\grads_AB__pos_3__neg_2.csv ...\n",
      "Splitting F:/20251018d/bin_15\\csv\\stats_and_grads__pos_3__neg_2.csv ...\n",
      "[SKIP] F:/20251018d/bin_15\\csv\\stats_and_grads__pos_3__neg_2.csv (no 'm/z' column)\n",
      "Splitting F:/20251018d/bin_15\\csv\\grads_AB__pos_3__neg_0_1_2.csv ...\n",
      "Splitting F:/20251018d/bin_15\\csv\\stats_and_grads__pos_3__neg_0_1_2.csv ...\n",
      "[SKIP] F:/20251018d/bin_15\\csv\\stats_and_grads__pos_3__neg_0_1_2.csv (no 'm/z' column)\n",
      "Splitting F:/20251018d/bin_15\\csv\\grads_AB__pos_2_1__neg_0.csv ...\n",
      "Splitting F:/20251018d/bin_15\\csv\\stats_and_grads__pos_2_1__neg_0.csv ...\n",
      "[SKIP] F:/20251018d/bin_15\\csv\\stats_and_grads__pos_2_1__neg_0.csv (no 'm/z' column)\n",
      "Splitting F:/20251018d/bin_15\\csv\\grads_AB__pos_3__neg_1.csv ...\n",
      "Splitting F:/20251018d/bin_15\\csv\\stats_and_grads__pos_3__neg_1.csv ...\n",
      "[SKIP] F:/20251018d/bin_15\\csv\\stats_and_grads__pos_3__neg_1.csv (no 'm/z' column)\n",
      "Splitting F:/20251018d/bin_15\\csv\\grads_AB__pos_3__neg_0.csv ...\n",
      "Splitting F:/20251018d/bin_15\\csv\\stats_and_grads__pos_3__neg_0.csv ...\n",
      "[SKIP] F:/20251018d/bin_15\\csv\\stats_and_grads__pos_3__neg_0.csv (no 'm/z' column)\n",
      "Split done. Wrote 32 files to F:/20251018d/bin_15\\csv\\result\n",
      "\n",
      "================= BIN 25 =================\n",
      "Bin 25: samples=118, dim=13692  class_counts={0: 33, 1: 30, 2: 26, 3: 29}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\benja\\miniconda3\\envs\\vae\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Seed base 111] Fold 1/5 trained 5 models (total: 5)\n",
      "[Seed base 111] Fold 2/5 trained 5 models (total: 10)\n",
      "[Seed base 111] Fold 3/5 trained 5 models (total: 15)\n",
      "[Seed base 111] Fold 4/5 trained 5 models (total: 20)\n",
      "[Seed base 111] Fold 5/5 trained 5 models (total: 25)\n",
      "[Seed base 777] Fold 1/5 trained 5 models (total: 5)\n",
      "[Seed base 777] Fold 2/5 trained 5 models (total: 10)\n",
      "[Seed base 777] Fold 3/5 trained 5 models (total: 15)\n",
      "[Seed base 777] Fold 4/5 trained 5 models (total: 20)\n",
      "[Seed base 777] Fold 5/5 trained 5 models (total: 25)\n",
      "  [COMPARE] pos_2__neg_1 | Cos(pos)=0.907976 Cos(neg|abs|)=0.480742  r=0.380\n",
      "  [COMPARE] pos_2__neg_0 | Cos(pos)=0.915487 Cos(neg|abs|)=0.923356  r=0.274\n",
      "  [COMPARE] pos_1__neg_0 | Cos(pos)=0.864559 Cos(neg|abs|)=0.878352  r=0.349\n",
      "  [COMPARE] pos_3__neg_2 | Cos(pos)=0.830906 Cos(neg|abs|)=0.945099  r=0.291\n",
      "  [COMPARE] pos_3__neg_0_1_2 | Cos(pos)=0.886466 Cos(neg|abs|)=0.955457  r=0.243\n",
      "  [COMPARE] pos_2_1__neg_0 | Cos(pos)=0.908335 Cos(neg|abs|)=0.936805  r=0.216\n",
      "  [COMPARE] pos_3__neg_1 | Cos(pos)=0.875844 Cos(neg|abs|)=0.958888  r=0.293\n",
      "  [COMPARE] pos_3__neg_0 | Cos(pos)=0.889420 Cos(neg|abs|)=0.941842  r=0.303\n",
      "Splitting F:/20251018d/bin_25\\csv\\grads_AB__pos_2__neg_1.csv ...\n",
      "Splitting F:/20251018d/bin_25\\csv\\stats_and_grads__pos_2__neg_1.csv ...\n",
      "[SKIP] F:/20251018d/bin_25\\csv\\stats_and_grads__pos_2__neg_1.csv (no 'm/z' column)\n",
      "Splitting F:/20251018d/bin_25\\csv\\grads_AB__pos_2__neg_0.csv ...\n",
      "Splitting F:/20251018d/bin_25\\csv\\stats_and_grads__pos_2__neg_0.csv ...\n",
      "[SKIP] F:/20251018d/bin_25\\csv\\stats_and_grads__pos_2__neg_0.csv (no 'm/z' column)\n",
      "Splitting F:/20251018d/bin_25\\csv\\grads_AB__pos_1__neg_0.csv ...\n",
      "Splitting F:/20251018d/bin_25\\csv\\stats_and_grads__pos_1__neg_0.csv ...\n",
      "[SKIP] F:/20251018d/bin_25\\csv\\stats_and_grads__pos_1__neg_0.csv (no 'm/z' column)\n",
      "Splitting F:/20251018d/bin_25\\csv\\grads_AB__pos_3__neg_2.csv ...\n",
      "Splitting F:/20251018d/bin_25\\csv\\stats_and_grads__pos_3__neg_2.csv ...\n",
      "[SKIP] F:/20251018d/bin_25\\csv\\stats_and_grads__pos_3__neg_2.csv (no 'm/z' column)\n",
      "Splitting F:/20251018d/bin_25\\csv\\grads_AB__pos_3__neg_0_1_2.csv ...\n",
      "Splitting F:/20251018d/bin_25\\csv\\stats_and_grads__pos_3__neg_0_1_2.csv ...\n",
      "[SKIP] F:/20251018d/bin_25\\csv\\stats_and_grads__pos_3__neg_0_1_2.csv (no 'm/z' column)\n",
      "Splitting F:/20251018d/bin_25\\csv\\grads_AB__pos_2_1__neg_0.csv ...\n",
      "Splitting F:/20251018d/bin_25\\csv\\stats_and_grads__pos_2_1__neg_0.csv ...\n",
      "[SKIP] F:/20251018d/bin_25\\csv\\stats_and_grads__pos_2_1__neg_0.csv (no 'm/z' column)\n",
      "Splitting F:/20251018d/bin_25\\csv\\grads_AB__pos_3__neg_1.csv ...\n",
      "Splitting F:/20251018d/bin_25\\csv\\stats_and_grads__pos_3__neg_1.csv ...\n",
      "[SKIP] F:/20251018d/bin_25\\csv\\stats_and_grads__pos_3__neg_1.csv (no 'm/z' column)\n",
      "Splitting F:/20251018d/bin_25\\csv\\grads_AB__pos_3__neg_0.csv ...\n",
      "Splitting F:/20251018d/bin_25\\csv\\stats_and_grads__pos_3__neg_0.csv ...\n",
      "[SKIP] F:/20251018d/bin_25\\csv\\stats_and_grads__pos_3__neg_0.csv (no 'm/z' column)\n",
      "Split done. Wrote 32 files to F:/20251018d/bin_25\\csv\\result\n",
      "\n",
      "================= BIN 35 =================\n",
      "Bin 35: samples=118, dim=13692  class_counts={0: 33, 1: 30, 2: 26, 3: 29}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\benja\\miniconda3\\envs\\vae\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Seed base 111] Fold 1/5 trained 5 models (total: 5)\n",
      "[Seed base 111] Fold 2/5 trained 5 models (total: 10)\n",
      "[Seed base 111] Fold 3/5 trained 5 models (total: 15)\n",
      "[Seed base 111] Fold 4/5 trained 5 models (total: 20)\n",
      "[Seed base 111] Fold 5/5 trained 5 models (total: 25)\n",
      "[Seed base 777] Fold 1/5 trained 5 models (total: 5)\n",
      "[Seed base 777] Fold 2/5 trained 5 models (total: 10)\n",
      "[Seed base 777] Fold 3/5 trained 5 models (total: 15)\n",
      "[Seed base 777] Fold 4/5 trained 5 models (total: 20)\n",
      "[Seed base 777] Fold 5/5 trained 5 models (total: 25)\n",
      "  [COMPARE] pos_2__neg_1 | Cos(pos)=0.730192 Cos(neg|abs|)=0.852749  r=0.623\n",
      "  [COMPARE] pos_2__neg_0 | Cos(pos)=0.933359 Cos(neg|abs|)=0.889741  r=0.353\n",
      "  [COMPARE] pos_1__neg_0 | Cos(pos)=0.926483 Cos(neg|abs|)=0.842590  r=0.395\n",
      "  [COMPARE] pos_3__neg_2 | Cos(pos)=0.859105 Cos(neg|abs|)=0.973880  r=0.239\n",
      "  [COMPARE] pos_3__neg_0_1_2 | Cos(pos)=0.860420 Cos(neg|abs|)=0.974485  r=0.159\n",
      "  [COMPARE] pos_2_1__neg_0 | Cos(pos)=0.943897 Cos(neg|abs|)=0.892494  r=0.305\n",
      "  [COMPARE] pos_3__neg_1 | Cos(pos)=0.815625 Cos(neg|abs|)=0.972159  r=0.175\n",
      "  [COMPARE] pos_3__neg_0 | Cos(pos)=0.868050 Cos(neg|abs|)=0.962098  r=0.268\n",
      "Splitting F:/20251018d/bin_35\\csv\\grads_AB__pos_2__neg_1.csv ...\n",
      "Splitting F:/20251018d/bin_35\\csv\\stats_and_grads__pos_2__neg_1.csv ...\n",
      "[SKIP] F:/20251018d/bin_35\\csv\\stats_and_grads__pos_2__neg_1.csv (no 'm/z' column)\n",
      "Splitting F:/20251018d/bin_35\\csv\\grads_AB__pos_2__neg_0.csv ...\n",
      "Splitting F:/20251018d/bin_35\\csv\\stats_and_grads__pos_2__neg_0.csv ...\n",
      "[SKIP] F:/20251018d/bin_35\\csv\\stats_and_grads__pos_2__neg_0.csv (no 'm/z' column)\n",
      "Splitting F:/20251018d/bin_35\\csv\\grads_AB__pos_1__neg_0.csv ...\n",
      "Splitting F:/20251018d/bin_35\\csv\\stats_and_grads__pos_1__neg_0.csv ...\n",
      "[SKIP] F:/20251018d/bin_35\\csv\\stats_and_grads__pos_1__neg_0.csv (no 'm/z' column)\n",
      "Splitting F:/20251018d/bin_35\\csv\\grads_AB__pos_3__neg_2.csv ...\n",
      "Splitting F:/20251018d/bin_35\\csv\\stats_and_grads__pos_3__neg_2.csv ...\n",
      "[SKIP] F:/20251018d/bin_35\\csv\\stats_and_grads__pos_3__neg_2.csv (no 'm/z' column)\n",
      "Splitting F:/20251018d/bin_35\\csv\\grads_AB__pos_3__neg_0_1_2.csv ...\n",
      "Splitting F:/20251018d/bin_35\\csv\\stats_and_grads__pos_3__neg_0_1_2.csv ...\n",
      "[SKIP] F:/20251018d/bin_35\\csv\\stats_and_grads__pos_3__neg_0_1_2.csv (no 'm/z' column)\n",
      "Splitting F:/20251018d/bin_35\\csv\\grads_AB__pos_2_1__neg_0.csv ...\n",
      "Splitting F:/20251018d/bin_35\\csv\\stats_and_grads__pos_2_1__neg_0.csv ...\n",
      "[SKIP] F:/20251018d/bin_35\\csv\\stats_and_grads__pos_2_1__neg_0.csv (no 'm/z' column)\n",
      "Splitting F:/20251018d/bin_35\\csv\\grads_AB__pos_3__neg_1.csv ...\n",
      "Splitting F:/20251018d/bin_35\\csv\\stats_and_grads__pos_3__neg_1.csv ...\n",
      "[SKIP] F:/20251018d/bin_35\\csv\\stats_and_grads__pos_3__neg_1.csv (no 'm/z' column)\n",
      "Splitting F:/20251018d/bin_35\\csv\\grads_AB__pos_3__neg_0.csv ...\n",
      "Splitting F:/20251018d/bin_35\\csv\\stats_and_grads__pos_3__neg_0.csv ...\n",
      "[SKIP] F:/20251018d/bin_35\\csv\\stats_and_grads__pos_3__neg_0.csv (no 'm/z' column)\n",
      "Split done. Wrote 32 files to F:/20251018d/bin_35\\csv\\result\n",
      "\n",
      "================= BIN 45 =================\n",
      "Bin 45: samples=118, dim=13692  class_counts={0: 33, 1: 30, 2: 26, 3: 29}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\benja\\miniconda3\\envs\\vae\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Seed base 111] Fold 1/5 trained 5 models (total: 5)\n",
      "[Seed base 111] Fold 2/5 trained 5 models (total: 10)\n",
      "[Seed base 111] Fold 3/5 trained 5 models (total: 15)\n",
      "[Seed base 111] Fold 4/5 trained 5 models (total: 20)\n",
      "[Seed base 111] Fold 5/5 trained 5 models (total: 25)\n",
      "[Seed base 777] Fold 1/5 trained 5 models (total: 5)\n",
      "[Seed base 777] Fold 2/5 trained 5 models (total: 10)\n",
      "[Seed base 777] Fold 3/5 trained 5 models (total: 15)\n",
      "[Seed base 777] Fold 4/5 trained 5 models (total: 20)\n",
      "[Seed base 777] Fold 5/5 trained 5 models (total: 25)\n",
      "  [COMPARE] pos_2__neg_1 | Cos(pos)=0.633620 Cos(neg|abs|)=0.643979  r=0.628\n",
      "  [COMPARE] pos_2__neg_0 | Cos(pos)=0.947583 Cos(neg|abs|)=0.715467  r=0.411\n",
      "  [COMPARE] pos_1__neg_0 | Cos(pos)=0.946522 Cos(neg|abs|)=0.646888  r=0.427\n",
      "  [COMPARE] pos_3__neg_2 | Cos(pos)=0.636357 Cos(neg|abs|)=0.965171  r=0.161\n",
      "  [COMPARE] pos_3__neg_0_1_2 | Cos(pos)=0.761158 Cos(neg|abs|)=0.965264  r=0.072\n",
      "  [COMPARE] pos_2_1__neg_0 | Cos(pos)=0.957448 Cos(neg|abs|)=0.703140  r=0.401\n",
      "  [COMPARE] pos_3__neg_1 | Cos(pos)=0.319515 Cos(neg|abs|)=0.966145  r=0.144\n",
      "  [COMPARE] pos_3__neg_0 | Cos(pos)=0.644763 Cos(neg|abs|)=0.905472  r=0.337\n",
      "Splitting F:/20251018d/bin_45\\csv\\grads_AB__pos_2__neg_1.csv ...\n",
      "Splitting F:/20251018d/bin_45\\csv\\stats_and_grads__pos_2__neg_1.csv ...\n",
      "[SKIP] F:/20251018d/bin_45\\csv\\stats_and_grads__pos_2__neg_1.csv (no 'm/z' column)\n",
      "Splitting F:/20251018d/bin_45\\csv\\grads_AB__pos_2__neg_0.csv ...\n",
      "Splitting F:/20251018d/bin_45\\csv\\stats_and_grads__pos_2__neg_0.csv ...\n",
      "[SKIP] F:/20251018d/bin_45\\csv\\stats_and_grads__pos_2__neg_0.csv (no 'm/z' column)\n",
      "Splitting F:/20251018d/bin_45\\csv\\grads_AB__pos_1__neg_0.csv ...\n",
      "Splitting F:/20251018d/bin_45\\csv\\stats_and_grads__pos_1__neg_0.csv ...\n",
      "[SKIP] F:/20251018d/bin_45\\csv\\stats_and_grads__pos_1__neg_0.csv (no 'm/z' column)\n",
      "Splitting F:/20251018d/bin_45\\csv\\grads_AB__pos_3__neg_2.csv ...\n",
      "Splitting F:/20251018d/bin_45\\csv\\stats_and_grads__pos_3__neg_2.csv ...\n",
      "[SKIP] F:/20251018d/bin_45\\csv\\stats_and_grads__pos_3__neg_2.csv (no 'm/z' column)\n",
      "Splitting F:/20251018d/bin_45\\csv\\grads_AB__pos_3__neg_0_1_2.csv ...\n",
      "Splitting F:/20251018d/bin_45\\csv\\stats_and_grads__pos_3__neg_0_1_2.csv ...\n",
      "[SKIP] F:/20251018d/bin_45\\csv\\stats_and_grads__pos_3__neg_0_1_2.csv (no 'm/z' column)\n",
      "Splitting F:/20251018d/bin_45\\csv\\grads_AB__pos_2_1__neg_0.csv ...\n",
      "Splitting F:/20251018d/bin_45\\csv\\stats_and_grads__pos_2_1__neg_0.csv ...\n",
      "[SKIP] F:/20251018d/bin_45\\csv\\stats_and_grads__pos_2_1__neg_0.csv (no 'm/z' column)\n",
      "Splitting F:/20251018d/bin_45\\csv\\grads_AB__pos_3__neg_1.csv ...\n",
      "Splitting F:/20251018d/bin_45\\csv\\stats_and_grads__pos_3__neg_1.csv ...\n",
      "[SKIP] F:/20251018d/bin_45\\csv\\stats_and_grads__pos_3__neg_1.csv (no 'm/z' column)\n",
      "Splitting F:/20251018d/bin_45\\csv\\grads_AB__pos_3__neg_0.csv ...\n",
      "Splitting F:/20251018d/bin_45\\csv\\stats_and_grads__pos_3__neg_0.csv ...\n",
      "[SKIP] F:/20251018d/bin_45\\csv\\stats_and_grads__pos_3__neg_0.csv (no 'm/z' column)\n",
      "Split done. Wrote 32 files to F:/20251018d/bin_45\\csv\\result\n",
      "\n",
      "================= BIN 55 =================\n",
      "Bin 55: samples=118, dim=13692  class_counts={0: 33, 1: 30, 2: 26, 3: 29}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\benja\\miniconda3\\envs\\vae\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Seed base 111] Fold 1/5 trained 5 models (total: 5)\n",
      "[Seed base 111] Fold 2/5 trained 5 models (total: 10)\n",
      "[Seed base 111] Fold 3/5 trained 5 models (total: 15)\n",
      "[Seed base 111] Fold 4/5 trained 5 models (total: 20)\n",
      "[Seed base 111] Fold 5/5 trained 5 models (total: 25)\n",
      "[Seed base 777] Fold 1/5 trained 5 models (total: 5)\n",
      "[Seed base 777] Fold 2/5 trained 5 models (total: 10)\n",
      "[Seed base 777] Fold 3/5 trained 5 models (total: 15)\n",
      "[Seed base 777] Fold 4/5 trained 5 models (total: 20)\n",
      "[Seed base 777] Fold 5/5 trained 5 models (total: 25)\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import os, re, json, gc, sys, ast\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Optional (not used in plotting)\n",
    "import seaborn as sns  # noqa: F401\n",
    "from scipy.signal import find_peaks  # noqa: F401\n",
    "from scipy.signal import savgol_filter  # noqa: F401\n",
    "from scipy.ndimage import gaussian_filter1d  # noqa: F401\n",
    "from scipy.linalg import svd  # noqa: F401\n",
    "from scipy import stats  # <-- NEW: t-test\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras import regularizers\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# ----------------------------\n",
    "# Repro & GPU memory growth\n",
    "# ----------------------------\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "try:\n",
    "    gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "    for _gpu in gpus:\n",
    "        tf.config.experimental.set_memory_growth(_gpu, True)\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "def hard_free():\n",
    "    \"\"\"Aggressively release memory after each bin.\"\"\"\n",
    "    try: plt.close('all')\n",
    "    except: pass\n",
    "    try: tf.keras.backend.clear_session()\n",
    "    except: pass\n",
    "    try: gc.collect(); gc.collect()\n",
    "    except: pass\n",
    "    try:\n",
    "        import ctypes, platform\n",
    "        if platform.system().lower() == \"linux\":\n",
    "            ctypes.CDLL(\"libc.so.6\").malloc_trim(0)\n",
    "    except: pass\n",
    "\n",
    "# ----------------------------\n",
    "# Grouped log-odds gradient\n",
    "# ----------------------------\n",
    "@tf.function(reduce_retracing=True)\n",
    "def _group_logodds_grad_for_model(x1, model, pos_ids, neg_ids, eps):\n",
    "    pos_ids = tf.constant(pos_ids, dtype=tf.int32)\n",
    "    neg_ids = tf.constant(neg_ids, dtype=tf.int32)\n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(x1)\n",
    "        p = model(x1, training=False)  # (1, C)\n",
    "        p_pos = tf.reduce_sum(tf.gather(p, pos_ids, axis=1), axis=1)  # (1,)\n",
    "        p_neg = tf.reduce_sum(tf.gather(p, neg_ids, axis=1), axis=1)  # (1,)\n",
    "        log_odds = tf.math.log(p_pos + eps) - tf.math.log(p_neg + eps)\n",
    "    g = tape.gradient(log_odds, x1)  # (1, D)\n",
    "    return tf.squeeze(g, axis=0)     # (D,)\n",
    "\n",
    "def compute_avg_group_logodds_gradient(X: np.ndarray, models: list, pos_ids=(2,3), neg_ids=(0,1), eps: float = 1e-8) -> np.ndarray:\n",
    "    X_t = tf.convert_to_tensor(X, dtype=tf.float32)\n",
    "    N = int(X_t.shape[0])\n",
    "    sample_grads = []\n",
    "    for i in range(N):\n",
    "        x_i = X_t[i:i+1]\n",
    "        grads_over_models = []\n",
    "        for m in models:\n",
    "            g = _group_logodds_grad_for_model(x_i, m, pos_ids, neg_ids, eps)\n",
    "            grads_over_models.append(g)\n",
    "        g_avg_models = tf.reduce_mean(tf.stack(grads_over_models, axis=0), axis=0)\n",
    "        sample_grads.append(g_avg_models)\n",
    "    avg_grad = tf.reduce_mean(tf.stack(sample_grads, axis=0), axis=0)\n",
    "    return avg_grad.numpy()\n",
    "\n",
    "# ----------------------------\n",
    "# Model\n",
    "# ----------------------------\n",
    "def build_model(input_dim: int, num_classes: int):\n",
    "    model = Sequential([\n",
    "        Dense(128, input_dim=input_dim, activation='relu', kernel_regularizer=regularizers.l1(0.01)),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(),\n",
    "                  loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# ----------------------------\n",
    "# Helpers (cosine + plotting)\n",
    "# ----------------------------\n",
    "def cosine_sim(a: np.ndarray, b: np.ndarray, eps: float = 1e-12) -> float:\n",
    "    a = np.asarray(a, dtype=float).ravel()\n",
    "    b = np.asarray(b, dtype=float).ravel()\n",
    "    n = min(a.size, b.size)\n",
    "    a = a[:n]; b = b[:n]\n",
    "    denom = (np.linalg.norm(a) * np.linalg.norm(b)) + eps\n",
    "    return float(np.dot(a, b) / denom)\n",
    "\n",
    "def mirror_plot(x, top_y, bottom_y, title, outfile):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(x, top_y, linewidth=1.0, label=\"Run A\")\n",
    "    plt.plot(x, -bottom_y, linewidth=1.0, label=\"Run B (mirrored)\")\n",
    "    plt.axhline(0.0, linewidth=0.8)\n",
    "    plt.xlabel(\"m/z (approx grid)\")\n",
    "    plt.ylabel(\"Gradient magnitude\")\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(outfile, dpi=200)\n",
    "    plt.close()\n",
    "\n",
    "# ----------------------------\n",
    "# NEW: Stats & plotting helpers for volcano + signed agreement\n",
    "# ----------------------------\n",
    "def ttest_and_log2fc(X: np.ndarray, y: np.ndarray, pos_ids, neg_ids, eps: float = 1e-12):\n",
    "    \"\"\"Welch's t-test (pos vs neg) per feature + log2FC (pos/neg).\"\"\"\n",
    "    pos_mask = np.isin(y, list(pos_ids))\n",
    "    neg_mask = np.isin(y, list(neg_ids))\n",
    "    Xp = X[pos_mask]; Xn = X[neg_mask]\n",
    "    # Handle edge cases gracefully\n",
    "    if Xp.shape[0] < 2 or Xn.shape[0] < 2:\n",
    "        # fall back to NaNs so plots show empty\n",
    "        pvals = np.full(X.shape[1], np.nan)\n",
    "        log2fc = np.full(X.shape[1], np.nan)\n",
    "        return pvals, log2fc\n",
    "\n",
    "    # Welch's t-test, axis=0 over features\n",
    "    tstat, pvals = stats.ttest_ind(Xp, Xn, axis=0, equal_var=False, nan_policy='omit')\n",
    "    mu_p = np.nanmean(Xp, axis=0)\n",
    "    mu_n = np.nanmean(Xn, axis=0)\n",
    "    log2fc = np.log2((mu_p + eps) / (mu_n + eps))\n",
    "    # Replace any nan/inf from degenerate features\n",
    "    pvals = np.where(np.isfinite(pvals), pvals, 1.0)\n",
    "    log2fc = np.where(np.isfinite(log2fc), log2fc, 0.0)\n",
    "    return pvals, log2fc\n",
    "\n",
    "def volcano_plot(log2fc: np.ndarray, pvals: np.ndarray,\n",
    "                 lfc_left: float, lfc_right: float, alpha: float,\n",
    "                 title: str, outfile: str):\n",
    "    xl = log2fc\n",
    "    yl = -np.log10(np.clip(pvals, 1e-300, 1.0))\n",
    "    sig_up   = (xl >= lfc_right) & (pvals <= alpha)\n",
    "    sig_down = (xl <= lfc_left)  & (pvals <= alpha)\n",
    "    other    = ~(sig_up | sig_down)\n",
    "\n",
    "    plt.figure(figsize=(10,6))\n",
    "    plt.scatter(xl[other], yl[other], s=15, alpha=0.6, color='lightgray')\n",
    "    plt.scatter(xl[sig_down], yl[sig_down], s=30, alpha=0.9, color='C0')  # blue\n",
    "    plt.scatter(xl[sig_up], yl[sig_up], s=30, alpha=0.9, color='C3')      # red\n",
    "    # thresholds\n",
    "    plt.axvline(lfc_left,  ls='--', lw=1, color='k')\n",
    "    plt.axvline(lfc_right, ls='--', lw=1, color='k')\n",
    "    plt.axhline(-np.log10(alpha), ls='--', lw=1, color='k')\n",
    "    plt.xlabel(\"log2 fold-change (pos / neg)\")\n",
    "    plt.ylabel(r\"$-\\log_{10}(p)$\")\n",
    "    plt.title(title)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(outfile, dpi=200)\n",
    "    plt.close()\n",
    "\n",
    "def signed_agreement_plot(log2fc: np.ndarray, pvals: np.ndarray, grad_mean: np.ndarray,\n",
    "                          lfc_left: float, lfc_right: float, alpha: float,\n",
    "                          title_prefix: str, outfile: str):\n",
    "    signed_stat = log2fc * (-np.log10(np.clip(pvals, 1e-300, 1.0)))\n",
    "    # Pearson r (ignore NaNs)\n",
    "    mask = np.isfinite(signed_stat) & np.isfinite(grad_mean)\n",
    "    if np.sum(mask) >= 2:\n",
    "        r = np.corrcoef(signed_stat[mask], grad_mean[mask])[0,1]\n",
    "    else:\n",
    "        r = np.nan\n",
    "\n",
    "    sig_up   = (log2fc >= lfc_right) & (pvals <= alpha)\n",
    "    sig_down = (log2fc <= lfc_left)  & (pvals <= alpha)\n",
    "    other    = ~(sig_up | sig_down)\n",
    "\n",
    "    plt.figure(figsize=(10,6))\n",
    "    plt.scatter(signed_stat[other], grad_mean[other], s=15, alpha=0.6, color='lightgray')\n",
    "    plt.scatter(signed_stat[sig_down], grad_mean[sig_down], s=30, alpha=0.9, color='C0')\n",
    "    plt.scatter(signed_stat[sig_up],   grad_mean[sig_up],   s=30, alpha=0.9, color='C3')\n",
    "    plt.axvline(0.0, ls='--', lw=1, color='k'); plt.axhline(0.0, ls='--', lw=1, color='k')\n",
    "    plt.xlabel(r\"log2FC × $-\\log_{10}(p)$  (signed)\")\n",
    "    plt.ylabel(\"Average log-odds gradient (RunA/B mean)\")\n",
    "    plt.title(f\"{title_prefix}  (Pearson r = {r:.3f})\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(outfile, dpi=200)\n",
    "    plt.close()\n",
    "    return float(r)\n",
    "\n",
    "# ----------------------------\n",
    "# Splitter (4 CSVs per combined CSV) with bin prefix\n",
    "# ----------------------------\n",
    "TARGET_COLS = [\"pos_runA\", \"pos_runB\", \"negabs_runA\", \"negabs_runB\"]\n",
    "MZ_COL = \"m/z\"\n",
    "\n",
    "def split_csv(input_path: str, out_dir: str, bin_value: int) -> list[str]:\n",
    "    \"\"\"Split one combined CSV into 4 CSVs, prefixing filenames with bin number.\"\"\"\n",
    "    df = pd.read_csv(input_path)\n",
    "    if MZ_COL not in df.columns:\n",
    "        print(f\"[SKIP] {input_path} (no '{MZ_COL}' column)\")\n",
    "        return []\n",
    "    available_targets = [c for c in TARGET_COLS if c in df.columns]\n",
    "    if not available_targets:\n",
    "        print(f\"[SKIP] {input_path} (none of {TARGET_COLS} found)\")\n",
    "        return []\n",
    "    base = os.path.splitext(os.path.basename(input_path))[0]\n",
    "    written = []\n",
    "    for col in available_targets:\n",
    "        out_df = df[[MZ_COL, col]].copy()\n",
    "        out_path = os.path.join(out_dir, f\"bin{bin_value}_{base}_{col}.csv\")\n",
    "        out_df.to_csv(out_path, index=False)\n",
    "        written.append(out_path)\n",
    "    return written\n",
    "\n",
    "def process_folder(folder_path: str, bin_value: int) -> list[str]:\n",
    "    \"\"\"Split all CSVs in a folder to 'result/' and return list of result paths.\"\"\"\n",
    "    if not os.path.isdir(folder_path):\n",
    "        print(f\"[WARN] Not a folder: {folder_path}\")\n",
    "        return []\n",
    "    out_dir = os.path.join(folder_path, \"result\")\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "    all_outputs = []\n",
    "    for fname in os.listdir(folder_path):\n",
    "        if fname.lower().endswith(\".csv\"):\n",
    "            fpath = os.path.join(folder_path, fname)\n",
    "            print(f\"Splitting {fpath} ...\")\n",
    "            outputs = split_csv(fpath, out_dir, bin_value)\n",
    "            all_outputs.extend(outputs)\n",
    "\n",
    "    print(f\"Split done. Wrote {len(all_outputs)} files to {out_dir}\")\n",
    "    return all_outputs\n",
    "\n",
    "# ----------------------------\n",
    "# Config (edit these)\n",
    "# ----------------------------\n",
    "CSV_PATH    = r\"F:/20251010/dataset_rt_batch.csv\"   # input dataset with 'bin' and 'target'\n",
    "EPOCHS      = 50\n",
    "BATCH_SIZE  = 32\n",
    "K_SPLITS    = 5\n",
    "N_REPEATS   = 5\n",
    "SEED_BASES  = [111, 777]                   # two independent runs\n",
    "OUT_ROOT    = r\"F:/20251018d/\"              # everything goes directly under bin_<N>/\n",
    "BIN_WHITELIST = None  # e.g. [35, 75]\n",
    "\n",
    "# Volcano thresholds (edit to taste)\n",
    "ALPHA = 0.05\n",
    "LFC_LEFT, LFC_RIGHT = -1.0, 1.0\n",
    "\n",
    "# >>> NEW: TIC normalization scale (used ONLY for volcano stats) <<<\n",
    "TIC_SCALE = 1e10\n",
    "\n",
    "# Fixed grouping list (no prompt, no AUTO)\n",
    "GROUPINGS = [((2,),(1,)), ((2,),(0,)), ((1,),(0,)), ((3,),(2,)), ((3,),(0,1,2)), ((2,1),(0,)), ((3,),(1,)),  ((3,),(0,))]\n",
    "\n",
    "# ----------------------------\n",
    "# KFold+Repeats trainer\n",
    "# ----------------------------\n",
    "def train_kfold_repeats(X: np.ndarray, Y: np.ndarray, seed_base: int):\n",
    "    kf = KFold(n_splits=K_SPLITS, shuffle=True, random_state=42)\n",
    "    all_models = []\n",
    "    num_classes = int(np.max(Y)) + 1\n",
    "    for fold, (tr, va) in enumerate(kf.split(X, Y), 1):\n",
    "        X_tr, y_tr = X[tr], Y[tr]\n",
    "        X_va, y_va = X[va], Y[va]\n",
    "        for r in range(N_REPEATS):\n",
    "            seed = seed_base * 1000 + fold * 100 + r\n",
    "            tf.keras.utils.set_random_seed(seed)\n",
    "            np.random.seed(seed)\n",
    "            m = build_model(X.shape[1], num_classes)\n",
    "            m.fit(X_tr, y_tr, epochs=EPOCHS, batch_size=BATCH_SIZE,\n",
    "                  validation_data=(X_va, y_va), verbose=0)\n",
    "            all_models.append(m)\n",
    "        print(f\"[Seed base {seed_base}] Fold {fold}/{K_SPLITS} trained {N_REPEATS} models (total: {len(all_models)})\")\n",
    "    return all_models\n",
    "\n",
    "# ----------------------------\n",
    "# MAIN (bin-root outputs + keep only split CSVs)\n",
    "# ----------------------------\n",
    "def main():\n",
    "    df = pd.read_csv(CSV_PATH)\n",
    "    # Discover bins\n",
    "    bins_found = sorted([b for b in df[\"bin\"].dropna().unique().tolist()])\n",
    "    if BIN_WHITELIST is not None:\n",
    "        bins_to_process = [b for b in bins_found if b in set(BIN_WHITELIST)]\n",
    "    else:\n",
    "        bins_to_process = bins_found\n",
    "    if len(bins_to_process) == 0:\n",
    "        raise ValueError(f\"No 'bin' values found in {CSV_PATH}\")\n",
    "\n",
    "    # Keep only classes 0..3 by default\n",
    "    df = df[df[\"target\"].astype(int).isin([0,1,2,3])].copy()\n",
    "    unique_labels = np.sort(df[\"target\"].astype(int).unique())\n",
    "    assert unique_labels[0] == 0 and np.array_equal(unique_labels, np.arange(unique_labels[-1] + 1)), \\\n",
    "        f\"Non-contiguous labels detected: {unique_labels}. Please remap to 0..C-1.\"\n",
    "    groupings = GROUPINGS\n",
    "    print(f\"\\nUsing fixed grouping(s): {groupings}\")\n",
    "\n",
    "    os.makedirs(OUT_ROOT, exist_ok=True)\n",
    "\n",
    "    for BIN_VALUE in bins_to_process:\n",
    "        print(f\"\\n================= BIN {BIN_VALUE} =================\")\n",
    "        models_A = models_B = None\n",
    "        X = Y = fdf = None\n",
    "\n",
    "        try:\n",
    "            fdf = df[df[\"bin\"] == BIN_VALUE].copy()\n",
    "            if fdf.empty:\n",
    "                print(f\"[WARN] No rows for bin {BIN_VALUE}; skipping.\")\n",
    "                continue\n",
    "\n",
    "            # Normalize all features except ['bin','target'] within this bin\n",
    "            cols_to_norm = fdf.columns.difference(['bin', 'target'])\n",
    "            fdf[cols_to_norm] = fdf[cols_to_norm].apply(lambda x: x / (x.max() + 1.0))\n",
    "\n",
    "            Y = fdf[\"target\"].astype(int).to_numpy()\n",
    "            feature_cols = fdf.columns.difference(['bin', 'target'])\n",
    "            X = np.nan_to_num(fdf[feature_cols].to_numpy(), copy=False).astype(np.float32)\n",
    "\n",
    "            if X.shape[0] < 2 or X.shape[1] < 1:\n",
    "                print(f\"[WARN] Insufficient data for bin {BIN_VALUE} (samples={X.shape[0]}, dim={X.shape[1]}). Skipping.\")\n",
    "                continue\n",
    "\n",
    "            print(f\"Bin {BIN_VALUE}: samples={X.shape[0]}, dim={X.shape[1]}  class_counts=\"\n",
    "                  f\"{dict(zip(*np.unique(Y, return_counts=True)))}\")\n",
    "\n",
    "            # Output dirs per bin (everything under bin folder)\n",
    "            OUT_DIR   = os.path.join(OUT_ROOT, f\"bin_{str(BIN_VALUE).replace('.', '_')}\")\n",
    "            CSV_DIR   = os.path.join(OUT_DIR, \"csv\")\n",
    "            PLOTS_DIR = os.path.join(OUT_DIR, \"plots\")\n",
    "            os.makedirs(OUT_DIR, exist_ok=True)\n",
    "            os.makedirs(CSV_DIR, exist_ok=True)\n",
    "            os.makedirs(PLOTS_DIR, exist_ok=True)\n",
    "\n",
    "            # Train ensembles ONCE per run (A, B)\n",
    "            models_A = train_kfold_repeats(X, Y, seed_base=SEED_BASES[0])\n",
    "            models_B = train_kfold_repeats(X, Y, seed_base=SEED_BASES[1])\n",
    "\n",
    "            # grid helper (for your mirror plots)\n",
    "            def _make_grid(n):\n",
    "                n_grid = min(10000, n)\n",
    "                x_grid = np.arange(600, 600 + 0.1 * n_grid, 0.1)[:n_grid]\n",
    "                return n_grid, x_grid\n",
    "\n",
    "            def save_compare_only(pos_ids, neg_ids):\n",
    "                # ---- Gradients (Run A & B) — unchanged, NO TIC normalization\n",
    "                grad_A = compute_avg_group_logodds_gradient(X, models_A, pos_ids=pos_ids, neg_ids=neg_ids, eps=1e-8)\n",
    "                grad_B = compute_avg_group_logodds_gradient(X, models_B, pos_ids=pos_ids, neg_ids=neg_ids, eps=1e-8)\n",
    "                grad_mean = (grad_A + grad_B) / 2.0\n",
    "\n",
    "                pos_tag = \"_\".join(map(str, pos_ids))\n",
    "                neg_tag = \"_\".join(map(str, neg_ids))\n",
    "                tag = f\"pos_{pos_tag}__neg_{neg_tag}\"\n",
    "\n",
    "                # ---- Mirror plots (unchanged from your flow)\n",
    "                n_grid, x_grid = _make_grid(min(grad_A.size, grad_B.size))\n",
    "                yA = grad_A[:n_grid]; yB = grad_B[:n_grid]\n",
    "                yA_pos = np.where(yA > 0, yA, 0.0)\n",
    "                yB_pos = np.where(yB > 0, yB, 0.0)\n",
    "                yA_neg = np.where(yA < 0, -yA, 0.0)  # abs\n",
    "                yB_neg = np.where(yB < 0, -yB, 0.0)\n",
    "\n",
    "                cos_pos = cosine_sim(yA_pos, yB_pos)\n",
    "                cos_neg = cosine_sim(yA_neg, yB_neg)\n",
    "\n",
    "                comb_csv = os.path.join(CSV_DIR, f\"grads_AB__{tag}.csv\")\n",
    "                pd.DataFrame({\n",
    "                    \"m/z\": x_grid,\n",
    "                    \"grad_runA\": yA,\n",
    "                    \"grad_runB\": yB,\n",
    "                    \"pos_runA\": yA_pos,\n",
    "                    \"pos_runB\": yB_pos,\n",
    "                    \"negabs_runA\": yA_neg,\n",
    "                    \"negabs_runB\": yB_neg,\n",
    "                }).to_csv(comb_csv, index=False)\n",
    "\n",
    "                pos_title = (f\"Bin {BIN_VALUE} — Mirror Positive Gradients \"\n",
    "                             f\"[{tag}] (cos={cos_pos:.4f})\")\n",
    "                neg_title = (f\"Bin {BIN_VALUE} — Mirror Negative Gradients |abs| \"\n",
    "                             f\"[{tag}] (cos={cos_neg:.4f})\")\n",
    "\n",
    "                mirror_plot(\n",
    "                    x_grid, yA_pos, yB_pos,\n",
    "                    title=pos_title,\n",
    "                    outfile=os.path.join(PLOTS_DIR, f\"{tag}__mirror_pos.png\")\n",
    "                )\n",
    "                mirror_plot(\n",
    "                    x_grid, yA_neg, yB_neg,\n",
    "                    title=neg_title,\n",
    "                    outfile=os.path.join(PLOTS_DIR, f\"{tag}__mirror_negabs.png\")\n",
    "                )\n",
    "\n",
    "                # ---- NEW: TIC normalization ONLY for volcano statistics\n",
    "                # Compute per-sample TIC on X, then scale by TIC_SCALE\n",
    "                tic = np.sum(X, axis=1)\n",
    "                tic = np.clip(tic, 1e-12, None)\n",
    "                X_tic = X / (tic[:, None] / TIC_SCALE)\n",
    "\n",
    "                # ---- Stats for volcano + ML agreement (now using TIC-normalized data)\n",
    "                pvals, log2fc = ttest_and_log2fc(X_tic, Y, pos_ids, neg_ids, eps=1e-12)\n",
    "\n",
    "                # Save per-feature stats + raw grads + mean grad\n",
    "                stats_csv = os.path.join(CSV_DIR, f\"stats_and_grads__{tag}.csv\")\n",
    "                pd.DataFrame({\n",
    "                    \"feature\": list(feature_cols),\n",
    "                    \"log2FC\": log2fc,\n",
    "                    \"pval\": pvals,\n",
    "                    \"neglog10p\": -np.log10(np.clip(pvals, 1e-300, 1.0)),\n",
    "                    \"grad_runA_full\": grad_A,\n",
    "                    \"grad_runB_full\": grad_B,\n",
    "                    \"grad_mean\": grad_mean,\n",
    "                    \"signed_stat\": log2fc * (-np.log10(np.clip(pvals, 1e-300, 1.0))),\n",
    "                }).to_csv(stats_csv, index=False)\n",
    "\n",
    "                volcano_plot(\n",
    "                    log2fc, pvals, LFC_LEFT, LFC_RIGHT, ALPHA,\n",
    "                    title=f\"Bin {BIN_VALUE} — Volcano ({tag})\",\n",
    "                    outfile=os.path.join(PLOTS_DIR, f\"{tag}__volcano.png\")\n",
    "                )\n",
    "                r = signed_agreement_plot(\n",
    "                    log2fc, pvals, grad_mean,\n",
    "                    LFC_LEFT, LFC_RIGHT, ALPHA,\n",
    "                    title_prefix=f\"Bin {BIN_VALUE} — Signed agreement ({tag})\",\n",
    "                    outfile=os.path.join(PLOTS_DIR, f\"{tag}__signed_agreement.png\")\n",
    "                )\n",
    "\n",
    "                # summary JSON directly under bin/\n",
    "                with open(os.path.join(OUT_DIR, f\"summary__{tag}.json\"), \"w\") as fC:\n",
    "                    json.dump({\n",
    "                        \"bin\": BIN_VALUE,\n",
    "                        \"grouping\": {\"pos_ids\": list(pos_ids), \"neg_ids\": list(neg_ids)},\n",
    "                        \"comparison\": \"Run A vs Run B\",\n",
    "                        \"cosine_pos\": cos_pos,\n",
    "                        \"cosine_neg_abs\": cos_neg,\n",
    "                        \"pearson_signed_agreement\": r,\n",
    "                        \"thresholds\": {\"alpha\": ALPHA, \"lfc_left\": LFC_LEFT, \"lfc_right\": LFC_RIGHT},\n",
    "                        \"paths\": {\n",
    "                            \"combined_csv_for_mirror\": comb_csv,\n",
    "                            \"stats_and_grads_csv\": stats_csv,\n",
    "                            \"plots_dir\": PLOTS_DIR,\n",
    "                        }\n",
    "                    }, fC, indent=2)\n",
    "\n",
    "                print(f\"  [COMPARE] {tag} | Cos(pos)={cos_pos:.6f} Cos(neg|abs|)={cos_neg:.6f}  r={r:.3f}\")\n",
    "\n",
    "            # run all groupings -> write combined CSVs/plots/JSONs\n",
    "            num_classes = int(np.max(Y)) + 1\n",
    "            for (pos_ids, neg_ids) in groupings:\n",
    "                for idx in (*pos_ids, *neg_ids):\n",
    "                    assert 0 <= idx < num_classes, f\"Class index {idx} out of range 0..{num_classes-1}\"\n",
    "                save_compare_only(pos_ids, neg_ids)\n",
    "\n",
    "            # --- Split step: create split CSVs into csv/result/ ---\n",
    "            split_outputs = process_folder(CSV_DIR, int(BIN_VALUE))\n",
    "\n",
    "            # Move split files from csv/result/ -> csv/ and delete originals\n",
    "            result_dir = os.path.join(CSV_DIR, \"result\")\n",
    "            moved = []\n",
    "            if os.path.isdir(result_dir):\n",
    "                for fname in os.listdir(result_dir):\n",
    "                    src = os.path.join(result_dir, fname)\n",
    "                    dst = os.path.join(CSV_DIR, fname)\n",
    "                    os.replace(src, dst)\n",
    "                    moved.append(dst)\n",
    "                try: os.rmdir(result_dir)\n",
    "                except OSError: pass\n",
    "\n",
    "            # Delete original combined CSVs (keep only split)\n",
    "            for fname in os.listdir(CSV_DIR):\n",
    "                if fname.lower().endswith(\".csv\") and fname.startswith(\"grads_AB__\"):\n",
    "                    try:\n",
    "                        os.remove(os.path.join(CSV_DIR, fname))\n",
    "                    except Exception as e:\n",
    "                        print(f\"[WARN] Could not remove {fname}: {e}\")\n",
    "\n",
    "            # Save manifest of final CSVs\n",
    "            with open(os.path.join(OUT_DIR, \"split_manifest.json\"), \"w\") as fman:\n",
    "                json.dump({\n",
    "                    \"bin\": BIN_VALUE,\n",
    "                    \"final_csvs\": sorted([os.path.basename(p) for p in moved]),\n",
    "                }, fman, indent=2)\n",
    "\n",
    "        finally:\n",
    "            try:\n",
    "                if models_A is not None:\n",
    "                    for _m in models_A: del _m\n",
    "                del models_A\n",
    "            except: pass\n",
    "            try:\n",
    "                if models_B is not None:\n",
    "                    for _m in models_B: del _m\n",
    "                del models_B\n",
    "            except: pass\n",
    "            for v in [\"X\",\"Y\",\"fdf\"]:\n",
    "                try: del globals()[v]\n",
    "                except: pass\n",
    "            hard_free()\n",
    "\n",
    "    print(\"\\nAll bins processed. (bin-root outputs; csv keeps only split files)\\n\")\n",
    "\n",
    "# ---- run ----\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9157633e",
   "metadata": {},
   "source": [
    "With synthetic dataset of the same dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f8a4db80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SYNTH] Wrote synthetic dataset to: F:/20251010/dataset_rt_batch.csv  | samples=400, D=10800\n",
      "[SYNTH] Per class markers: 100 up + 100 down (unique sets per class)\n",
      "\n",
      "Using fixed grouping(s): [((2,), (1,)), ((2,), (0,)), ((1,), (0,)), ((3,), (2,)), ((3,), (0, 1, 2)), ((2, 1), (0,)), ((3,), (1,)), ((3,), (0,))]\n",
      "\n",
      "================= BIN 1 =================\n",
      "Bin 1: samples=400, dim=10800  class_counts={0: 100, 1: 100, 2: 100, 3: 100}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\benja\\miniconda3\\envs\\vae\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Seed base 111] Fold 1/5 trained 5 models (total: 5)\n",
      "[Seed base 111] Fold 2/5 trained 5 models (total: 10)\n",
      "[Seed base 111] Fold 3/5 trained 5 models (total: 15)\n",
      "[Seed base 111] Fold 4/5 trained 5 models (total: 20)\n",
      "[Seed base 111] Fold 5/5 trained 5 models (total: 25)\n",
      "[Seed base 777] Fold 1/5 trained 5 models (total: 5)\n",
      "[Seed base 777] Fold 2/5 trained 5 models (total: 10)\n",
      "[Seed base 777] Fold 3/5 trained 5 models (total: 15)\n",
      "[Seed base 777] Fold 4/5 trained 5 models (total: 20)\n",
      "[Seed base 777] Fold 5/5 trained 5 models (total: 25)\n",
      "  [COMPARE] pos_2__neg_1 | Cos(pos)=0.386884 Cos(neg|abs|)=0.323962  r=0.320\n",
      "  [COMPARE] pos_2__neg_0 | Cos(pos)=0.303017 Cos(neg|abs|)=0.369628  r=0.314\n",
      "  [COMPARE] pos_1__neg_0 | Cos(pos)=0.291641 Cos(neg|abs|)=0.440761  r=0.319\n",
      "  [COMPARE] pos_3__neg_2 | Cos(pos)=0.459026 Cos(neg|abs|)=0.261516  r=0.317\n",
      "  [COMPARE] pos_3__neg_0_1_2 | Cos(pos)=0.487418 Cos(neg|abs|)=0.256704  r=0.279\n",
      "  [COMPARE] pos_2_1__neg_0 | Cos(pos)=0.289770 Cos(neg|abs|)=0.404246  r=0.289\n",
      "  [COMPARE] pos_3__neg_1 | Cos(pos)=0.512161 Cos(neg|abs|)=0.242362  r=0.313\n",
      "  [COMPARE] pos_3__neg_0 | Cos(pos)=0.426046 Cos(neg|abs|)=0.295507  r=0.305\n",
      "Splitting F:/20251018e/bin_1\\csv\\grads_AB__pos_2__neg_1.csv ...\n",
      "Splitting F:/20251018e/bin_1\\csv\\stats_and_grads__pos_2__neg_1.csv ...\n",
      "[SKIP] F:/20251018e/bin_1\\csv\\stats_and_grads__pos_2__neg_1.csv (no 'm/z' column)\n",
      "Splitting F:/20251018e/bin_1\\csv\\grads_AB__pos_2__neg_0.csv ...\n",
      "Splitting F:/20251018e/bin_1\\csv\\stats_and_grads__pos_2__neg_0.csv ...\n",
      "[SKIP] F:/20251018e/bin_1\\csv\\stats_and_grads__pos_2__neg_0.csv (no 'm/z' column)\n",
      "Splitting F:/20251018e/bin_1\\csv\\grads_AB__pos_1__neg_0.csv ...\n",
      "Splitting F:/20251018e/bin_1\\csv\\stats_and_grads__pos_1__neg_0.csv ...\n",
      "[SKIP] F:/20251018e/bin_1\\csv\\stats_and_grads__pos_1__neg_0.csv (no 'm/z' column)\n",
      "Splitting F:/20251018e/bin_1\\csv\\grads_AB__pos_3__neg_2.csv ...\n",
      "Splitting F:/20251018e/bin_1\\csv\\stats_and_grads__pos_3__neg_2.csv ...\n",
      "[SKIP] F:/20251018e/bin_1\\csv\\stats_and_grads__pos_3__neg_2.csv (no 'm/z' column)\n",
      "Splitting F:/20251018e/bin_1\\csv\\grads_AB__pos_3__neg_0_1_2.csv ...\n",
      "Splitting F:/20251018e/bin_1\\csv\\stats_and_grads__pos_3__neg_0_1_2.csv ...\n",
      "[SKIP] F:/20251018e/bin_1\\csv\\stats_and_grads__pos_3__neg_0_1_2.csv (no 'm/z' column)\n",
      "Splitting F:/20251018e/bin_1\\csv\\grads_AB__pos_2_1__neg_0.csv ...\n",
      "Splitting F:/20251018e/bin_1\\csv\\stats_and_grads__pos_2_1__neg_0.csv ...\n",
      "[SKIP] F:/20251018e/bin_1\\csv\\stats_and_grads__pos_2_1__neg_0.csv (no 'm/z' column)\n",
      "Splitting F:/20251018e/bin_1\\csv\\grads_AB__pos_3__neg_1.csv ...\n",
      "Splitting F:/20251018e/bin_1\\csv\\stats_and_grads__pos_3__neg_1.csv ...\n",
      "[SKIP] F:/20251018e/bin_1\\csv\\stats_and_grads__pos_3__neg_1.csv (no 'm/z' column)\n",
      "Splitting F:/20251018e/bin_1\\csv\\grads_AB__pos_3__neg_0.csv ...\n",
      "Splitting F:/20251018e/bin_1\\csv\\stats_and_grads__pos_3__neg_0.csv ...\n",
      "[SKIP] F:/20251018e/bin_1\\csv\\stats_and_grads__pos_3__neg_0.csv (no 'm/z' column)\n",
      "Split done. Wrote 32 files to F:/20251018e/bin_1\\csv\\result\n",
      "\n",
      "All bins processed. (bin-root outputs; csv keeps only split files)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import os, re, json, gc, sys, ast\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Optional (not used in plotting)\n",
    "import seaborn as sns  # noqa: F401\n",
    "from scipy.signal import find_peaks  # noqa: F401\n",
    "from scipy.signal import savgol_filter  # noqa: F401\n",
    "from scipy.ndimage import gaussian_filter1d  # noqa: F401\n",
    "from scipy.linalg import svd  # noqa: F401\n",
    "from scipy import stats  # <-- NEW: t-test\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras import regularizers\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# ----------------------------\n",
    "# Repro & GPU memory growth\n",
    "# ----------------------------\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "try:\n",
    "    gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "    for _gpu in gpus:\n",
    "        tf.config.experimental.set_memory_growth(_gpu, True)\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "def hard_free():\n",
    "    \"\"\"Aggressively release memory after each bin.\"\"\"\n",
    "    try: plt.close('all')\n",
    "    except: pass\n",
    "    try: tf.keras.backend.clear_session()\n",
    "    except: pass\n",
    "    try: gc.collect(); gc.collect()\n",
    "    except: pass\n",
    "    try:\n",
    "        import ctypes, platform\n",
    "        if platform.system().lower() == \"linux\":\n",
    "            ctypes.CDLL(\"libc.so.6\").malloc_trim(0)\n",
    "    except: pass\n",
    "\n",
    "# ----------------------------\n",
    "# Grouped log-odds gradient\n",
    "# ----------------------------\n",
    "@tf.function(reduce_retracing=True)\n",
    "def _group_logodds_grad_for_model(x1, model, pos_ids, neg_ids, eps):\n",
    "    pos_ids = tf.constant(pos_ids, dtype=tf.int32)\n",
    "    neg_ids = tf.constant(neg_ids, dtype=tf.int32)\n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(x1)\n",
    "        p = model(x1, training=False)  # (1, C)\n",
    "        p_pos = tf.reduce_sum(tf.gather(p, pos_ids, axis=1), axis=1)  # (1,)\n",
    "        p_neg = tf.reduce_sum(tf.gather(p, neg_ids, axis=1), axis=1)  # (1,)\n",
    "        log_odds = tf.math.log(p_pos + eps) - tf.math.log(p_neg + eps)\n",
    "    g = tape.gradient(log_odds, x1)  # (1, D)\n",
    "    return tf.squeeze(g, axis=0)     # (D,)\n",
    "\n",
    "def compute_avg_group_logodds_gradient(X: np.ndarray, models: list, pos_ids=(2,3), neg_ids=(0,1), eps: float = 1e-8) -> np.ndarray:\n",
    "    X_t = tf.convert_to_tensor(X, dtype=tf.float32)\n",
    "    N = int(X_t.shape[0])\n",
    "    sample_grads = []\n",
    "    for i in range(N):\n",
    "        x_i = X_t[i:i+1]\n",
    "        grads_over_models = []\n",
    "        for m in models:\n",
    "            g = _group_logodds_grad_for_model(x_i, m, pos_ids, neg_ids, eps)\n",
    "            grads_over_models.append(g)\n",
    "        g_avg_models = tf.reduce_mean(tf.stack(grads_over_models, axis=0), axis=0)\n",
    "        sample_grads.append(g_avg_models)\n",
    "    avg_grad = tf.reduce_mean(tf.stack(sample_grads, axis=0), axis=0)\n",
    "    return avg_grad.numpy()\n",
    "\n",
    "# ----------------------------\n",
    "# Model\n",
    "# ----------------------------\n",
    "def build_model(input_dim: int, num_classes: int):\n",
    "    model = Sequential([\n",
    "        Dense(128, input_dim=input_dim, activation='relu', kernel_regularizer=regularizers.l1(0.01)),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(),\n",
    "                  loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# ----------------------------\n",
    "# Helpers (cosine + plotting)\n",
    "# ----------------------------\n",
    "def cosine_sim(a: np.ndarray, b: np.ndarray, eps: float = 1e-12) -> float:\n",
    "    a = np.asarray(a, dtype=float).ravel()\n",
    "    b = np.asarray(b, dtype=float).ravel()\n",
    "    n = min(a.size, b.size)\n",
    "    a = a[:n]; b = b[:n]\n",
    "    denom = (np.linalg.norm(a) * np.linalg.norm(b)) + eps\n",
    "    return float(np.dot(a, b) / denom)\n",
    "\n",
    "def mirror_plot(x, top_y, bottom_y, title, outfile):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(x, top_y, linewidth=1.0, label=\"Run A\")\n",
    "    plt.plot(x, -bottom_y, linewidth=1.0, label=\"Run B (mirrored)\")\n",
    "    plt.axhline(0.0, linewidth=0.8)\n",
    "    plt.xlabel(\"m/z (approx grid)\")\n",
    "    plt.ylabel(\"Gradient magnitude\")\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(outfile, dpi=200)\n",
    "    plt.close()\n",
    "\n",
    "# ----------------------------\n",
    "# Stats & plotting helpers for volcano + signed agreement\n",
    "# ----------------------------\n",
    "def ttest_and_log2fc(X: np.ndarray, y: np.ndarray, pos_ids, neg_ids, eps: float = 1e-12):\n",
    "    \"\"\"Welch's t-test (pos vs neg) per feature + log2FC (pos/neg).\"\"\"\n",
    "    pos_mask = np.isin(y, list(pos_ids))\n",
    "    neg_mask = np.isin(y, list(neg_ids))\n",
    "    Xp = X[pos_mask]; Xn = X[neg_mask]\n",
    "    # Handle edge cases gracefully\n",
    "    if Xp.shape[0] < 2 or Xn.shape[0] < 2:\n",
    "        pvals = np.full(X.shape[1], np.nan)\n",
    "        log2fc = np.full(X.shape[1], np.nan)\n",
    "        return pvals, log2fc\n",
    "\n",
    "    # Welch's t-test, axis=0 over features\n",
    "    _tstat, pvals = stats.ttest_ind(Xp, Xn, axis=0, equal_var=False, nan_policy='omit')\n",
    "    mu_p = np.nanmean(Xp, axis=0)\n",
    "    mu_n = np.nanmean(Xn, axis=0)\n",
    "    log2fc = np.log2((mu_p + eps) / (mu_n + eps))\n",
    "    # Replace any nan/inf from degenerate features\n",
    "    pvals = np.where(np.isfinite(pvals), pvals, 1.0)\n",
    "    log2fc = np.where(np.isfinite(log2fc), log2fc, 0.0)\n",
    "    return pvals, log2fc\n",
    "\n",
    "def volcano_plot(log2fc: np.ndarray, pvals: np.ndarray,\n",
    "                 lfc_left: float, lfc_right: float, alpha: float,\n",
    "                 title: str, outfile: str):\n",
    "    xl = log2fc\n",
    "    yl = -np.log10(np.clip(pvals, 1e-300, 1.0))\n",
    "    sig_up   = (xl >= lfc_right) & (pvals <= alpha)\n",
    "    sig_down = (xl <= lfc_left)  & (pvals <= alpha)\n",
    "    other    = ~(sig_up | sig_down)\n",
    "\n",
    "    plt.figure(figsize=(10,6))\n",
    "    plt.scatter(xl[other], yl[other], s=15, alpha=0.6, color='lightgray')\n",
    "    plt.scatter(xl[sig_down], yl[sig_down], s=30, alpha=0.9, color='C0')  # blue\n",
    "    plt.scatter(xl[sig_up], yl[sig_up], s=30, alpha=0.9, color='C3')      # red\n",
    "    # thresholds\n",
    "    plt.axvline(lfc_left,  ls='--', lw=1, color='k')\n",
    "    plt.axvline(lfc_right, ls='--', lw=1, color='k')\n",
    "    plt.axhline(-np.log10(alpha), ls='--', lw=1, color='k')\n",
    "    plt.xlabel(\"log2 fold-change (pos / neg)\")\n",
    "    plt.ylabel(r\"$-\\log_{10}(p)$\")\n",
    "    plt.title(title)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(outfile, dpi=200)\n",
    "    plt.close()\n",
    "\n",
    "def signed_agreement_plot(log2fc: np.ndarray, pvals: np.ndarray, grad_mean: np.ndarray,\n",
    "                          lfc_left: float, lfc_right: float, alpha: float,\n",
    "                          title_prefix: str, outfile: str):\n",
    "    signed_stat = log2fc * (-np.log10(np.clip(pvals, 1e-300, 1.0)))\n",
    "    # Pearson r (ignore NaNs)\n",
    "    mask = np.isfinite(signed_stat) & np.isfinite(grad_mean)\n",
    "    if np.sum(mask) >= 2:\n",
    "        r = np.corrcoef(signed_stat[mask], grad_mean[mask])[0,1]\n",
    "    else:\n",
    "        r = np.nan\n",
    "\n",
    "    sig_up   = (log2fc >= lfc_right) & (pvals <= alpha)\n",
    "    sig_down = (log2fc <= lfc_left)  & (pvals <= alpha)\n",
    "    other    = ~(sig_up | sig_down)\n",
    "\n",
    "    plt.figure(figsize=(10,6))\n",
    "    plt.scatter(signed_stat[other], grad_mean[other], s=15, alpha=0.6, color='lightgray')\n",
    "    plt.scatter(signed_stat[sig_down], grad_mean[sig_down], s=30, alpha=0.9, color='C0')\n",
    "    plt.scatter(signed_stat[sig_up],   grad_mean[sig_up],   s=30, alpha=0.9, color='C3')\n",
    "    plt.axvline(0.0, ls='--', lw=1, color='k'); plt.axhline(0.0, ls='--', lw=1, color='k')\n",
    "    plt.xlabel(r\"log2FC × $-\\log_{10}(p)$  (signed)\")\n",
    "    plt.ylabel(\"Average log-odds gradient (RunA/B mean)\")\n",
    "    plt.title(f\"{title_prefix}  (Pearson r = {r:.3f})\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(outfile, dpi=200)\n",
    "    plt.close()\n",
    "    return float(r)\n",
    "\n",
    "# ----------------------------\n",
    "# Splitter (4 CSVs per combined CSV) with bin prefix\n",
    "# ----------------------------\n",
    "TARGET_COLS = [\"pos_runA\", \"pos_runB\", \"negabs_runA\", \"negabs_runB\"]\n",
    "MZ_COL = \"m/z\"\n",
    "\n",
    "def split_csv(input_path: str, out_dir: str, bin_value: int) -> list[str]:\n",
    "    \"\"\"Split one combined CSV into 4 CSVs, prefixing filenames with bin number.\"\"\"\n",
    "    df = pd.read_csv(input_path)\n",
    "    if MZ_COL not in df.columns:\n",
    "        print(f\"[SKIP] {input_path} (no '{MZ_COL}' column)\")\n",
    "        return []\n",
    "    available_targets = [c for c in TARGET_COLS if c in df.columns]\n",
    "    if not available_targets:\n",
    "        print(f\"[SKIP] {input_path} (none of {TARGET_COLS} found)\")\n",
    "        return []\n",
    "    base = os.path.splitext(os.path.basename(input_path))[0]\n",
    "    written = []\n",
    "    for col in available_targets:\n",
    "        out_df = df[[MZ_COL, col]].copy()\n",
    "        out_path = os.path.join(out_dir, f\"bin{bin_value}_{base}_{col}.csv\")\n",
    "        out_df.to_csv(out_path, index=False)\n",
    "        written.append(out_path)\n",
    "    return written\n",
    "\n",
    "def process_folder(folder_path: str, bin_value: int) -> list[str]:\n",
    "    \"\"\"Split all CSVs in a folder to 'result/' and return list of result paths.\"\"\"\n",
    "    if not os.path.isdir(folder_path):\n",
    "        print(f\"[WARN] Not a folder: {folder_path}\")\n",
    "        return []\n",
    "    out_dir = os.path.join(folder_path, \"result\")\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "    all_outputs = []\n",
    "    for fname in os.listdir(folder_path):\n",
    "        if fname.lower().endswith(\".csv\"):\n",
    "            fpath = os.path.join(folder_path, fname)\n",
    "            print(f\"Splitting {fpath} ...\")\n",
    "            outputs = split_csv(fpath, out_dir, bin_value)\n",
    "            all_outputs.extend(outputs)\n",
    "\n",
    "    print(f\"Split done. Wrote {len(all_outputs)} files to {out_dir}\")\n",
    "    return all_outputs\n",
    "\n",
    "# ----------------------------\n",
    "# Config (edit these)\n",
    "# ----------------------------\n",
    "CSV_PATH    = r\"F:/20251010/dataset_rt_batch.csv\"   # input dataset with 'bin' and 'target'\n",
    "EPOCHS      = 50\n",
    "BATCH_SIZE  = 32\n",
    "K_SPLITS    = 5\n",
    "N_REPEATS   = 5\n",
    "SEED_BASES  = [111, 777]                   # two independent runs\n",
    "OUT_ROOT    = r\"F:/20251018e/\"             # everything goes directly under bin_<N>/\n",
    "BIN_WHITELIST = None  # e.g. [35, 75]\n",
    "\n",
    "# Volcano thresholds\n",
    "ALPHA = 0.05\n",
    "LFC_LEFT, LFC_RIGHT = -0.5, 0.5\n",
    "\n",
    "# Present but NOT used for volcano stats anymore\n",
    "TIC_SCALE = 1e10\n",
    "\n",
    "# Fixed grouping list (no prompt, no AUTO)\n",
    "GROUPINGS = [((2,),(1,)), ((2,),(0,)), ((1,),(0,)), ((3,),(2,)), ((3,),(0,1,2)), ((2,1),(0,)), ((3,),(1,)),  ((3,),(0,))]\n",
    "\n",
    "# Always use synthetic data\n",
    "FORCE_SYNTHETIC = True\n",
    "\n",
    "# ----------------------------\n",
    "# Synthetic dataset generator (100 up + 100 down per class, unique)\n",
    "# ----------------------------\n",
    "def generate_synthetic_csv(csv_path: str,\n",
    "                           n_classes: int = 4,\n",
    "                           samples_per_class: int = 100,\n",
    "                           n_noise_features: int = 10000,\n",
    "                           effect_size: float = 1.0,\n",
    "                           feature_sd: float = 1.0,\n",
    "                           bin_value: int = 1,\n",
    "                           random_state: int = 42):\n",
    "    \"\"\"\n",
    "    Creates a CSV at csv_path with columns: 'bin','target', and feature columns f0000..fXXXX.\n",
    "    For each class c, we set 200 unique marker features: 100 up-regulated, 100 down-regulated (non-overlapping across classes).\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(random_state)\n",
    "\n",
    "    # Total marker features: 200 per class (100 up + 100 down), disjoint across classes\n",
    "    markers_per_class = 200\n",
    "    total_marker_features = markers_per_class * n_classes\n",
    "\n",
    "    # Add some extra noise features\n",
    "    D = total_marker_features + n_noise_features\n",
    "\n",
    "    # Build disjoint marker index sets per class\n",
    "    all_feat_indices = np.arange(D)\n",
    "    class_markers = {}\n",
    "    start = 0\n",
    "    for c in range(n_classes):\n",
    "        idxs = all_feat_indices[start:start+markers_per_class]\n",
    "        start += markers_per_class\n",
    "        up_idxs = idxs[:markers_per_class//2]    # first 100\n",
    "        down_idxs = idxs[markers_per_class//2:]  # next 100\n",
    "        class_markers[c] = (up_idxs, down_idxs)\n",
    "\n",
    "    # Generate base noise for all samples\n",
    "    N = samples_per_class * n_classes\n",
    "    X = rng.normal(loc=0.0, scale=feature_sd, size=(N, D))\n",
    "\n",
    "    # Apply class effects (unique markers per class)\n",
    "    y = np.repeat(np.arange(n_classes), samples_per_class)\n",
    "    for c in range(n_classes):\n",
    "        idx = np.where(y == c)[0]\n",
    "        up, down = class_markers[c]\n",
    "        X[idx[:, None], up]  += effect_size\n",
    "        X[idx[:, None], down] -= effect_size\n",
    "\n",
    "    # Make features strictly positive (so later log2FC is stable after your scaling)\n",
    "    X = np.exp(X / 2.0)  # log-normal-ish, > 0, preserves separations\n",
    "\n",
    "    # Create DataFrame\n",
    "    cols = [f\"f{str(i).zfill(4)}\" for i in range(D)]\n",
    "    df = pd.DataFrame(X, columns=cols)\n",
    "    df.insert(0, \"target\", y.astype(int))\n",
    "    df.insert(0, \"bin\", bin_value)\n",
    "\n",
    "    os.makedirs(os.path.dirname(csv_path), exist_ok=True)\n",
    "    df.to_csv(csv_path, index=False)\n",
    "    print(f\"[SYNTH] Wrote synthetic dataset to: {csv_path}  | samples={N}, D={D}\")\n",
    "    print(f\"[SYNTH] Per class markers: 100 up + 100 down (unique sets per class)\")\n",
    "\n",
    "# ----------------------------\n",
    "# KFold+Repeats trainer\n",
    "# ----------------------------\n",
    "def train_kfold_repeats(X: np.ndarray, Y: np.ndarray, seed_base: int):\n",
    "    kf = KFold(n_splits=K_SPLITS, shuffle=True, random_state=42)\n",
    "    all_models = []\n",
    "    num_classes = int(np.max(Y)) + 1\n",
    "    for fold, (tr, va) in enumerate(kf.split(X, Y), 1):\n",
    "        X_tr, y_tr = X[tr], Y[tr]\n",
    "        X_va, y_va = X[va], Y[va]\n",
    "        for r in range(N_REPEATS):\n",
    "            seed = seed_base * 1000 + fold * 100 + r\n",
    "            tf.keras.utils.set_random_seed(seed)\n",
    "            np.random.seed(seed)\n",
    "            m = build_model(X.shape[1], num_classes)\n",
    "            m.fit(X_tr, y_tr, epochs=EPOCHS, batch_size=BATCH_SIZE,\n",
    "                  validation_data=(X_va, y_va), verbose=0)\n",
    "            all_models.append(m)\n",
    "        print(f\"[Seed base {seed_base}] Fold {fold}/{K_SPLITS} trained {N_REPEATS} models (total: {len(all_models)})\")\n",
    "    return all_models\n",
    "\n",
    "# ----------------------------\n",
    "# MAIN (bin-root outputs + keep only split CSVs)\n",
    "# ----------------------------\n",
    "def main():\n",
    "    # --- Always generate synthetic dataset (100 up + 100 down unique per class) ---\n",
    "    if FORCE_SYNTHETIC:\n",
    "        generate_synthetic_csv(\n",
    "            csv_path=CSV_PATH,\n",
    "            n_classes=4,\n",
    "            samples_per_class=100,   # 100 samples per class\n",
    "            n_noise_features=10000,\n",
    "            effect_size=1.0,\n",
    "            feature_sd=1.0,\n",
    "            bin_value=1,\n",
    "            random_state=42\n",
    "        )\n",
    "\n",
    "    df = pd.read_csv(CSV_PATH)\n",
    "    # Discover bins\n",
    "    bins_found = sorted([b for b in df[\"bin\"].dropna().unique().tolist()])\n",
    "    if BIN_WHITELIST is not None:\n",
    "        bins_to_process = [b for b in bins_found if b in set(BIN_WHITELIST)]\n",
    "    else:\n",
    "        bins_to_process = bins_found\n",
    "    if len(bins_to_process) == 0:\n",
    "        raise ValueError(f\"No 'bin' values found in {CSV_PATH}\")\n",
    "\n",
    "    # Keep only classes 0..3 by default\n",
    "    df = df[df[\"target\"].astype(int).isin([0,1,2,3])].copy()\n",
    "    unique_labels = np.sort(df[\"target\"].astype(int).unique())\n",
    "    assert unique_labels[0] == 0 and np.array_equal(unique_labels, np.arange(unique_labels[-1] + 1)), \\\n",
    "        f\"Non-contiguous labels detected: {unique_labels}. Please remap to 0..C-1.\"\n",
    "    groupings = GROUPINGS\n",
    "    print(f\"\\nUsing fixed grouping(s): {groupings}\")\n",
    "\n",
    "    os.makedirs(OUT_ROOT, exist_ok=True)\n",
    "\n",
    "    for BIN_VALUE in bins_to_process:\n",
    "        print(f\"\\n================= BIN {BIN_VALUE} =================\")\n",
    "        models_A = models_B = None\n",
    "        X = Y = fdf = None\n",
    "\n",
    "        try:\n",
    "            fdf = df[df[\"bin\"] == BIN_VALUE].copy()\n",
    "            if fdf.empty:\n",
    "                print(f\"[WARN] No rows for bin {BIN_VALUE}; skipping.\")\n",
    "                continue\n",
    "\n",
    "            # Normalize all features except ['bin','target'] within this bin\n",
    "            cols_to_norm = fdf.columns.difference(['bin', 'target'])\n",
    "            fdf[cols_to_norm] = fdf[cols_to_norm].apply(lambda x: x / (x.max() + 1.0))\n",
    "\n",
    "            Y = fdf[\"target\"].astype(int).to_numpy()\n",
    "            feature_cols = fdf.columns.difference(['bin', 'target'])\n",
    "            X = np.nan_to_num(fdf[feature_cols].to_numpy(), copy=False).astype(np.float32)\n",
    "\n",
    "            if X.shape[0] < 2 or X.shape[1] < 1:\n",
    "                print(f\"[WARN] Insufficient data for bin {BIN_VALUE} (samples={X.shape[0]}, dim={X.shape[1]}). Skipping.\")\n",
    "                continue\n",
    "\n",
    "            print(f\"Bin {BIN_VALUE}: samples={X.shape[0]}, dim={X.shape[1]}  class_counts=\"\n",
    "                  f\"{dict(zip(*np.unique(Y, return_counts=True)))}\")\n",
    "\n",
    "            # Output dirs per bin (everything under bin folder)\n",
    "            OUT_DIR   = os.path.join(OUT_ROOT, f\"bin_{str(BIN_VALUE).replace('.', '_')}\")\n",
    "            CSV_DIR   = os.path.join(OUT_DIR, \"csv\")\n",
    "            PLOTS_DIR = os.path.join(OUT_DIR, \"plots\")\n",
    "            os.makedirs(OUT_DIR, exist_ok=True)\n",
    "            os.makedirs(CSV_DIR, exist_ok=True)\n",
    "            os.makedirs(PLOTS_DIR, exist_ok=True)\n",
    "\n",
    "            # Train ensembles ONCE per run (A, B)\n",
    "            models_A = train_kfold_repeats(X, Y, seed_base=SEED_BASES[0])\n",
    "            models_B = train_kfold_repeats(X, Y, seed_base=SEED_BASES[1])\n",
    "\n",
    "            # grid helper (for your mirror plots)\n",
    "            def _make_grid(n):\n",
    "                n_grid = min(10000, n)\n",
    "                x_grid = np.arange(600, 600 + 0.1 * n_grid, 0.1)[:n_grid]\n",
    "                return n_grid, x_grid\n",
    "\n",
    "            def save_compare_only(pos_ids, neg_ids):\n",
    "                # ---- Gradients (Run A & B) — unchanged, NO TIC normalization\n",
    "                grad_A = compute_avg_group_logodds_gradient(X, models_A, pos_ids=pos_ids, neg_ids=neg_ids, eps=1e-8)\n",
    "                grad_B = compute_avg_group_logodds_gradient(X, models_B, pos_ids=pos_ids, neg_ids=neg_ids, eps=1e-8)\n",
    "                grad_mean = (grad_A + grad_B) / 2.0\n",
    "\n",
    "                pos_tag = \"_\".join(map(str, pos_ids))\n",
    "                neg_tag = \"_\".join(map(str, neg_ids))\n",
    "                tag = f\"pos_{pos_tag}__neg_{neg_tag}\"\n",
    "\n",
    "                # ---- Mirror plots\n",
    "                n_grid, x_grid = _make_grid(min(grad_A.size, grad_B.size))\n",
    "                yA = grad_A[:n_grid]; yB = grad_B[:n_grid]\n",
    "                yA_pos = np.where(yA > 0, yA, 0.0)\n",
    "                yB_pos = np.where(yB > 0, yB, 0.0)\n",
    "                yA_neg = np.where(yA < 0, -yA, 0.0)  # abs\n",
    "                yB_neg = np.where(yB < 0, -yB, 0.0)\n",
    "\n",
    "                cos_pos = cosine_sim(yA_pos, yB_pos)\n",
    "                cos_neg = cosine_sim(yA_neg, yB_neg)\n",
    "\n",
    "                comb_csv = os.path.join(CSV_DIR, f\"grads_AB__{tag}.csv\")\n",
    "                pd.DataFrame({\n",
    "                    \"m/z\": x_grid,\n",
    "                    \"grad_runA\": yA,\n",
    "                    \"grad_runB\": yB,\n",
    "                    \"pos_runA\": yA_pos,\n",
    "                    \"pos_runB\": yB_pos,\n",
    "                    \"negabs_runA\": yA_neg,\n",
    "                    \"negabs_runB\": yB_neg,\n",
    "                }).to_csv(comb_csv, index=False)\n",
    "\n",
    "                pos_title = (f\"Bin {BIN_VALUE} — Mirror Positive Gradients \"\n",
    "                             f\"[{tag}] (cos={cos_pos:.4f})\")\n",
    "                neg_title = (f\"Bin {BIN_VALUE} — Mirror Negative Gradients |abs| \"\n",
    "                             f\"[{tag}] (cos={cos_neg:.4f})\")\n",
    "\n",
    "                mirror_plot(\n",
    "                    x_grid, yA_pos, yB_pos,\n",
    "                    title=pos_title,\n",
    "                    outfile=os.path.join(PLOTS_DIR, f\"{tag}__mirror_pos.png\")\n",
    "                )\n",
    "                mirror_plot(\n",
    "                    x_grid, yA_neg, yB_neg,\n",
    "                    title=neg_title,\n",
    "                    outfile=os.path.join(PLOTS_DIR, f\"{tag}__mirror_negabs.png\")\n",
    "                )\n",
    "\n",
    "                # ---- Volcano statistics WITHOUT TIC normalization\n",
    "                pvals, log2fc = ttest_and_log2fc(X, Y, pos_ids, neg_ids, eps=1e-12)\n",
    "\n",
    "                # Save per-feature stats + raw grads + mean grad\n",
    "                stats_csv = os.path.join(CSV_DIR, f\"stats_and_grads__{tag}.csv\")\n",
    "                pd.DataFrame({\n",
    "                    \"feature\": list(feature_cols),\n",
    "                    \"log2FC\": log2fc,\n",
    "                    \"pval\": pvals,\n",
    "                    \"neglog10p\": -np.log10(np.clip(pvals, 1e-300, 1.0)),\n",
    "                    \"grad_runA_full\": grad_A,\n",
    "                    \"grad_runB_full\": grad_B,\n",
    "                    \"grad_mean\": grad_mean,\n",
    "                    \"signed_stat\": log2fc * (-np.log10(np.clip(pvals, 1e-300, 1.0))),\n",
    "                }).to_csv(stats_csv, index=False)\n",
    "\n",
    "                volcano_plot(\n",
    "                    log2fc, pvals, LFC_LEFT, LFC_RIGHT, ALPHA,\n",
    "                    title=f\"Bin {BIN_VALUE} — Volcano ({tag})\",\n",
    "                    outfile=os.path.join(PLOTS_DIR, f\"{tag}__volcano.png\")\n",
    "                )\n",
    "                r = signed_agreement_plot(\n",
    "                    log2fc, pvals, grad_mean,\n",
    "                    LFC_LEFT, LFC_RIGHT, ALPHA,\n",
    "                    title_prefix=f\"Bin {BIN_VALUE} — Signed agreement ({tag})\",\n",
    "                    outfile=os.path.join(PLOTS_DIR, f\"{tag}__signed_agreement.png\")\n",
    "                )\n",
    "\n",
    "                # summary JSON directly under bin/\n",
    "                with open(os.path.join(OUT_DIR, f\"summary__{tag}.json\"), \"w\") as fC:\n",
    "                    json.dump({\n",
    "                        \"bin\": BIN_VALUE,\n",
    "                        \"grouping\": {\"pos_ids\": list(pos_ids), \"neg_ids\": list(neg_ids)},\n",
    "                        \"comparison\": \"Run A vs Run B\",\n",
    "                        \"cosine_pos\": cos_pos,\n",
    "                        \"cosine_neg_abs\": cos_neg,\n",
    "                        \"pearson_signed_agreement\": r,\n",
    "                        \"thresholds\": {\"alpha\": ALPHA, \"lfc_left\": LFC_LEFT, \"lfc_right\": LFC_RIGHT},\n",
    "                        \"paths\": {\n",
    "                            \"combined_csv_for_mirror\": comb_csv,\n",
    "                            \"stats_and_grads_csv\": stats_csv,\n",
    "                            \"plots_dir\": PLOTS_DIR,\n",
    "                        }\n",
    "                    }, fC, indent=2)\n",
    "\n",
    "                print(f\"  [COMPARE] {tag} | Cos(pos)={cos_pos:.6f} Cos(neg|abs|)={cos_neg:.6f}  r={r:.3f}\")\n",
    "\n",
    "            # run all groupings -> write combined CSVs/plots/JSONs\n",
    "            num_classes = int(np.max(Y)) + 1\n",
    "            for (pos_ids, neg_ids) in groupings:\n",
    "                for idx in (*pos_ids, *neg_ids):\n",
    "                    assert 0 <= idx < num_classes, f\"Class index {idx} out of range 0..{num_classes-1}\"\n",
    "                save_compare_only(pos_ids, neg_ids)\n",
    "\n",
    "            # --- Split step: create split CSVs into csv/result/ ---\n",
    "            split_outputs = process_folder(CSV_DIR, int(BIN_VALUE))\n",
    "\n",
    "            # Move split files from csv/result/ -> csv/ and delete originals\n",
    "            result_dir = os.path.join(CSV_DIR, \"result\")\n",
    "            moved = []\n",
    "            if os.path.isdir(result_dir):\n",
    "                for fname in os.listdir(result_dir):\n",
    "                    src = os.path.join(result_dir, fname)\n",
    "                    dst = os.path.join(CSV_DIR, fname)\n",
    "                    os.replace(src, dst)\n",
    "                    moved.append(dst)\n",
    "                try: os.rmdir(result_dir)\n",
    "                except OSError: pass\n",
    "\n",
    "            # Delete original combined CSVs (keep only split)\n",
    "            for fname in os.listdir(CSV_DIR):\n",
    "                if fname.lower().endswith(\".csv\") and fname.startswith(\"grads_AB__\"):\n",
    "                    try:\n",
    "                        os.remove(os.path.join(CSV_DIR, fname))\n",
    "                    except Exception as e:\n",
    "                        print(f\"[WARN] Could not remove {fname}: {e}\")\n",
    "\n",
    "            # Save manifest of final CSVs\n",
    "            with open(os.path.join(OUT_DIR, \"split_manifest.json\"), \"w\") as fman:\n",
    "                json.dump({\n",
    "                    \"bin\": BIN_VALUE,\n",
    "                    \"final_csvs\": sorted([os.path.basename(p) for p in moved]),\n",
    "                }, fman, indent=2)\n",
    "\n",
    "        finally:\n",
    "            try:\n",
    "                if models_A is not None:\n",
    "                    for _m in models_A: del _m\n",
    "                del models_A\n",
    "            except: pass\n",
    "            try:\n",
    "                if models_B is not None:\n",
    "                    for _m in models_B: del _m\n",
    "                del models_B\n",
    "            except: pass\n",
    "            for v in [\"X\",\"Y\",\"fdf\"]:\n",
    "                try: del globals()[v]\n",
    "                except: pass\n",
    "            hard_free()\n",
    "\n",
    "    print(\"\\nAll bins processed. (bin-root outputs; csv keeps only split files)\\n\")\n",
    "\n",
    "# ---- run ----\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "702d9d5c",
   "metadata": {},
   "source": [
    "Realistic training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "323de349",
   "metadata": {},
   "outputs": [],
   "source": [
    "# simulate_ms1_smallmols_multiclass_0p1amu.py\n",
    "# ---------------------------------------------------\n",
    "# - Profile MS1 at 0.1 amu bins (100–1000 m/z)\n",
    "# - 100 singly charged small molecules ([M+H]+, z=1) with averagine isotopes\n",
    "# - Multi-class dataset: each class has its own up/down-regulated molecules\n",
    "# - Outputs:\n",
    "#     ms1_multiclass_train.csv            (rows = scans, columns = target + mz_*)\n",
    "#     ms1_multiclass_meta.csv             (molecule params + which class up/down)\n",
    "#     ms1_multiclass_labelmap.json        (index -> class label)\n",
    "#     (optional) quick QC plots per class (means)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from math import sqrt\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "# -------------------------------\n",
    "# Configuration (edit as needed)\n",
    "# -------------------------------\n",
    "SEED = 2026\n",
    "OUT_DIR = Path(\"ms1_multiclass_out\"); OUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# Spectral & molecule parameters\n",
    "N_MOLECULES = 100\n",
    "MASS_RANGE = (150.0, 900.0)      # neutral masses (Da)\n",
    "MZ_MIN, MZ_MAX = 100.0, 1000.0\n",
    "STEP = 0.1                        # 0.1 amu bins\n",
    "PROTON = 1.007276466812           # [M+H]+, z=1\n",
    "RES_FWHM = 0.1                    # fixed peak FWHM ~0.1 Da at all m/z\n",
    "BASELINE_NOISE = 20.0\n",
    "DYN_RANGE = (3e3, 2e5)            # base intensity scale before jitter/FC\n",
    "\n",
    "# Dataset design\n",
    "N_CLASSES = 4                     # number of classes\n",
    "N_PER_CLASS = 20                  # scans per class\n",
    "JITTER_LOW, JITTER_HIGH = 0.8, 1.2  # per-scan molecule jitter (±20%)\n",
    "\n",
    "# Regulation design per class\n",
    "UP_PER_CLASS = 10                 # number of up-regulated molecules per class\n",
    "DOWN_PER_CLASS = 10               # number of down-regulated molecules per class\n",
    "UP_FC  = 5.0                      # fold-change for up-regulated\n",
    "DOWN_FC = 0.2                     # fold-change for down-regulated\n",
    "\n",
    "# Plot mean spectra per class?\n",
    "MAKE_QC_PLOTS = True\n",
    "\n",
    "# -------------------------------\n",
    "# Helpers\n",
    "# -------------------------------\n",
    "rng = np.random.default_rng(SEED)\n",
    "mz = np.arange(MZ_MIN, MZ_MAX + STEP, STEP)\n",
    "POINTS = len(mz)\n",
    "\n",
    "def gaussian(x, mu, fwhm=RES_FWHM):\n",
    "    sigma = fwhm / 2.355\n",
    "    return np.exp(-0.5 * ((x - mu) / sigma) ** 2)\n",
    "\n",
    "def iso_env_small(mass):\n",
    "    \"\"\"Averagine-ish C-13 envelope (discrete Gaussian over isotope index k).\"\"\"\n",
    "    C = (mass / 111.1254) * 4.9384\n",
    "    p = 0.0109\n",
    "    mean_k = p * C\n",
    "    var_k = p * (1 - p) * C\n",
    "    sd_k = sqrt(max(var_k, 1e-9))\n",
    "    k = np.arange(0, int(mean_k + 6 * sd_k) + 1)\n",
    "    env = np.exp(-0.5 * ((k - mean_k) / sd_k) ** 2)\n",
    "    env /= env.sum() + 1e-12\n",
    "    return k, env\n",
    "\n",
    "# -------------------------------\n",
    "# Define molecules (100)\n",
    "# -------------------------------\n",
    "# Spread base concentrations across 1→1000 so molecules have diverse intensities\n",
    "concs = np.geomspace(1.0, 1000.0, N_MOLECULES)\n",
    "rng.shuffle(concs)\n",
    "\n",
    "molecules = []\n",
    "for i in range(N_MOLECULES):\n",
    "    mass = rng.uniform(*MASS_RANGE)\n",
    "    apex_mz = mass + PROTON               # [M+H]+, z=1\n",
    "    base_int = rng.uniform(*DYN_RANGE) * concs[i]\n",
    "    k_vals, k_fracs = iso_env_small(mass)\n",
    "    molecules.append({\n",
    "        \"molecule_id\": i,\n",
    "        \"neutral_mass_Da\": mass,\n",
    "        \"mz_apex\": apex_mz,\n",
    "        \"k_vals\": k_vals,\n",
    "        \"k_fracs\": k_fracs,\n",
    "        \"base_intensity\": base_int,\n",
    "        \"relative_conc\": concs[i],\n",
    "    })\n",
    "\n",
    "base_scales = np.array([m[\"base_intensity\"] for m in molecules])\n",
    "\n",
    "# -------------------------------\n",
    "# Build class-specific signatures\n",
    "# -------------------------------\n",
    "# For each class, pick UP_PER_CLASS and DOWN_PER_CLASS molecule IDs.\n",
    "# We allow partial overlap across classes to make the problem realistic.\n",
    "all_ids = np.arange(N_MOLECULES)\n",
    "class_signatures = []\n",
    "for c in range(N_CLASSES):\n",
    "    up_ids = rng.choice(all_ids, size=UP_PER_CLASS, replace=False)\n",
    "    remaining = np.setdiff1d(all_ids, up_ids)\n",
    "    down_ids = rng.choice(remaining, size=DOWN_PER_CLASS, replace=False)\n",
    "    class_signatures.append({\"up\": np.sort(up_ids), \"down\": np.sort(down_ids)})\n",
    "\n",
    "# -------------------------------\n",
    "# Scan simulator\n",
    "# -------------------------------\n",
    "def simulate_scan(intensity_scales: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Render one MS1 profile at 0.1 amu bins.\"\"\"\n",
    "    sig = np.zeros_like(mz)\n",
    "    for i, m in enumerate(molecules):\n",
    "        for k, frac in zip(m[\"k_vals\"], m[\"k_fracs\"]):\n",
    "            pk_mz = m[\"mz_apex\"] + k  # z=1 → +1.000 m/z per isotope\n",
    "            if MZ_MIN <= pk_mz <= MZ_MAX:\n",
    "                sig += intensity_scales[i] * frac * gaussian(mz, pk_mz)\n",
    "    baseline = 80.0 + 20.0 * (mz - MZ_MIN) / (MZ_MAX - MZ_MIN)\n",
    "    return np.clip(sig + rng.normal(0, BASELINE_NOISE, size=POINTS), 0, None)\n",
    "\n",
    "# -------------------------------\n",
    "# Generate scans for each class\n",
    "# -------------------------------\n",
    "rows = []\n",
    "label_map = {i: f\"class_{i}\" for i in range(N_CLASSES)}  # customize labels here\n",
    "\n",
    "for class_idx in range(N_CLASSES):\n",
    "    sig = class_signatures[class_idx]\n",
    "    up_mask = np.zeros(N_MOLECULES, dtype=bool);  up_mask[sig[\"up\"]] = True\n",
    "    dn_mask = np.zeros(N_MOLECULES, dtype=bool);  dn_mask[sig[\"down\"]] = True\n",
    "\n",
    "    for _ in range(N_PER_CLASS):\n",
    "        scales = base_scales * rng.uniform(JITTER_LOW, JITTER_HIGH, N_MOLECULES)\n",
    "        scales[up_mask] *= UP_FC\n",
    "        scales[dn_mask] *= DOWN_FC\n",
    "        spectrum = simulate_scan(scales)\n",
    "        rows.append((class_idx, spectrum))\n",
    "\n",
    "# -------------------------------\n",
    "# Save training table (wide)\n",
    "# -------------------------------\n",
    "train_df = pd.DataFrame(\n",
    "    [r[1] for r in rows],\n",
    "    columns=[f\"mz_{i}\" for i in range(POINTS)]\n",
    ")\n",
    "train_df.insert(0, \"target\", [r[0] for r in rows])  # integer targets 0..N_CLASSES-1\n",
    "\n",
    "train_path = OUT_DIR / \"ms1_multiclass_train.csv\"\n",
    "train_df.to_csv(train_path, index=False)\n",
    "\n",
    "# -------------------------------\n",
    "# Save metadata: molecules + class signatures\n",
    "# -------------------------------\n",
    "meta_df = pd.DataFrame({\n",
    "    \"molecule_id\": [m[\"molecule_id\"] for m in molecules],\n",
    "    \"neutral_mass_Da\": [m[\"neutral_mass_Da\"] for m in molecules],\n",
    "    \"mz_apex\": [m[\"mz_apex\"] for m in molecules],\n",
    "    \"base_intensity\": [m[\"base_intensity\"] for m in molecules],\n",
    "    \"relative_conc\": [m[\"relative_conc\"] for m in molecules],\n",
    "})\n",
    "# Expand signatures into Boolean columns per class (up/down)\n",
    "for c in range(N_CLASSES):\n",
    "    up_col = np.isin(meta_df[\"molecule_id\"], class_signatures[c][\"up\"])\n",
    "    dn_col = np.isin(meta_df[\"molecule_id\"], class_signatures[c][\"down\"])\n",
    "    meta_df[f\"class{c}_is_up\"] = up_col\n",
    "    meta_df[f\"class{c}_is_down\"] = dn_col\n",
    "\n",
    "meta_path = OUT_DIR / \"ms1_multiclass_meta.csv\"\n",
    "meta_df.to_csv(meta_path, index=False)\n",
    "\n",
    "# Save label map and signatures for reproducibility\n",
    "with open(OUT_DIR / \"ms1_multiclass_labelmap.json\", \"w\") as f:\n",
    "    json.dump(label_map, f, indent=2)\n",
    "\n",
    "with open(OUT_DIR / \"ms1_multiclass_signatures.json\", \"w\") as f:\n",
    "    json.dump(\n",
    "        {str(c): {\"up\": class_signatures[c][\"up\"].tolist(),\n",
    "                  \"down\": class_signatures[c][\"down\"].tolist()}\n",
    "         for c in range(N_CLASSES)},\n",
    "        f, indent=2\n",
    "    )\n",
    "\n",
    "# -------------------------------\n",
    "# Optional QC plots: mean spectra per class\n",
    "# -------------------------------\n",
    "if MAKE_QC_PLOTS:\n",
    "    fig, ax = plt.subplots(figsize=(14, 5))\n",
    "    cmap = plt.cm.get_cmap(\"tab10\", N_CLASSES)\n",
    "    for c in range(N_CLASSES):\n",
    "        block = train_df.loc[train_df[\"target\"] == c].drop(columns=\"target\").values\n",
    "        ax.plot(mz, block.mean(axis=0), lw=0.9, label=label_map[c], color=cmap(c))\n",
    "    ax.set_title(\"Class-wise Mean MS1 Spectra (0.1 amu)\")\n",
    "    ax.set_xlabel(\"m/z\"); ax.set_ylabel(\"Intensity (a.u.)\")\n",
    "    ax.legend(ncol=2)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(OUT_DIR / \"ms1_multiclass_means.png\", dpi=180)\n",
    "    plt.close()\n",
    "\n",
    "print(\"Saved:\")\n",
    "print(\" -\", train_path)\n",
    "print(\" -\", meta_path)\n",
    "print(\" -\", OUT_DIR / \"ms1_multiclass_labelmap.json\")\n",
    "print(\" -\", OUT_DIR / \"ms1_multiclass_signatures.json\")\n",
    "if MAKE_QC_PLOTS:\n",
    "    print(\" -\", OUT_DIR / \"ms1_multiclass_means.png\")\n",
    "print(f\"Rows: {len(train_df):,}  |  Columns: {train_df.shape[1]:,}  (includes 'target')\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vae",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
